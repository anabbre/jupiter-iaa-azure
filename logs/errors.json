{"timestamp": "2025-11-10T19:00:20.792477+01:00", "level": "ERROR", "message": "Error indexando documento", "module": "create_book_index", "function": "index_documents", "line": 147, "request_id": "qdrant_index_1762797608", "session_id": "anonymous", "extra": {"numero": 20, "total": 20, "error": "'str' object has no attribute 'metadata'", "tipo_error": "AttributeError", "request_id": ["page_content='Preface\nA long time ago, in a datacenter far, far away, an ancient group of powerful\nbeings known as “sysadmins” used to deploy infrastructure manually. Every\nserver, every database, every load balancer, and every bit of network\nconfiguration was created and managed by hand. It was a dark and fearful\nage: fear of downtime, fear of accidental misconfiguration, fear of slow and\nfragile deployments, and fear of what would happen if the sysadmins fell to\nthe dark side (i.e., took a vacation). The good news is that thanks to the\nDevOps movement, there is now a better way to do things: Terraform.\nTerraform is an open source tool created by HashiCorp that allows you to\ndefine your infrastructure as code using a simple, declarative language and\nto deploy and manage that infrastructure across a variety of public cloud\nproviders (e.g., Amazon Web Services [AWS], Microsoft Azure, Google\nCloud Platform, DigitalOcean) and private cloud and virtualization\nplatforms (e.g., OpenStack, VMware) using a few commands. For example,\ninstead of manually clicking around a web page or running dozens of\ncommands, here is all the code it takes to configure a server on AWS:\nprovider \"aws\" {\n  region = \"us-east-2\"\n} \n \nresource \"aws_instance\" \"example\" {\n  ami           = \"ami-0fb653ca2d3203ac1\"\n  instance_type = \"t2.micro\"\n}\nAnd to deploy it, you just run the following:\n$ terraform init \n$ terraform apply' metadata={'original_pages_range': '6', 'source': '001_Preface', 'file_path': '/mnt/c/Users/Carlos/Documents/Github/jupiter-iaa-azure/Scripts/RAG/../../data/optimized_chunks/Libro-TF/001_Preface.pdf', 'num_pages': 1}", "page_content='Thanks to its simplicity and power, Terraform has emerged as a key player\nin the DevOps world. It allows you to replace the tedious, fragile, and\nmanual parts of infrastructure management with a solid, automated\nfoundation upon which you can build all your other DevOps practices (e.g.,\nautomated testing, Continuous Integration, Continuous Delivery) and\ntooling (e.g., Docker, Chef, Puppet).\nThis book is the fastest way to get up and running with Terraform.\nYou’ll go from deploying the most basic “Hello, World” Terraform example\n(in fact, you just saw it!) all the way up to running a full tech stack (virtual\nservers, Kubernetes clusters, Docker containers, load balancers, databases)\ncapable of supporting a large amount of traffic and a large team of\ndevelopers—all in the span of just a few chapters. This is a hands-on\ntutorial that not only teaches you DevOps and infrastructure as code (IaC)\nprinciples but also walks you through dozens of code examples that you can\ntry at home, so make sure you have your computer handy.\nBy the time you’re done, you’ll be ready to use Terraform in the real world.\nWho Should Read This Book\nThis book is for anyone responsible for the code after it has been written.\nThat includes sysadmins, operations engineers, release engineers, site\nreliability engineers, DevOps engineers, infrastructure developers, full-\nstack developers, engineering managers, and CTOs. No matter what your\ntitle is, if you’re the one managing infrastructure, deploying code,\nconfiguring servers, scaling clusters, backing up data, monitoring apps, and\nresponding to alerts at 3 a.m., this book is for you.\nCollectively, all of these tasks are usually referred to as operations. In the\npast, it was common to find developers who knew how to write code but\ndid not understand operations; likewise, it was common to find sysadmins\nwho understood operations but did not know how to write code. You could\nget away with that divide in the past, but in the modern world, as cloud\ncomputing and the DevOps movement become ubiquitous, just about every\n\ndeveloper will need to learn operational skills, and every sysadmin will\nneed to learn coding skills.\nThis book does not assume that you’re already an expert coder or expert\nsysadmin—a basic familiarity with programming, the command line, and\nserver-based software (e.g., websites) should suffice. Everything else you\nneed you’ll be able to pick up as you go, so that by the end of the book, you\nwill have a solid grasp of one of the most critical aspects of modern\ndevelopment and operations: managing infrastructure as code.\nIn fact, you’ll learn not only how to manage infrastructure as code using\nTerraform but also how this fits into the overall DevOps world. Here are\nsome of the questions you’ll be able to answer by the end of the book:\nWhy use IaC at all?\nWhat are the differences between configuration management,\norchestration, provisioning, and server templating?\nWhen should you use Terraform, Chef, Ansible, Puppet, Pulumi,\nCloudFormation, Docker, Packer, or Kubernetes?\nHow does Terraform work, and how do you use it to manage your\ninfrastructure?\nHow do you create reusable Terraform modules?\nHow do you securely manage secrets when working with Terraform?\nHow do you use Terraform with multiple regions, accounts, and\nclouds?\nHow do you write Terraform code that’s reliable enough for production\nusage?\nHow do you test your Terraform code?\nHow do you make Terraform a part of your automated deployment\nprocess?' metadata={'original_pages_range': '7-8', 'source': '002_Who_Should_Read_This_Book', 'file_path': '/mnt/c/Users/Carlos/Documents/Github/jupiter-iaa-azure/Scripts/RAG/../../data/optimized_chunks/Libro-TF/002_Who_Should_Read_This_Book.pdf', 'num_pages': 2}", "page_content='What are the best practices for using Terraform as a team?\nThe only tools you need are a computer (Terraform runs on most operating\nsystems), an internet connection, and the desire to learn.\nWhy I Wrote This Book\nTerraform is a powerful tool. It works with all popular cloud providers. It\nuses a clean, simple language and has strong support for reuse, testing, and\nversioning. It’s open source and has a friendly, active community. But there\nis one area where it’s lacking: maturity.\nTerraform has become wildly popular, but it’s still a relatively new\ntechnology, and despite its popularity, it’s still difficult to find books, blog\nposts, or experts to help you become proficient with the tool. The official\nTerraform documentation does a good job of introducing the basic syntax\nand features, but it includes little information on idiomatic patterns, best\npractices, testing, reusability, or team workflows. It’s like trying to become\nfluent in French by studying only the vocabulary but not any of the\ngrammar or idioms.\nThe reason I wrote this book is to help developers become fluent in\nTerraform. I’ve been using Terraform for six out of the seven years it has\nexisted, mostly at my company, Gruntwork, where Terraform is one of the\ncore tools we’ve used to create a library of more than 300,000 lines of\nreusable, battle-tested infrastructure code that’s used in production by\nhundreds of companies. Writing and maintaining this much infrastructure\ncode over this many years and using it with so many different companies\nand use cases has taught us a lot of hard lessons. My goal is to share these\nlessons with you so that you can cut this lengthy process down and become\nfluent in a matter of days.\nOf course, you can’t become fluent just by reading. To become fluent in\nFrench, you need to spend time conversing with native French speakers,\nwatching French TV shows, and listening to French music. To become\nfluent in Terraform, you need to write real Terraform code, use it to manage' metadata={'original_pages_range': '9', 'source': '003_Why_I_Wrote_This_Book', 'file_path': '/mnt/c/Users/Carlos/Documents/Github/jupiter-iaa-azure/Scripts/RAG/../../data/optimized_chunks/Libro-TF/003_Why_I_Wrote_This_Book.pdf', 'num_pages': 1}", "page_content='real software, and deploy that software on real servers. Therefore, be ready\nto read, write, and execute a lot of code.\nWhat You Will Find in This Book\nHere’s an outline of what the book covers:\nChapter 1, “Why Terraform”\nHow DevOps is transforming the way we run software; an overview of\ninfrastructure-as-code tools, including configuration management,\nserver templating, orchestration, and provisioning tools; the benefits of\ninfrastructure as code; a comparison of Terraform, Chef, Puppet,\nAnsible, Pulumi, OpenStack Heat, and CloudFormation; how to\ncombine tools such as Terraform, Packer, Docker, Ansible, and\nKubernetes.\nChapter 2, “Getting Started with Terraform”\nInstalling Terraform; an overview of Terraform syntax; an overview of\nthe Terraform CLI tool; how to deploy a single server; how to deploy a\nweb server; how to deploy a cluster of web servers; how to deploy a\nload balancer; how to clean up resources you’ve created.\nChapter 3, “How to Manage Terraform State”\nWhat Terraform state is; how to store state so that multiple team\nmembers can access it; how to lock state files to prevent race\nconditions; how to isolate state files to limit the damage from errors;\nhow to use Terraform workspaces; a best-practices file and folder layout\nfor Terraform projects; how to use read-only state.\nChapter 4, “How to Create Reusable Infrastructure with Terraform\nModules”\nWhat modules are; how to create a basic module; how to make a\nmodule configurable with inputs and outputs; local values; versioned\n\nmodules; module gotchas; using modules to define reusable,\nconfigurable pieces of infrastructure.\nChapter 5, “Terraform Tips and Tricks: Loops, If-Statements, Deployment,\nand Gotchas”\nLoops with the count parameter, for_each and for expressions,\nand the for string directive; conditionals with the count parameter,\nfor_each and for expressions, and the if string directive; built-in\nfunctions; zero-downtime deployment; common Terraform gotchas and\npitfalls, including count and for_each limitations, zero-downtime\ndeployment gotchas, how valid plans can fail, and how to refactor\nTerraform code safely.\nChapter 6, “Managing Secrets with Terraform”\nAn introduction to secrets management; an overview of the different\ntypes of secrets, different ways to store secrets, and different ways to\naccess secrets; a comparison of common secret management tools such\nas HashiCorp Vault, AWS Secrets Manager, and Azure Key Vault; how\nto manage secrets when working with providers, including\nauthentication via environment variables, IAM roles, and OIDC; how to\nmanage secrets when working with resources and data sources,\nincluding how to use environment variables, encrypted files, and\ncentralized secret stores; how to securely handle state files and plan\nfiles.\nChapter 7, “Working with Multiple Providers”\nA closer look at how Terraform providers work, including how to install\nthem, how to control the version, and how to use them in your code;\nhow to use multiple copies of the same provider, including how to\ndeploy to multiple AWS regions, how to deploy to multiple AWS\naccounts, and how to build reusable modules that can use multiple\nproviders; how to use multiple different providers together, including an\n\nexample of using Terraform to run a Kubernetes cluster (EKS) in AWS\nand deploy Dockerized apps into the cluster.\nChapter 8, “Production-Grade Terraform Code”\nWhy DevOps projects always take longer than you expect; the\nproduction-grade infrastructure checklist; how to build Terraform\nmodules for production; small modules; composable modules; testable\nmodules; releasable modules; Terraform Registry; variable validation;\nversioning Terraform, Terraform providers, Terraform modules, and\nTerragrunt; Terraform escape hatches.\nChapter 9, “How to Test Terraform Code”\nManual tests for Terraform code; sandbox environments and cleanup;\nautomated tests for Terraform code; Terratest; unit tests; integration\ntests; end-to-end tests; dependency injection; running tests in parallel;\ntest stages; retries; the test pyramid; static analysis; plan testing; server\ntesting.\nChapter 10, “How to Use Terraform as a Team”\nHow to adopt Terraform as a team; how to convince your boss; a\nworkflow for deploying application code; a workflow for deploying\ninfrastructure code; version control; the golden rule of Terraform; code\nreviews; coding guidelines; Terraform style; CI/CD for Terraform; the\ndeployment process.\nFeel free to read the book from beginning to end or jump around to the\nchapters that interest you the most. Note that the examples in each chapter\nreference and build upon the examples from the previous chapters, so if you\nskip around, use the open source code examples (as described in “Open\nSource Code Examples”) to get your bearings. At the end of the book, in\nthe Appendix A, you’ll find a list of recommended reading where you can\nlearn more about Terraform, operations, IaC, and DevOps.' metadata={'original_pages_range': '10-12', 'source': '004_What_You_Will_Find_in_This_Book', 'file_path': '/mnt/c/Users/Carlos/Documents/Github/jupiter-iaa-azure/Scripts/RAG/../../data/optimized_chunks/Libro-TF/004_What_You_Will_Find_in_This_Book.pdf', 'num_pages': 3}", "page_content='Changes from the Second Edition to the\nThird Edition\nThe first edition of this book came out in 2017, the second edition came out\nin 2019, and although it’s hard for me to believe it, I’m now working on the\nthird edition in 2022. Time flies. It’s remarkable how much has changed\nover the years!\nIf you’ve read the second edition of the book and want to know what’s new,\nor if you’re just curious to see how Terraform has evolved between 2019\nand 2022, here are some of the highlights of what changed between the\nsecond and third editions:\nHundreds of pages of updated content\nThe third edition of the book is about a hundred pages longer than the\nsecond edition. I also estimate that roughly one-third to one-half of the\npages originally in the second edition were updated as well. Why so\nmuch churn? Well, Terraform went through six major releases since the\nsecond edition came out: 0.13, 0.14, 0.15, 1.0, 1.1, and 1.2. Moreover,\nmany Terraform providers went through major upgrades of their own,\nincluding the AWS Provider, which was at version 2 when the second\nedition came out and is now at version 4. Plus, the Terraform\ncommunity has seen massive growth over the last few years, which has\nled to the emergence of many new best practices, tools, and modules.\nI’ve tried to capture as much of this change as I could in the third\nedition, adding two completely new chapters and making major updates\nto all the existing chapters, as described next.\nNew provider functionality\nTerraform has significantly improved how you work with providers. In\nthe third edition, I’ve added an entirely new chapter, Chapter 7, that\ndescribes how to work with multiple providers: e.g., how to deploy into\nmultiple regions, multiple accounts, and multiple clouds. Also, by\npopular demand, this chapter includes a brand-new set of examples\nshowing how to use Terraform, Kubernetes, Docker, AWS, and EKS to\n\nrun containerized apps. Finally, I’ve also updated all the other chapters\nto highlight new provider features from the last several releases,\nincluding the required_providers block introduced in Terraform\n0.13, the lock file introduced in Terraform 0.14, and the\nconfiguration_aliases parameter introduced in Terraform 0.15.\nBetter secrets management\nWhen using Terraform code, you often have to deal with many types of\nsecrets: database passwords, API keys, cloud provider credentials, TLS\ncertificates, and so on. In the third edition, I added an entirely new\nchapter, Chapter 6, dedicated to this topic, including a comparison of\ncommon secret management tools, as well as lots of new example code\nthat shows a variety of techniques for securely using secrets with\nTerraform, including environment variables, encrypted files, centralized\nsecret stores, IAM roles, OIDC, and more.\nNew module functionality\nTerraform 0.13 added the ability to use count, for_each, and\ndepends_on on module blocks, making modules considerably more\npowerful, flexible, and reusable. You can find examples of how to use\nthese new features in Chapters 5 and 7.\nNew validation functionality\nIn Chapter 8, I’ve added examples of how to use the validation\nfeature introduced in Terraform 0.13 to perform basic checks on\nvariables (such as enforcing minimum or maximum values) and the\nprecondition and postcondition features introduced in\nTerraform 1.2 to perform basic checks on resources and data sources,\neither before running apply (such as enforcing that the AMI a user\nselected uses the x86_64 architecture) or after running apply (such as\nchecking that the EBS volume you’re using was successfully\nencrypted). In Chapter 6, I show how to use the sensitive parameter\n\nintroduced in Terraform 0.14 and 0.15, which ensures that secrets won’t\nbe logged when you run plan or apply.\nNew refactoring functionality\nTerraform 1.1 introduced the moved block, which provides a much\nbetter way to handle certain types of refactoring, such as renaming a\nresource. In the past, this type of refactoring required users to manually\nrun error-prone terraform state mv operations, whereas now, as\nyou’ll see in a new example in Chapter 5, this process can be fully\nautomated, making upgrades safer and more compatible.\nMore testing options\nThe tools available for automated testing of Terraform code continue to\nimprove. In Chapter 9, I’ve added example code and comparisons of\nstatic analysis tools for Terraform, including tfsec, tflint,\nterrascan, and the validate command; plan testing tools for\nTerraform, including Terratest, OPA, and Sentinel; and server testing\ntools, including inspec, serverspec, and goss. I also added a\ncomparison of all the testing approaches out there, so you can pick the\nbest ones for your use cases.\nImproved stability\nTerraform 1.0 was a big milestone for Terraform, not only signifying\nthat the tool had reached a certain level of maturity but also coming\nwith a number of compatibility promises. Namely, there is a promise\nthat all the 1.x releases will be backward compatible, so upgrading\nbetween v1.x releases should no longer require changes to your code,\nworkflows, or state files. Terraform state files are now cross-compatible\nwith Terraform 0.14, 0.15, and all 1.x releases, and Terraform remote\nstate data sources are cross-compatible with Terraform 0.12.30, 0.13.6,\n0.14.0, 0.15.0, and all 1.x releases. I’ve also updated Chapter 8 with\nexamples of how to better manage versioning of Terraform (including' metadata={'original_pages_range': '13-15', 'source': '005_Changes_from_the_Second_Edition_to_the_Third_Edition', 'file_path': '/mnt/c/Users/Carlos/Documents/Github/jupiter-iaa-azure/Scripts/RAG/../../data/optimized_chunks/Libro-TF/005_Changes_from_the_Second_Edition_to_the_Third_Edition.pdf', 'num_pages': 3}", "page_content='using tfenv), Terragrunt (including using tgswitch), and Terraform\nproviders (including how to use the lock file).\nImproved maturity\nTerraform has been downloaded over 100 million times, has had over\n1,500 open source contributors, and is in use at ~79% of Fortune 500\ncompanies, so it’s safe to say that the ecosystem has grown and\nmatured significantly over the last several years. There are now more\ndevelopers, providers, reusable modules, tools, plugins, classes, books,\nand tutorials for Terraform than ever before. Moreover, HashiCorp, the\ncompany that created Terraform, had its IPO (initial public offering) in\n2021, so Terraform is no longer backed by a small startup but by a large,\nstable, publicly traded company, for which Terraform is its biggest\nbusiness line.\nMany other changes\nThere were many other changes along the way, including the launch of\nTerraform Cloud (a web UI for using Terraform); the improved maturity\nof popular community tools such as Terragrunt, Terratest, and tfenv; the\naddition of many new provider features (including new ways to do zero-\ndowntime deployment, such as instance refresh, which I’ve added to\nChapter 5) and new functions (e.g., I added examples of how to use the\none function in Chapter 5 and the try function in Chapter 7); the\ndeprecation of many old features (e.g., template_file data source,\nmany aws_s3_bucket parameters, list and map, support for\nexternal references on destroy provisioners); and much more.\nChanges from the First Edition to the Second\nEdition\nGoing back in time even further, the second edition of the book added\nroughly 150 pages of new content on top of the first edition. Here is a\n1\n\nsummary of those changes, which also covers how Terraform changed\nbetween 2017 and 2019:\nFour major Terraform releases\nTerraform was at version 0.8 when the first edition came out; between\nthen and the time of the second edition, Terraform had four major\nreleases, all the way up to version 0.12. These releases introduced some\namazing new functionality, as I’ll describe shortly, as well as a fair\namount of upgrade work for users!\nAutomated testing improvements\nThe tooling and practices for writing automated tests for Terraform code\nevolved considerably between 2017 and 2019. In the second edition, I\nadded Chapter 9, a completely new chapter dedicated to testing,\ncovering topics such as unit tests, integration tests, end-to-end tests,\ndependency injection, test parallelism, static analysis, and more.\nModule improvements\nThe tooling and practices for creating Terraform modules also evolved\nconsiderably. In the second edition, I added Chapter 8, a new chapter\nthat contains a guide to building reusable, battle-tested, production-\ngrade Terraform modules—the kind of modules you’d bet your\ncompany on.\nWorkflow improvements\nChapter 10 was completely rewritten in the second edition to reflect the\nchanges in how teams integrate Terraform into their workflows,\nincluding a detailed guide on how to take application code and\ninfrastructure code from development through testing and all the way to\nproduction.\nHCL2\n2\n\nTerraform 0.12 overhauled the underlying language from HCL to\nHCL2. This included support for first-class expressions, rich type\nconstraints, lazily evaluated conditional expressions, support for null,\nfor_each and for expressions, dynamic inline blocks, and more. All\nthe code examples in the second edition of the book were updated to use\nHCL2, and the new language features were covered extensively in\nChapters 5 and 8.\nTerraform state revamp\nTerraform 0.9 introduced backends as a first-class way to store and\nshare Terraform state, including built-in support for locking. Terraform\n0.9 also introduced state environments as a way to manage deployments\nacross multiple environments. In Terraform 0.10, state environments\nwere replaced with Terraform workspaces. I cover all of these topics in\nChapter 3.\nTerraform providers split\nIn Terraform 0.10, the core Terraform code was split up from the code\nfor all the providers (i.e., the code for AWS, GCP, Azure, etc.). This\nallowed providers to be developed in their own repositories, at their\nown cadence, with their own versioning. However, you now must run\nterraform init to download the provider code every time you\nstart working with a new module, as discussed in Chapters 2 and 9.\nMassive provider growth\nFrom 2016 to 2019, Terraform grew from a handful of major cloud\nproviders (the usual suspects, such as AWS, GCP, and Azure) to more\nthan one hundred official providers and many more community\nproviders. This means that you can now use Terraform to not only\nmanage many other types of clouds (e.g., there are now providers for\nAlicloud, Oracle Cloud Infrastructure, VMware vSphere, and others)\nbut also to manage many other aspects of your world as code, including\nversion control systems with the GitHub, GitLab, and Bitbucket\n3\n\nproviders; data stores with the MySQL, PostgreSQL, and InfluxDB\nproviders; monitoring and alerting systems with the Datadog, New\nRelic, and Grafana providers; platform tools with the Kubernetes, Helm,\nHeroku, Rundeck, and RightScale providers; and much more.\nMoreover, each provider has much better coverage these days: AWS\nnow covers the majority of important AWS services and often adds\nsupport for new services even before CloudFormation does!\nTerraform Registry\nHashiCorp launched the Terraform Registry in 2017, a UI that made it\neasy to browse and consume open source, reusable Terraform modules\ncontributed by the community. In 2018, HashiCorp added the ability to\nrun a Private Terraform Registry within your own organization.\nTerraform 0.11 added first-class syntax support for consuming modules\nfrom a Terraform Registry. We look at the Registry in Chapter 8.\nBetter error handling\nTerraform 0.9 updated state error handling: if there was an error writing\nstate to a remote backend, the state would be saved locally in an\nerrored.tfstate file. Terraform 0.12 completely overhauled error\nhandling, by catching errors earlier, showing clearer error messages, and\nincluding the filepath, line number, and a code snippet in the error\nmessage.\nMany other changes\nThere were many other changes along the way, including the\nintroduction of local values (see “Module Locals”), new “escape\nhatches” for having Terraform interact with the outside world via scripts\n(see “Beyond Terraform Modules”), running plan as part of the\napply command, fixes for the create_before_destroy cycle\nissues, major improvements to the count parameter so that it can\ninclude references to data sources and resources, dozens of new built-in\nfunctions, an overhaul in provider inheritance, and much more.' metadata={'original_pages_range': '16-19', 'source': '006_Changes_from_the_First_Edition_to_the_Second_Edition', 'file_path': '/mnt/c/Users/Carlos/Documents/Github/jupiter-iaa-azure/Scripts/RAG/../../data/optimized_chunks/Libro-TF/006_Changes_from_the_First_Edition_to_the_Second_Edition.pdf', 'num_pages': 4}", "page_content='What You Won’t Find in This Book\nThis book is not meant to be an exhaustive reference manual for Terraform.\nI do not cover all of the cloud providers, or all of the resources supported by\neach cloud provider, or every available Terraform command. For these\nnitty-gritty details, I refer you instead to the Terraform documentation.\nThe documentation contains many useful answers, but if you’re new to\nTerraform, infrastructure as code, or operations, you won’t even know what\nquestions to ask. Therefore, this book is focused on what the documentation\ndoes not cover: namely, how to go beyond introductory examples and use\nTerraform in a real-world setting. My goal is to get you up and running\nquickly by discussing why you might want to use Terraform in the first\nplace, how to fit it into your workflow, and what practices and patterns tend\nto work best.\nTo demonstrate these patterns, I’ve included a number of code examples.\nI’ve tried to make it as easy as possible for you to try these examples at\nhome by minimizing dependencies on any third parties. This is why almost\nall the examples use just a single cloud provider, AWS, so that you need to\nsign up only for a single third-party service (also, AWS offers a generous\nfree tier, so running the example code shouldn’t cost you much). This is\nwhy the book and the example code do not cover or require HashiCorp’s\npaid services, Terraform Cloud or Terraform Enterprise. And this is why\nI’ve released all of the code examples as open source.\nOpen Source Code Examples\nYou can find all of the code samples in the book at the following URL:\nhttps://github.com/brikis98/terraform-up-and-running-code\nYou might want to check out this repo before you begin reading so you can\nfollow along with all the examples on your own computer:' metadata={'original_pages_range': '20', 'source': '007_What_You_Won’t_Find_in_This_Book', 'file_path': '/mnt/c/Users/Carlos/Documents/Github/jupiter-iaa-azure/Scripts/RAG/../../data/optimized_chunks/Libro-TF/007_What_You_Won’t_Find_in_This_Book.pdf', 'num_pages': 1}", "page_content='What You Won’t Find in This Book\nThis book is not meant to be an exhaustive reference manual for Terraform.\nI do not cover all of the cloud providers, or all of the resources supported by\neach cloud provider, or every available Terraform command. For these\nnitty-gritty details, I refer you instead to the Terraform documentation.\nThe documentation contains many useful answers, but if you’re new to\nTerraform, infrastructure as code, or operations, you won’t even know what\nquestions to ask. Therefore, this book is focused on what the documentation\ndoes not cover: namely, how to go beyond introductory examples and use\nTerraform in a real-world setting. My goal is to get you up and running\nquickly by discussing why you might want to use Terraform in the first\nplace, how to fit it into your workflow, and what practices and patterns tend\nto work best.\nTo demonstrate these patterns, I’ve included a number of code examples.\nI’ve tried to make it as easy as possible for you to try these examples at\nhome by minimizing dependencies on any third parties. This is why almost\nall the examples use just a single cloud provider, AWS, so that you need to\nsign up only for a single third-party service (also, AWS offers a generous\nfree tier, so running the example code shouldn’t cost you much). This is\nwhy the book and the example code do not cover or require HashiCorp’s\npaid services, Terraform Cloud or Terraform Enterprise. And this is why\nI’ve released all of the code examples as open source.\nOpen Source Code Examples\nYou can find all of the code samples in the book at the following URL:\nhttps://github.com/brikis98/terraform-up-and-running-code\nYou might want to check out this repo before you begin reading so you can\nfollow along with all the examples on your own computer:\n\ngit clone https://github.com/brikis98/terraform-up-and-running-\ncode.git\nThe code examples in that repo are in the code folder, and they are\norganized first by the tool or language (e.g., Terraform, Packer, OPA) and\nthen by chapter. The one exception is the Go code used for automated tests\nin Chapter 9, which lives in the terraform folder to follow the examples,\nmodules, and test folder layout recommended in that chapter. Table P-1\nshows a few examples of where to find different types of code examples in\nthe code samples repo.\nTable P-1. Where to find different types of code examples in the\ncode samples repo\nType of code Chapter Folder to look at in the samples repo\nTerraform Chapter 2 code/terraform/02-intro-to-terraform-syntax\nTerraform Chapter 5 code/terraform/05-tips-and-tricks\nPacker Chapter 1 code/packer/01-why-terraform\nOPA Chapter 9 code/opa/09-testing-terraform-code\nGo Chapter 9 code/terraform/09-testing-terraform-code/test\nIt’s worth noting that most of the examples show you what the code looks\nlike at the end of a chapter. If you want to maximize your learning, you’re\nbetter off writing the code yourself, from scratch, and checking the\n“official” solutions only at the very end.\nYou’ll begin writing code in Chapter 2, where you’ll learn how to use\nTerraform to deploy a basic cluster of web servers from scratch. After that,\nfollow the instructions in each subsequent chapter on how to develop and\nimprove this web server cluster example. Make the changes as instructed,\ntry to write all the code yourself, and use the sample code in the GitHub\nrepo only as a way to check your work or get yourself unstuck.' metadata={'original_pages_range': '20-21', 'source': '008_Open_Source_Code_Examples', 'file_path': '/mnt/c/Users/Carlos/Documents/Github/jupiter-iaa-azure/Scripts/RAG/../../data/optimized_chunks/Libro-TF/008_Open_Source_Code_Examples.pdf', 'num_pages': 2}", "page_content='A NOTE ABOUT VERSIONS\nAll of the examples in this book were tested against Terraform 1.x and\nAWS Provider 4.x, which were the most recent major releases as of this\nwriting. Because Terraform is a relatively new tool, it is possible that\nfuture releases will contain backward-incompatible changes and that\nsome of the best practices will change and evolve over time.\nI’ll try to release updates as often as I can, but the Terraform project\nmoves fast, so you’ll need to do some work to keep up with it on your\nown. For the latest news, blog posts, and talks on Terraform and\nDevOps, be sure to check out this book’s website and subscribe to the\nnewsletter!\nUsing the Code Examples\nIf you have a technical question or a problem using the code examples,\nplease send email to bookquestions@oreilly.com.\nThis book is here to help you get your job done. In general, if example code\nis offered with this book, you may use it in your programs and\ndocumentation. You do not need to contact us for permission unless you’re\nreproducing a significant portion of the code. For example, writing a\nprogram that uses several chunks of code from this book does not require\npermission. Selling or distributing examples from O’Reilly books does\nrequire permission. Answering a question by citing this book and quoting\nexample code does not require permission. Incorporating a significant\namount of example code from this book into your product’s documentation\ndoes require permission.\nWe appreciate, but generally do not require, attribution. An attribution\nusually includes the title, author, publisher, and ISBN. For example:\n“Terraform: Up and Running, Third Edition by Yevgeniy Brikman\n(O’Reilly). Copyright 2022 Yevgeniy Brikman, 978-1-098-11674-3.”' metadata={'original_pages_range': '22', 'source': '009_Using_the_Code_Examples', 'file_path': '/mnt/c/Users/Carlos/Documents/Github/jupiter-iaa-azure/Scripts/RAG/../../data/optimized_chunks/Libro-TF/009_Using_the_Code_Examples.pdf', 'num_pages': 1}", "page_content='If you feel your use of code examples falls outside fair use or the\npermission given above, feel free to contact O’Reilly Media at\npermissions@oreilly.com.\nConventions Used in This Book\nThe following typographical conventions are used in this book:\nItalic\nIndicates new terms, URLs, email addresses, filenames, and file\nextensions.\nConstant width\nUsed for program listings, as well as within paragraphs to refer to\nprogram elements such as variable or function names, databases, data\ntypes, environment variables, statements, and keywords.\nConstant width bold\nShows commands or other text that should be typed literally by the user.\nTIP\nThis element signifies a tip or suggestion.\nNOTE\nThis element signifies a general note.\nWARNING\nThis element indicates a warning or caution.' metadata={'original_pages_range': '23', 'source': '010_Conventions_Used_in_This_Book', 'file_path': '/mnt/c/Users/Carlos/Documents/Github/jupiter-iaa-azure/Scripts/RAG/../../data/optimized_chunks/Libro-TF/010_Conventions_Used_in_This_Book.pdf', 'num_pages': 1}", "page_content='O’Reilly Online Learning\nNOTE\nFor more than 40 years, O’Reilly Media has provided technology and business training,\nknowledge, and insight to help companies succeed.\nOur unique network of experts and innovators share their knowledge and\nexpertise through books, articles, and our online learning platform.\nO’Reilly’s online learning platform gives you on-demand access to live\ntraining courses, in-depth learning paths, interactive coding environments,\nand a vast collection of text and video from O’Reilly and 200+ other\npublishers. For more information, visit https://oreilly.com.\nHow to Contact O’Reilly Media\nPlease address comments and questions concerning this book to the\npublisher:\nO’Reilly Media, Inc.\n1005 Gravenstein Highway North\nSebastopol, CA 95472\n800-998-9938 (in the United States or Canada)\n707-829-0515 (international or local)\n707-829-0104 (fax)\nWe have a web page for this book, where we list errata, examples, and any\nadditional information. You can access this page at\nhttps://oreil.ly/terraform-UR3.' metadata={'original_pages_range': '24', 'source': '011_O’Reilly_Online_Learning_y_1_mas', 'file_path': '/mnt/c/Users/Carlos/Documents/Github/jupiter-iaa-azure/Scripts/RAG/../../data/optimized_chunks/Libro-TF/011_O’Reilly_Online_Learning_y_1_mas.pdf', 'num_pages': 1}", "page_content='Email bookquestions@oreilly.com to comment or ask technical questions\nabout this book.\nFor news and information about our books and courses, visit\nhttps://oreilly.com.\nFind us on LinkedIn: https://linkedin.com/company/oreilly-media.\nFollow us on Twitter: https://twitter.com/oreillymedia.\nWatch us on YouTube: https://youtube.com/oreillymedia.\nAcknowledgments\nJosh Padnick\nThis book would not have been possible without you. You were the one\nwho introduced me to Terraform in the first place, taught me all the\nbasics, and helped me figure out all the advanced parts. Thank you for\nsupporting me while I took our collective learnings and turned them into\na book. Thank you for being an awesome cofounder and making it\npossible to run a startup while still living a fun life. And thank you most\nof all for being a good friend and a good person.\nO’Reilly Media\nThank you for publishing another one of my books. Reading and\nwriting have profoundly transformed my life, and I’m proud to have\nyour help in sharing some of my writing with others. A special thanks to\nBrian Anderson, Virginia Wilson, and Corbin Collins for all your help\non the first, second, and third editions, respectively.\nGruntwork employees\nI can’t thank you all enough for (a) joining our tiny startup, (b) building\namazing software, (c) holding down the fort while I worked on the third\nedition of this book, and (d) being amazing colleagues and friends.\n\nGruntwork customers\nThank you for taking a chance on a small, unknown company and\nvolunteering to be guinea pigs for our Terraform experiments.\nGruntwork’s mission is to make it 10 times easier to understand,\ndevelop, and deploy software. We haven’t always succeeded at that\nmission (I’ve captured many of our mistakes in this book!), so I’m\ngrateful for your patience and willingness to be part of our audacious\nattempt to improve the world of software.\nHashiCorp\nThank you for building an amazing collection of DevOps tools,\nincluding Terraform, Packer, Consul, and Vault. You’ve improved the\nworld of DevOps and, with it, the lives of millions of software\ndevelopers.\nReviewers\nThank you to Kief Morris, Seth Vargo, Mattias Gees, Ricardo Ferreira,\nAkash Mahajan, Moritz Heiber, Taylor Dolezal, and Anton Babenko for\nreading early versions of this book and providing lots of detailed,\nconstructive feedback. Your suggestions have made this book\nsignificantly better.\nReaders of the first and second editions\nThose of you who bought the first and second editions of this book\nmade the third edition possible. Thank you. Your feedback, questions,\npull requests, and constant prodding for updates motivated a whole\nbunch of new and updated content. I hope you find the updates useful,\nand I’m looking forward to the continued prodding.\nMom, Dad, Larisa, Molly\nI accidentally wrote another book. That probably means I didn’t spend\nas much time with you as I wanted. Thank you for putting up with me\n\nanyway. I love you.\n1 Per the HashiCorp S1.\n2 Check out the Terraform upgrade guides for details.\n3 You can find the full list of Terraform providers in the Terraform Registry.' metadata={'original_pages_range': '25-27', 'source': '012_Acknowledgments', 'file_path': '/mnt/c/Users/Carlos/Documents/Github/jupiter-iaa-azure/Scripts/RAG/../../data/optimized_chunks/Libro-TF/012_Acknowledgments.pdf', 'num_pages': 3}", "page_content='Chapter 1. Why Terraform\nSoftware isn’t done when the code is working on your computer. It’s not\ndone when the tests pass. And it’s not done when someone gives you a\n“ship it” on a code review. Software isn’t done until you deliver it to the\nuser.\nSoftware delivery consists of all of the work you need to do to make the\ncode available to a customer, such as running that code on production\nservers, making the code resilient to outages and traffic spikes, and\nprotecting the code from attackers. Before you dive into the details of\nTerraform, it’s worth taking a step back to see where Terraform fits into the\nbigger picture of software delivery.\nIn this chapter, you’ll dive into the following topics:\nWhat is DevOps?\nWhat is infrastructure as code?\nWhat are the benefits of infrastructure as code?\nHow does Terraform work?\nHow does Terraform compare to other infrastructure-as-code tools?\nWhat Is DevOps?\nIn the not-so-distant past, if you wanted to build a software company, you\nalso needed to manage a lot of hardware. You would set up cabinets and\nracks, load them up with servers, hook up wiring, install cooling, build\nredundant power systems, and so on. It made sense to have one team,\ntypically called Developers (“Devs”), dedicated to writing the software, and\na separate team, typically called Operations (“Ops”), dedicated to managing\nthis hardware.' metadata={'original_pages_range': '28', 'source': '013_1._Why_Terraform', 'file_path': '/mnt/c/Users/Carlos/Documents/Github/jupiter-iaa-azure/Scripts/RAG/../../data/optimized_chunks/Libro-TF/013_1._Why_Terraform.pdf', 'num_pages': 1}", "page_content='Chapter 1. Why Terraform\nSoftware isn’t done when the code is working on your computer. It’s not\ndone when the tests pass. And it’s not done when someone gives you a\n“ship it” on a code review. Software isn’t done until you deliver it to the\nuser.\nSoftware delivery consists of all of the work you need to do to make the\ncode available to a customer, such as running that code on production\nservers, making the code resilient to outages and traffic spikes, and\nprotecting the code from attackers. Before you dive into the details of\nTerraform, it’s worth taking a step back to see where Terraform fits into the\nbigger picture of software delivery.\nIn this chapter, you’ll dive into the following topics:\nWhat is DevOps?\nWhat is infrastructure as code?\nWhat are the benefits of infrastructure as code?\nHow does Terraform work?\nHow does Terraform compare to other infrastructure-as-code tools?\nWhat Is DevOps?\nIn the not-so-distant past, if you wanted to build a software company, you\nalso needed to manage a lot of hardware. You would set up cabinets and\nracks, load them up with servers, hook up wiring, install cooling, build\nredundant power systems, and so on. It made sense to have one team,\ntypically called Developers (“Devs”), dedicated to writing the software, and\na separate team, typically called Operations (“Ops”), dedicated to managing\nthis hardware.\n\nThe typical Dev team would build an application and “toss it over the wall”\nto the Ops team. It was then up to Ops to figure out how to deploy and run\nthat application. Most of this was done manually. In part, that was\nunavoidable, because much of the work had to do with physically hooking\nup hardware (e.g., racking servers, hooking up network cables). But even\nthe work Ops did in software, such as installing the application and its\ndependencies, was often done by manually executing commands on a\nserver.\nThis works well for a while, but as the company grows, you eventually run\ninto problems. It typically plays out like this: because releases are done\nmanually, as the number of servers increases, releases become slow, painful,\nand unpredictable. The Ops team occasionally makes mistakes, so you end\nup with snowflake servers, wherein each one has a subtly different\nconfiguration from all the others (a problem known as configuration drift).\nAs a result, the number of bugs increases. Developers shrug and say, “It\nworks on my machine!” Outages and downtime become more frequent.\nThe Ops team, tired from their pagers going off at 3 a.m. after every release,\nreduce the release cadence to once per week. Then to once per month. Then\nonce every six months. Weeks before the biannual release, teams begin\ntrying to merge all of their projects together, leading to a huge mess of\nmerge conflicts. No one can stabilize the release branch. Teams begin\nblaming one another. Silos form. The company grinds to a halt.\nNowadays, a profound shift is taking place. Instead of managing their own\ndatacenters, many companies are moving to the cloud, taking advantage of\nservices such as Amazon Web Services (AWS), Microsoft Azure, and\nGoogle Cloud Platform (GCP). Instead of investing heavily in hardware,\nmany Ops teams are spending all their time working on software, using\ntools such as Chef, Puppet, Terraform, Docker, and Kubernetes. Instead of\nracking servers and plugging in network cables, many sysadmins are\nwriting code.\nAs a result, both Dev and Ops spend most of their time working on\nsoftware, and the distinction between the two teams is blurring. It might\n\nstill make sense to have a separate Dev team responsible for the application\ncode and an Ops team responsible for the operational code, but it’s clear\nthat Dev and Ops need to work more closely together. This is where the\nDevOps movement comes from.\nDevOps isn’t the name of a team or a job title or a particular technology.\nInstead, it’s a set of processes, ideas, and techniques. Everyone has a\nslightly different definition of DevOps, but for this book, I’m going to go\nwith the following:\nThe goal of DevOps is to make software delivery vastly more efficient.\nInstead of multiday merge nightmares, you integrate code continuously and\nalways keep it in a deployable state. Instead of deploying code once per\nmonth, you can deploy code dozens of times per day, or even after every\nsingle commit. And instead of constant outages and downtime, you build\nresilient, self-healing systems and use monitoring and alerting to catch\nproblems that can’t be resolved automatically.\nThe results from companies that have undergone DevOps transformations\nare astounding. For example, Nordstrom found that after applying DevOps\npractices to its organization, it was able to increase the number of features it\ndelivered per month by 100%, reduce defects by 50%, reduce lead times\n(the time from coming up with an idea to running code in production) by\n60%, and reduce the number of production incidents by 60% to 90%. After\nHP’s LaserJet Firmware division began using DevOps practices, the amount\nof time its developers spent on developing new features went from 5% to\n40%, and overall development costs were reduced by 40%. Etsy used\nDevOps practices to go from stressful, infrequent deployments that caused\nnumerous outages to deploying 25 to 50 times per day, with far fewer\noutages.\nThere are four core values in the DevOps movement: culture, automation,\nmeasurement, and sharing (sometimes abbreviated as the acronym CAMS).\nThis book is not meant as a comprehensive overview of DevOps (check out\nAppendix A for recommended reading), so I will just focus on one of these\nvalues: automation.\n1' metadata={'original_pages_range': '28-30', 'source': '014_What_Is_DevOps', 'file_path': '/mnt/c/Users/Carlos/Documents/Github/jupiter-iaa-azure/Scripts/RAG/../../data/optimized_chunks/Libro-TF/014_What_Is_DevOps.pdf', 'num_pages': 3}", "page_content='The goal is to automate as much of the software delivery process as\npossible. That means that you manage your infrastructure not by clicking\naround a web page or manually executing shell commands, but through\ncode. This is a concept that is typically called infrastructure as code.\nWhat Is Infrastructure as Code?\nThe idea behind infrastructure as code (IaC) is that you write and execute\ncode to define, deploy, update, and destroy your infrastructure. This\nrepresents an important shift in mindset in which you treat all aspects of\noperations as software—even those aspects that represent hardware (e.g.,\nsetting up physical servers). In fact, a key insight of DevOps is that you can\nmanage almost everything in code, including servers, databases, networks,\nlogfiles, application configuration, documentation, automated tests,\ndeployment processes, and so on.\nThere are five broad categories of IaC tools:\nAd hoc scripts\nConfiguration management tools\nServer templating tools\nOrchestration tools\nProvisioning tools\nLet’s look at these one at a time.\nAd Hoc Scripts\nThe most straightforward approach to automating anything is to write an ad\nhoc script. You take whatever task you were doing manually, break it down\ninto discrete steps, use your favorite scripting language (e.g., Bash, Ruby,\nPython) to define each of those steps in code, and execute that script on\nyour server, as shown in Figure 1-1.' metadata={'original_pages_range': '31', 'source': '015_What_Is_Infrastructure_as_Code', 'file_path': '/mnt/c/Users/Carlos/Documents/Github/jupiter-iaa-azure/Scripts/RAG/../../data/optimized_chunks/Libro-TF/015_What_Is_Infrastructure_as_Code.pdf', 'num_pages': 1}", "page_content='The goal is to automate as much of the software delivery process as\npossible. That means that you manage your infrastructure not by clicking\naround a web page or manually executing shell commands, but through\ncode. This is a concept that is typically called infrastructure as code.\nWhat Is Infrastructure as Code?\nThe idea behind infrastructure as code (IaC) is that you write and execute\ncode to define, deploy, update, and destroy your infrastructure. This\nrepresents an important shift in mindset in which you treat all aspects of\noperations as software—even those aspects that represent hardware (e.g.,\nsetting up physical servers). In fact, a key insight of DevOps is that you can\nmanage almost everything in code, including servers, databases, networks,\nlogfiles, application configuration, documentation, automated tests,\ndeployment processes, and so on.\nThere are five broad categories of IaC tools:\nAd hoc scripts\nConfiguration management tools\nServer templating tools\nOrchestration tools\nProvisioning tools\nLet’s look at these one at a time.\nAd Hoc Scripts\nThe most straightforward approach to automating anything is to write an ad\nhoc script. You take whatever task you were doing manually, break it down\ninto discrete steps, use your favorite scripting language (e.g., Bash, Ruby,\nPython) to define each of those steps in code, and execute that script on\nyour server, as shown in Figure 1-1.\n\nFigure 1-1. The most straightforward way to automate things is to create an ad hoc script that you\nrun on your servers.\nFor example, here is a Bash script called setup-webserver.sh that configures\na web server by installing dependencies, checking out some code from a Git\nrepo, and firing up an Apache web server:' metadata={'original_pages_range': '31-32', 'source': '016_Ad_Hoc_Scripts', 'file_path': '/mnt/c/Users/Carlos/Documents/Github/jupiter-iaa-azure/Scripts/RAG/../../data/optimized_chunks/Libro-TF/016_Ad_Hoc_Scripts.pdf', 'num_pages': 2}", "page_content='# Update the apt-get cache \nsudo apt-get update \n \n# Install PHP and Apache \nsudo apt-get install -y php apache2 \n \n# Copy the code from the repository \nsudo git clone https://github.com/brikis98/php-app.git \n/var/www/html/app \n \n# Start Apache \nsudo service apache2 start\nThe great thing about ad hoc scripts is that you can use popular, general-\npurpose programming languages, and you can write the code however you\nwant. The terrible thing about ad hoc scripts is that you can use popular,\ngeneral-purpose programming languages, and you can write the code\nhowever you want.\nWhereas tools that are purpose-built for IaC provide concise APIs for\naccomplishing complicated tasks, if you’re using a general-purpose\nprogramming language, you need to write completely custom code for\nevery task. Moreover, tools designed for IaC usually enforce a particular\nstructure for your code, whereas with a general-purpose programming\nlanguage, each developer will use their own style and do something\ndifferent. Neither of these problems is a big deal for an eight-line script that\ninstalls Apache, but it gets messy if you try to use ad hoc scripts to manage\ndozens of servers, databases, load balancers, network configurations, and so\non.\nIf you’ve ever had to maintain a large repository of Bash scripts, you know\nthat it almost always devolves into a mess of unmaintainable spaghetti\ncode. Ad hoc scripts are great for small, one-off tasks, but if you’re going to\nbe managing all of your infrastructure as code, then you should use an IaC\ntool that is purpose-built for the job.\nConfiguration Management Tools\n\nChef, Puppet, and Ansible are all configuration management tools, which\nmeans that they are designed to install and manage software on existing\nservers. For example, here is an Ansible role called web-server.yml that\nconfigures the same Apache web server as the setup-webserver.sh script:\n- name: Update the apt-get cache\n  apt:\n    update_cache: yes \n \n- name: Install PHP\n  apt:\n    name: php \n \n- name: Install Apache\n  apt:\n    name: apache2 \n \n- name: Copy the code from the repository\n  git: repo=https://github.com/brikis98/php-app.git \ndest=/var/www/html/app \n \n- name: Start Apache\n  service: name=apache2 state=started enabled=yes\nThe code looks similar to the Bash script, but using a tool like Ansible\noffers a number of advantages:\nCoding conventions\nAnsible enforces a consistent, predictable structure, including\ndocumentation, file layout, clearly named parameters, secrets\nmanagement, and so on. While every developer organizes their ad hoc\nscripts in a different way, most configuration management tools come\nwith a set of conventions that makes it easier to navigate the code.\nIdempotence\nWriting an ad hoc script that works once isn’t too difficult; writing an ad\nhoc script that works correctly even if you run it over and over again is\nmuch harder. Every time you go to create a folder in your script, you\nneed to remember to check whether that folder already exists; every\n\ntime you add a line of configuration to a file, you need to check that line\ndoesn’t already exist; every time you want to run an app, you need to\ncheck that the app isn’t already running.\nCode that works correctly no matter how many times you run it is called\nidempotent code. To make the Bash script from the previous section\nidempotent, you’d need to add many lines of code, including lots of if-\nstatements. Most Ansible functions, on the other hand, are idempotent\nby default. For example, the web-server.yml Ansible role will install\nApache only if it isn’t installed already and will try to start the Apache\nweb server only if it isn’t running already.\nDistribution\nAd hoc scripts are designed to run on a single, local machine. Ansible\nand other configuration management tools are designed specifically for\nmanaging large numbers of remote servers, as shown in Figure 1-2.\n\nFigure 1-2. A configuration management tool like Ansible can execute your code across a large\nnumber of servers.\nFor example, to apply the web-server.yml role to five servers, you first\ncreate a file called hosts that contains the IP addresses of those servers:\n[webservers] \n11.11.11.11 \n11.11.11.12 \n11.11.11.13' metadata={'original_pages_range': '33-36', 'source': '017_Configuration_Management_Tools', 'file_path': '/mnt/c/Users/Carlos/Documents/Github/jupiter-iaa-azure/Scripts/RAG/../../data/optimized_chunks/Libro-TF/017_Configuration_Management_Tools.pdf', 'num_pages': 4}", "page_content='11.11.11.14 \n11.11.11.15\nNext, you define the following Ansible playbook:\n- hosts: webservers\n  roles:\n  - webserver\nFinally, you execute the playbook as follows:\nansible-playbook playbook.yml\nThis instructs Ansible to configure all five servers in parallel.\nAlternatively, by setting a parameter called serial in the playbook,\nyou can do a rolling deployment, which updates the servers in batches.\nFor example, setting serial to 2 directs Ansible to update two of the\nservers at a time, until all five are done. Duplicating any of this logic in\nan ad hoc script would take dozens or even hundreds of lines of code.\nServer Templating Tools\nAn alternative to configuration management that has been growing in\npopularity recently are server templating tools such as Docker, Packer, and\nVagrant. Instead of launching a bunch of servers and configuring them by\nrunning the same code on each one, the idea behind server templating tools\nis to create an image of a server that captures a fully self-contained\n“snapshot” of the operating system (OS), the software, the files, and all\nother relevant details. You can then use some other IaC tool to install that\nimage on all of your servers, as shown in Figure 1-3.\n\n\n\nFigure 1-3. You can use a server templating tool like Packer to create a self-contained image of a\nserver. You can then use other tools, such as Ansible, to install that image across all of your servers.\nThere are two broad categories of tools for working with images (Figure 1-\n4):\nVirtual machines\nA virtual machine (VM) emulates an entire computer system, including\nthe hardware. You run a hypervisor, such as VMware, VirtualBox, or\nParallels, to virtualize (i.e., simulate) the underlying CPU, memory,\nhard drive, and networking.\nThe benefit of this is that any VM image that you run on top of the\nhypervisor can see only the virtualized hardware, so it’s fully isolated\nfrom the host machine and any other VM images, and it will run exactly\nthe same way in all environments (e.g., your computer, a QA server, a\nproduction server). The drawback is that virtualizing all this hardware\nand running a totally separate OS for each VM incurs a lot of overhead\nin terms of CPU usage, memory usage, and startup time. You can define\nVM images as code using tools such as Packer and Vagrant.\nContainers\nA container emulates the user space of an OS. You run a container\nengine, such as Docker, CoreOS rkt, or cri-o, to create isolated\nprocesses, memory, mount points, and networking.\nThe benefit of this is that any container you run on top of the container\nengine can see only its own user space, so it’s isolated from the host\nmachine and other containers and will run exactly the same way in all\nenvironments (your computer, a QA server, a production server, etc.).\nThe drawback is that all of the containers running on a single server\nshare that server’s OS kernel and hardware, so it’s much more difficult\nto achieve the level of isolation and security you get with a VM.\nHowever, because the kernel and hardware are shared, your containers\ncan boot up in milliseconds and have virtually no CPU or memory\noverhead. You can define container images as code using tools such as\n2\n3\n\nDocker and CoreOS rkt; you’ll see an example of how to use Docker in\nChapter 7.\n\n\n\nFigure 1-4. The two main types of images: VMs, on the left, and containers, on the right. VMs\nvirtualize the hardware, whereas containers virtualize only user space.\nFor example, here is a Packer template called web-server.json that creates\nan Amazon Machine Image (AMI), which is a VM image that you can run\non AWS:\n{\n  \"builders\": [{\n    \"ami_name\": \"packer-example-\",\n    \"instance_type\": \"t2.micro\",\n    \"region\": \"us-east-2\",\n    \"type\": \"amazon-ebs\",\n    \"source_ami\": \"ami-0fb653ca2d3203ac1\",\n    \"ssh_username\": \"ubuntu\"\n  }],\n  \"provisioners\": [{\n    \"type\": \"shell\",\n    \"inline\": [\n      \"sudo apt-get update\",\n      \"sudo apt-get install -y php apache2\",\n      \"sudo git clone https://github.com/brikis98/php-app.git \n/var/www/html/app\"\n    ],\n    \"environment_vars\": [\n      \"DEBIAN_FRONTEND=noninteractive\"\n    ],\n    \"pause_before\": \"60s\"\n  }]\n}\nThis Packer template configures the same Apache web server that you saw\nin setup-webserver.sh using the same Bash code. The only difference\nbetween the code in the Packer template and the previous examples is that\nthis Packer template does not start the Apache web server (e.g., by calling\nsudo service apache2 start). That’s because server templates\nare typically used to install software in images, but it’s only when you run\nthe image—for example, by deploying it on a server—that you should\nactually run that software.\nTo build an AMI from this template, run packer build\nwebserver.json. After the build completes, you can install that AMI' metadata={'original_pages_range': '37-42', 'source': '018_Server_Templating_Tools', 'file_path': '/mnt/c/Users/Carlos/Documents/Github/jupiter-iaa-azure/Scripts/RAG/../../data/optimized_chunks/Libro-TF/018_Server_Templating_Tools.pdf', 'num_pages': 6}", "page_content='on all of your AWS servers and configure each server to run Apache when\nthe server is booting (you’ll see an example of this in the next section), and\nthey will all run exactly the same way.\nNote that the different server templating tools have slightly different\npurposes. Packer is typically used to create images that you run directly on\ntop of production servers, such as an AMI that you run in your production\nAWS account. Vagrant is typically used to create images that you run on\nyour development computers, such as a VirtualBox image that you run on\nyour Mac or Windows laptop. Docker is typically used to create images of\nindividual applications. You can run the Docker images on production or\ndevelopment computers, as long as some other tool has configured that\ncomputer with the Docker Engine. For example, a common pattern is to use\nPacker to create an AMI that has the Docker Engine installed, deploy that\nAMI on a cluster of servers in your AWS account, and then deploy\nindividual Docker containers across that cluster to run your applications.\nServer templating is a key component of the shift to immutable\ninfrastructure. This idea is inspired by functional programming, where\nvariables are immutable, so after you’ve set a variable to a value, you can\nnever change that variable again. If you need to update something, you\ncreate a new variable. Because variables never change, it’s a lot easier to\nreason about your code.\nThe idea behind immutable infrastructure is similar: once you’ve deployed\na server, you never make changes to it again. If you need to update\nsomething, such as deploying a new version of your code, you create a new\nimage from your server template and you deploy it on a new server.\nBecause servers never change, it’s a lot easier to reason about what’s\ndeployed.\nOrchestration Tools\nServer templating tools are great for creating VMs and containers, but how\ndo you actually manage them? For most real-world use cases, you’ll need a\nway to do the following:\n\nDeploy VMs and containers, making efficient use of your hardware.\nRoll out updates to an existing fleet of VMs and containers using\nstrategies such as rolling deployment, blue-green deployment, and\ncanary deployment.\nMonitor the health of your VMs and containers and automatically\nreplace unhealthy ones (auto healing).\nScale the number of VMs and containers up or down in response to\nload (auto scaling).\nDistribute traffic across your VMs and containers (load balancing).\nAllow your VMs and containers to find and talk to one another over\nthe network (service discovery).\nHandling these tasks is the realm of orchestration tools such as Kubernetes,\nMarathon/Mesos, Amazon Elastic Container Service (Amazon ECS),\nDocker Swarm, and Nomad. For example, Kubernetes allows you to define\nhow to manage your Docker containers as code. You first deploy a\nKubernetes cluster, which is a group of servers that Kubernetes will\nmanage and use to run your Docker containers. Most major cloud providers\nhave native support for deploying managed Kubernetes clusters, such as\nAmazon Elastic Kubernetes Service (EKS), Google Kubernetes Engine\n(GKE), and Azure Kubernetes Service (AKS).\nOnce you have a working cluster, you can define how to run your Docker\ncontainer as code in a YAML file:\napiVersion: apps/v1 \n \n# Use a Deployment to deploy multiple replicas of your Docker\n# container(s) and to declaratively roll out updates to them\nkind: Deployment \n \n# Metadata about this Deployment, including its name\nmetadata:\n  name: example-app \n \n# The specification that configures this Deployment\n\nspec:\n  # This tells the Deployment how to find your container(s)\n  selector:\n    matchLabels:\n      app: example-app \n \n  # This tells the Deployment to run three replicas of your\n  # Docker container(s)\n  replicas: 3 \n \n  # Specifies how to update the Deployment. Here, we\n  # configure a rolling update.\n  strategy:\n    rollingUpdate:\n      maxSurge: 3\n      maxUnavailable: 0\n    type: RollingUpdate \n \n  # This is the template for what container(s) to deploy\n  template: \n \n    # The metadata for these container(s), including labels\n    metadata:\n      labels:\n        app: example-app \n \n    # The specification for your container(s)\n    spec:\n      containers: \n \n        # Run Apache listening on port 80\n        - name: example-app\n          image: httpd:2.4.39\n          ports:\n            - containerPort: 80\nThis file instructs Kubernetes to create a Deployment, which is a declarative\nway to define the following:\nOne or more Docker containers to run together. This group of\ncontainers is called a Pod. The Pod defined in the preceding code\ncontains a single Docker container that runs Apache.\nThe settings for each Docker container in the Pod. The Pod in the\npreceding code configures Apache to listen on port 80.' metadata={'original_pages_range': '43-45', 'source': '019_Orchestration_Tools', 'file_path': '/mnt/c/Users/Carlos/Documents/Github/jupiter-iaa-azure/Scripts/RAG/../../data/optimized_chunks/Libro-TF/019_Orchestration_Tools.pdf', 'num_pages': 3}", "page_content='How many copies (aka replicas) of the Pod to run in your cluster. The\npreceding code configures three replicas. Kubernetes automatically\nfigures out where in your cluster to deploy each Pod, using a\nscheduling algorithm to pick the optimal servers in terms of high\navailability (e.g., try to run each Pod on a separate server so a single\nserver crash doesn’t take down your app), resources (e.g., pick servers\nthat have available the ports, CPU, memory, and other resources\nrequired by your containers), performance (e.g., try to pick servers\nwith the least load and fewest containers on them), and so on.\nKubernetes also constantly monitors the cluster to ensure that there are\nalways three replicas running, automatically replacing any Pods that\ncrash or stop responding.\nHow to deploy updates. When deploying a new version of the Docker\ncontainer, the preceding code rolls out three new replicas, waits for\nthem to be healthy, and then undeploys the three old replicas.\nThat’s a lot of power in just a few lines of YAML! You run kubectl\napply -f example-app.yml to instruct Kubernetes to deploy your\napp. You can then make changes to the YAML file and run kubectl\napply again to roll out the updates. You can also manage both the\nKubernetes cluster and the apps within it using Terraform; you’ll see an\nexample of this in Chapter 7.\nProvisioning Tools\nWhereas configuration management, server templating, and orchestration\ntools define the code that runs on each server, provisioning tools such as\nTerraform, CloudFormation, OpenStack Heat, and Pulumi are responsible\nfor creating the servers themselves. In fact, you can use provisioning tools\nto create not only servers but also databases, caches, load balancers, queues,\nmonitoring, subnet configurations, firewall settings, routing rules, Secure\nSockets Layer (SSL) certificates, and almost every other aspect of your\ninfrastructure, as shown in Figure 1-5.\nFor example, the following code deploys a web server using Terraform:\n\nresource \"aws_instance\" \"app\" {\n  instance_type     = \"t2.micro\"\n  availability_zone = \"us-east-2a\"\n  ami               = \"ami-0fb653ca2d3203ac1\" \n \n  user_data = <<-EOF\n              #!/bin/bash \n              sudo service apache2 start \n              EOF\n}\nDon’t worry if you’re not yet familiar with some of the syntax. For now,\njust focus on two parameters:\nami\nThis parameter specifies the ID of an AMI to deploy on the server. You\ncould set this parameter to the ID of an AMI built from the web-\nserver.json Packer template in the previous section, which has PHP,\nApache, and the application source code.\nuser_data\nThis is a Bash script that executes when the web server is booting. The\npreceding code uses this script to boot up Apache.\nIn other words, this code shows you provisioning and server templating\nworking together, which is a common pattern in immutable infrastructure.\n\n' metadata={'original_pages_range': '46-48', 'source': '020_Provisioning_Tools', 'file_path': '/mnt/c/Users/Carlos/Documents/Github/jupiter-iaa-azure/Scripts/RAG/../../data/optimized_chunks/Libro-TF/020_Provisioning_Tools.pdf', 'num_pages': 3}", "page_content='Figure 1-5. You can use provisioning tools with your cloud provider to create servers, databases,\nload balancers, and all other parts of your infrastructure.\nWhat Are the Benefits of Infrastructure as\nCode?\nNow that you’ve seen all the different flavors of IaC, a good question to ask\nis, why bother? Why learn a bunch of new languages and tools and\nencumber yourself with yet more code to manage?\nThe answer is that code is powerful. In exchange for the upfront investment\nof converting your manual practices to code, you get dramatic\nimprovements in your ability to deliver software. According to the 2016\nState of DevOps Report, organizations that use DevOps practices, such as\nIaC, deploy 200 times more frequently, recover from failures 24 times\nfaster, and have lead times that are 2,555 times lower.\nWhen your infrastructure is defined as code, you are able to use a wide\nvariety of software engineering practices to dramatically improve your\nsoftware delivery process, including the following:\nSelf-service\nMost teams that deploy code manually have a small number of\nsysadmins (often, just one) who are the only ones who know all the\nmagic incantations to make the deployment work and are the only ones\nwith access to production. This becomes a major bottleneck as the\ncompany grows. If your infrastructure is defined in code, the entire\ndeployment process can be automated, and developers can kick off their\nown deployments whenever necessary.\nSpeed and safety\nIf the deployment process is automated, it will be significantly faster,\nsince a computer can carry out the deployment steps far faster than a\nperson, and safer, given that an automated process will be more\nconsistent, more repeatable, and not prone to manual error.\n\nDocumentation\nIf the state of your infrastructure is locked away in a single sysadmin’s\nhead, and that sysadmin goes on vacation or leaves the company or gets\nhit by a bus, you may suddenly realize you can no longer manage your\nown infrastructure. On the other hand, if your infrastructure is defined\nas code, then the state of your infrastructure is in source files that\nanyone can read. In other words, IaC acts as documentation, allowing\neveryone in the organization to understand how things work, even if the\nsysadmin goes on vacation.\nVersion control\nYou can store your IaC source files in version control, which means that\nthe entire history of your infrastructure is now captured in the commit\nlog. This becomes a powerful tool for debugging issues, because any\ntime a problem pops up, your first step will be to check the commit log\nand find out what changed in your infrastructure, and your second step\nmight be to resolve the problem by simply reverting back to a previous,\nknown-good version of your IaC code.\nValidation\nIf the state of your infrastructure is defined in code, for every single\nchange, you can perform a code review, run a suite of automated tests,\nand pass the code through static analysis tools—all practices that are\nknown to significantly reduce the chance of defects.\nReuse\nYou can package your infrastructure into reusable modules so that\ninstead of doing every deployment for every product in every\nenvironment from scratch, you can build on top of known, documented,\nbattle-tested pieces.\nHappiness\n4\n5' metadata={'original_pages_range': '49-50', 'source': '021_What_Are_the_Benefits_of_Infrastructure_as_Code', 'file_path': '/mnt/c/Users/Carlos/Documents/Github/jupiter-iaa-azure/Scripts/RAG/../../data/optimized_chunks/Libro-TF/021_What_Are_the_Benefits_of_Infrastructure_as_Code.pdf', 'num_pages': 2}", "page_content='There is one other very important, and often overlooked, reason for why\nyou should use IaC: happiness. Deploying code and managing\ninfrastructure manually is repetitive and tedious. Developers and\nsysadmins resent this type of work, since it involves no creativity, no\nchallenge, and no recognition. You could deploy code perfectly for\nmonths, and no one will take notice—until that one day when you mess\nit up. That creates a stressful and unpleasant environment. IaC offers a\nbetter alternative that allows computers to do what they do best\n(automation) and developers to do what they do best (coding).\nNow that you have a sense of why IaC is important, the next question is\nwhether Terraform is the best IaC tool for you. To answer that, I’m first\ngoing to go through a very quick primer on how Terraform works, and then\nI’ll compare it to the other popular IaC options out there, such as Chef,\nPuppet, and Ansible.\nHow Does Terraform Work?\nHere is a high-level and somewhat simplified view of how Terraform\nworks. Terraform is an open source tool created by HashiCorp and written\nin the Go programming language. The Go code compiles down into a single\nbinary (or rather, one binary for each of the supported operating systems)\ncalled, not surprisingly, terraform.\nYou can use this binary to deploy infrastructure from your laptop or a build\nserver or just about any other computer, and you don’t need to run any extra\ninfrastructure to make that happen. That’s because under the hood, the\nterraform binary makes API calls on your behalf to one or more\nproviders, such as AWS, Azure, Google Cloud, DigitalOcean, OpenStack,\nand more. This means that Terraform gets to leverage the infrastructure\nthose providers are already running for their API servers, as well as the\nauthentication mechanisms you’re already using with those providers (e.g.,\nthe API keys you already have for AWS).\n\nHow does Terraform know what API calls to make? The answer is that you\ncreate Terraform configurations, which are text files that specify what\ninfrastructure you want to create. These configurations are the “code” in\n“infrastructure as code.” Here’s an example Terraform configuration:\nresource \"aws_instance\" \"example\" {\n  ami           = \"ami-0fb653ca2d3203ac1\"\n  instance_type = \"t2.micro\"\n} \n \nresource \"google_dns_record_set\" \"a\" {\n  name         = \"demo.google-example.com\"\n  managed_zone = \"example-zone\"\n  type         = \"A\"\n  ttl          = 300\n  rrdatas      = [aws_instance.example.public_ip]\n}\nEven if you’ve never seen Terraform code before, you shouldn’t have too\nmuch trouble reading it. This snippet instructs Terraform to make API calls\nto AWS to deploy a server, and then make API calls to Google Cloud to\ncreate a Domain Name System (DNS) entry pointing to the AWS server’s\nIP address. In just a single, simple syntax (which you’ll learn in Chapter 2),\nTerraform allows you to deploy interconnected resources across multiple\ncloud providers.\nYou can define your entire infrastructure—servers, databases, load\nbalancers, network topology, and so on—in Terraform configuration files\nand commit those files to version control. You then run certain Terraform\ncommands, such as terraform apply, to deploy that infrastructure.\nThe terraform binary parses your code, translates it into a series of API\ncalls to the cloud providers specified in the code, and makes those API calls\nas efficiently as possible on your behalf, as shown in Figure 1-6.\n\nFigure 1-6. Terraform is a binary that translates the contents of your configurations into API calls to\ncloud providers.\nWhen someone on your team needs to make changes to the infrastructure,\ninstead of updating the infrastructure manually and directly on the servers,\nthey make their changes in the Terraform configuration files, validate those\nchanges through automated tests and code reviews, commit the updated\ncode to version control, and then run the terraform apply command\nto have Terraform make the necessary API calls to deploy the changes.' metadata={'original_pages_range': '51-53', 'source': '022_How_Does_Terraform_Work', 'file_path': '/mnt/c/Users/Carlos/Documents/Github/jupiter-iaa-azure/Scripts/RAG/../../data/optimized_chunks/Libro-TF/022_How_Does_Terraform_Work.pdf', 'num_pages': 3}", "page_content='TRANSPARENT PORTABILITY BETWEEN CLOUDPROVIDERS\nBecause Terraform supports many different cloud providers, a common question that\narises is whether it supports transparent portability between them. For example, if you\nused Terraform to define a bunch of servers, databases, load balancers, and other\ninfrastructure in AWS, could you instruct Terraform to deploy exactly the same\ninfrastructure in another cloud provider, such as Azure or Google Cloud, in just a few\ncommands?\nThis question turns out to be a bit of a red herring. The reality is that you can’t deploy\n“exactly the same infrastructure” in a different cloud provider because the cloud\nproviders don’t offer the same types of infrastructure! The servers, load balancers, and\ndatabases offered by AWS are very different from those in Azure and Google Cloud in\nterms of features, configuration, management, security, scalability, availability,\nobservability, and so on. There is no easy way to “transparently” paper over these\ndifferences, especially as functionality in one cloud provider often doesn’t exist at all in\nthe others. Terraform’s approach is to allow you to write code that is specific to each\nprovider, taking advantage of that provider’s unique functionality, but to use the same\nlanguage, toolset, and IaC practices under the hood for all providers.\nHow Does Terraform Compare to Other IaC\nTools?\nInfrastructure as code is wonderful, but the process of picking an IaC tool is\nnot. Many of the IaC tools overlap in what they do. Many of them are open\nsource. Many of them offer commercial support. Unless you’ve used each\none yourself, it’s not clear what criteria you should use to pick one or the\nother.\nWhat makes this even more difficult is that most of the comparisons you\nfind between these tools do little more than list the general properties of\neach one and make it sound as if you could be equally successful with any\nof them. And although that’s technically true, it’s not helpful. It’s a bit like\ntelling a programming newbie that you could be equally successful building\na website with PHP, C, or assembly—a statement that’s technically true but\none that omits a huge amount of information that is essential for making a\ngood decision.' metadata={'original_pages_range': '54', 'source': '023_How_Does_Terraform_Compare_to_Other_IaC_Tools', 'file_path': '/mnt/c/Users/Carlos/Documents/Github/jupiter-iaa-azure/Scripts/RAG/../../data/optimized_chunks/Libro-TF/023_How_Does_Terraform_Compare_to_Other_IaC_Tools.pdf', 'num_pages': 1}", "page_content='In the following sections, I’m going to do a detailed comparison between\nthe most popular configuration management and provisioning tools:\nTerraform, Chef, Puppet, Ansible, Pulumi, CloudFormation, and OpenStack\nHeat. My goal is to help you decide whether Terraform is a good choice by\nexplaining why my company, Gruntwork, picked Terraform as our IaC tool\nof choice and, in some sense, why I wrote this book. As with all\ntechnology decisions, it’s a question of trade-offs and priorities, and even\nthough your particular priorities might be different than mine, my hope is\nthat sharing this thought process will help you to make your own decision.\nHere are the main trade-offs to consider:\nConfiguration management versus provisioning\nMutable infrastructure versus immutable infrastructure\nProcedural language versus declarative language\nGeneral-purpose language versus domain-specific language\nMaster versus masterless\nAgent versus agentless\nPaid versus free offering\nLarge community versus small community\nMature versus cutting-edge\nUse of multiple tools together\nConfiguration Management Versus Provisioning\nAs you saw earlier, Chef, Puppet, and Ansible are all configuration\nmanagement tools, whereas CloudFormation, Terraform, OpenStack Heat,\nand Pulumi are all provisioning tools.\nAlthough the distinction is not entirely clear cut, given that configuration\nmanagement tools can typically do some degree of provisioning (e.g., you\n6' metadata={'original_pages_range': '55', 'source': '024_Configuration_Management_Versus_Provisioning', 'file_path': '/mnt/c/Users/Carlos/Documents/Github/jupiter-iaa-azure/Scripts/RAG/../../data/optimized_chunks/Libro-TF/024_Configuration_Management_Versus_Provisioning.pdf', 'num_pages': 1}", "page_content='can deploy a server with Ansible) and that provisioning tools can typically\ndo some degree of configuration (e.g., you can run configuration scripts on\neach server you provision with Terraform), you typically want to pick the\ntool that’s the best fit for your use case.\nIn particular, if you use server templating tools, the vast majority of your\nconfiguration management needs are already taken care of. Once you have\nan image created from a Dockerfile or Packer template, all that’s left to\ndo is provision the infrastructure for running those images. And when it\ncomes to provisioning, a provisioning tool is going to be your best choice.\nIn Chapter 7, you’ll see an example of how to use Terraform and Docker\ntogether, which is a particularly popular combination these days.\nThat said, if you’re not using server templating tools, a good alternative is\nto use a configuration management and provisioning tool together. For\nexample, a popular combination is to use Terraform to provision your\nservers and Ansible to configure each one.\nMutable Infrastructure Versus Immutable Infrastructure\nConfiguration management tools such as Chef, Puppet, and Ansible\ntypically default to a mutable infrastructure paradigm.\nFor example, if you instruct Chef to install a new version of OpenSSL, it\nwill run the software update on your existing servers, and the changes will\nhappen in place. Over time, as you apply more and more updates, each\nserver builds up a unique history of changes. As a result, each server\nbecomes slightly different than all the others, leading to subtle\nconfiguration bugs that are difficult to diagnose and reproduce (this is the\nsame configuration drift problem that happens when you manage servers\nmanually, although it’s much less problematic when using a configuration\nmanagement tool). Even with automated tests, these bugs are difficult to\ncatch; a configuration management change might work just fine on a test\nserver, but that same change might behave differently on a production\nserver because the production server has accumulated months of changes\nthat aren’t reflected in the test environment.' metadata={'original_pages_range': '56', 'source': '025_Mutable_Infrastructure_Versus_Immutable_Infrastructure', 'file_path': '/mnt/c/Users/Carlos/Documents/Github/jupiter-iaa-azure/Scripts/RAG/../../data/optimized_chunks/Libro-TF/025_Mutable_Infrastructure_Versus_Immutable_Infrastructure.pdf', 'num_pages': 1}", "page_content='If you’re using a provisioning tool such as Terraform to deploy machine\nimages created by Docker or Packer, most “changes” are actually\ndeployments of a completely new server. For example, to deploy a new\nversion of OpenSSL, you would use Packer to create a new image with the\nnew version of OpenSSL, deploy that image across a set of new servers,\nand then terminate the old servers. Because every deployment uses\nimmutable images on fresh servers, this approach reduces the likelihood of\nconfiguration drift bugs, makes it easier to know exactly what software is\nrunning on each server, and allows you to easily deploy any previous\nversion of the software (any previous image) at any time. It also makes your\nautomated testing more effective, because an immutable image that passes\nyour tests in the test environment is likely to behave exactly the same way\nin the production environment.\nOf course, it’s possible to force configuration management tools to do\nimmutable deployments, too, but it’s not the idiomatic approach for those\ntools, whereas it’s a natural way to use provisioning tools. It’s also worth\nmentioning that the immutable approach has downsides of its own. For\nexample, rebuilding an image from a server template and redeploying all\nyour servers for a trivial change can take a long time. Moreover,\nimmutability lasts only until you actually run the image. After a server is up\nand running, it will begin making changes on the hard drive and\nexperiencing some degree of configuration drift (although this is mitigated\nif you deploy frequently).\nProcedural Language Versus Declarative Language\nChef and Ansible encourage a procedural style in which you write code that\nspecifies, step by step, how to achieve some desired end state.\nTerraform, CloudFormation, Puppet, OpenStack Heat, and Pulumi all\nencourage a more declarative style in which you write code that specifies\nyour desired end state, and the IaC tool itself is responsible for figuring out\nhow to achieve that state.\n\nTo demonstrate the difference, let’s go through an example. Imagine that\nyou want to deploy 10 servers (EC2 Instances in AWS lingo) to run an AMI\nwith ID ami-0fb653ca2d3203ac1 (Ubuntu 20.04). Here is a\nsimplified example of an Ansible template that does this using a procedural\napproach:\n- ec2:\n    count: 10\n    image: ami-0fb653ca2d3203ac1\n    instance_type: t2.micro\nAnd here is a simplified example of a Terraform configuration that does the\nsame thing using a declarative approach:\nresource \"aws_instance\" \"example\" {\n  count         = 10\n  ami           = \"ami-0fb653ca2d3203ac1\"\n  instance_type = \"t2.micro\"\n}\nOn the surface, these two approaches might look similar, and when you\ninitially execute them with Ansible or Terraform, they will produce similar\nresults. The interesting thing is what happens when you want to make a\nchange.\nFor example, imagine traffic has gone up, and you want to increase the\nnumber of servers to 15. With Ansible, the procedural code you wrote\nearlier is no longer useful; if you just updated the number of servers to 15\nand reran that code, it would deploy 15 new servers, giving you 25 total! So\ninstead, you need to be aware of what is already deployed and write a\ntotally new procedural script to add the five new servers:\n- ec2:\n    count: 5\n    image: ami-0fb653ca2d3203ac1\n    instance_type: t2.micro\n\nWith declarative code, because all you do is declare the end state that you\nwant and Terraform figures out how to get to that end state, Terraform will\nalso be aware of any state it created in the past. Therefore, to deploy five\nmore servers, all you need to do is go back to the same Terraform\nconfiguration and update the count from 10 to 15:\nresource \"aws_instance\" \"example\" {\n  count         = 15\n  ami           = \"ami-0fb653ca2d3203ac1\"\n  instance_type = \"t2.micro\"\n}\nIf you applied this configuration, Terraform would realize it had already\ncreated 10 servers and therefore all it needs to do is create five new servers.\nIn fact, before applying this configuration, you can use Terraform’s plan\ncommand to preview what changes it would make:\n$ terraform plan \n \n# aws_instance.example[11] will be created \n+ resource \"aws_instance\" \"example\" { \n    + ami            = \"ami-0fb653ca2d3203ac1\" \n    + instance_type  = \"t2.micro\" \n    + (...) \n  } \n \n# aws_instance.example[12] will be created \n+ resource \"aws_instance\" \"example\" { \n    + ami            = \"ami-0fb653ca2d3203ac1\" \n    + instance_type  = \"t2.micro\" \n    + (...) \n  } \n \n# aws_instance.example[13] will be created \n+ resource \"aws_instance\" \"example\" { \n    + ami            = \"ami-0fb653ca2d3203ac1\" \n    + instance_type  = \"t2.micro\" \n    + (...) \n  } \n \n# aws_instance.example[14] will be created \n+ resource \"aws_instance\" \"example\" { \n    + ami            = \"ami-0fb653ca2d3203ac1\"\n\n+ instance_type  = \"t2.micro\" \n    + (...) \n  } \n \nPlan: 5 to add, 0 to change, 0 to destroy.\nNow what happens when you want to deploy a different version of the app,\nsuch as AMI ID ami-02bcbb802e03574ba? With the procedural\napproach, both of your previous Ansible templates are again not useful, so\nyou need to write yet another template to track down the 10 servers you\ndeployed previously (or was it 15 now?) and carefully update each one to\nthe new version. With the declarative approach of Terraform, you go back\nto the exact same configuration file again and simply change the ami\nparameter to ami-02bcbb802e03574ba:\nresource \"aws_instance\" \"example\" {\n  count         = 15\n  ami           = \"ami-02bcbb802e03574ba\"\n  instance_type = \"t2.micro\"\n}\nObviously, these examples are simplified. Ansible does allow you to use\ntags to search for existing EC2 Instances before deploying new ones (e.g.,\nusing the instance_tags and count_tag parameters), but having to\nmanually figure out this sort of logic for every single resource you manage\nwith Ansible, based on each resource’s past history, can be surprisingly\ncomplicated: for example, you may have to manually configure your code\nto look up existing Instances not only by tag but also by image version,\nAvailability Zone, and other parameters. This highlights two major\nproblems with procedural IaC tools:\nProcedural code does not fully capture the state of the infrastructure\nReading through the three preceding Ansible templates is not enough to\nknow what’s deployed. You’d also need to know the order in which\nthose templates were applied. Had you applied them in a different order,\nyou might have ended up with different infrastructure, and that’s not\nsomething you can see in the codebase itself. In other words, to reason' metadata={'original_pages_range': '57-60', 'source': '026_Procedural_Language_Versus_Declarative_Language', 'file_path': '/mnt/c/Users/Carlos/Documents/Github/jupiter-iaa-azure/Scripts/RAG/../../data/optimized_chunks/Libro-TF/026_Procedural_Language_Versus_Declarative_Language.pdf', 'num_pages': 4}", "page_content='about an Ansible or Chef codebase, you need to know the full history of\nevery change that has ever happened.\nProcedural code limits reusability\nThe reusability of procedural code is inherently limited because you\nmust manually take into account the current state of the infrastructure.\nBecause that state is constantly changing, code you used a week ago\nmight no longer be usable because it was designed to modify a state of\nyour infrastructure that no longer exists. As a result, procedural\ncodebases tend to grow large and complicated over time.\nWith Terraform’s declarative approach, the code always represents the latest\nstate of your infrastructure. At a glance, you can determine what’s currently\ndeployed and how it’s configured, without having to worry about history or\ntiming. This also makes it easy to create reusable code, since you don’t\nneed to manually account for the current state of the world. Instead, you just\nfocus on describing your desired state, and Terraform figures out how to get\nfrom one state to the other automatically. As a result, Terraform codebases\ntend to stay small and easy to understand.\nGeneral-Purpose Language Versus Domain-Specific\nLanguage\nChef and Pulumi allow you to use a general-purpose programming\nlanguage (GPL) to manage infrastructure as code: Chef supports Ruby;\nPulumi supports a wide variety of GPLs, including JavaScript, TypeScript,\nPython, Go, C#, Java, and others. Terraform, Puppet, Ansible,\nCloudFormation, and OpenStack Heat each use a domain-specific language\n(DSL) to manage infrastructure as code: Terraform uses HCL; Puppet uses\nPuppet Language; Ansible, CloudFormation, and OpenStack Heat use\nYAML (CloudFormation also supports JSON).\nThe distinction between GPLs and DSLs is not entirely clear-cut—it’s more\nof a helpful mental model than a clean, separate categorization—but the\nbasic idea is that DSLs are designed for use in one specific domain,\n\nwhereas GPLs can be used across a broad range of domains. For example,\nthe HCL code you write for Terraform works only with Terraform and is\nlimited solely to the functionality supported by Terraform, such as\ndeploying infrastructure. This is in contrast to using a GPL such as\nJavaScript with Pulumi, where the code you write can not only manage\ninfrastructure using Pulumi libraries but also perform almost any other\nprogramming task you wish, such as run a web app (in fact, Pulumi offers\nan Automation API you can use to embed Pulumi within your application\ncode), perform complicated control logic (loops, conditionals, and\nabstraction are all easier to do in a GPL than a DSL), run various\nvalidations and tests, integrate with other tools and APIs, and so on.\nDSLs have several advantages over GPLs:\nEasier to learn\nSince DSLs, by design, deal with just one domain, they tend to be\nsmaller and simpler languages than GPLs and therefore are easier to\nlearn than GPLs. Most developers will be able to learn Terraform faster\nthan, say, Java.\nClearer and more concise\nSince DSLs are designed for one specific purpose, with all the\nkeywords in the language built to do that one thing, code written in\nDSLs tends to be easier to understand and more concise than code\nwritten to do the exact same thing but written in a GPL. The code to\ndeploy a single server in AWS is usually going to be shorter and easier\nto understand in Terraform than in Java.\nMore uniform\nMost DSLs are limited in what they allow you to do. This has some\ndrawbacks, as I’ll mention shortly, but one of the advantages is that\ncode written in DSLs typically uses a uniform, predictable structure, so\nit’s easier to navigate and understand than code written in GPLs, where\nevery developer might solve the same problem in a completely different' metadata={'original_pages_range': '61-62', 'source': '027_General-Purpose_Language_Versus_Domain-Specific_Language', 'file_path': '/mnt/c/Users/Carlos/Documents/Github/jupiter-iaa-azure/Scripts/RAG/../../data/optimized_chunks/Libro-TF/027_General-Purpose_Language_Versus_Domain-Specific_Language.pdf', 'num_pages': 2}", "page_content='way. There’s really only one way to deploy a server in AWS using\nTerraform; there are hundreds of ways to do the same thing with Java.\nGPLs also have several advantages over DSLs:\nPossibly no need to learn anything new\nSince GPLs are used in many domains, there’s a chance you might not\nhave to learn a new language at all. This is especially true of Pulumi, as\nit supports several of the most popular languages in the world, including\nJavaScript, Python, and Java. If you already know Java, you’ll be able\nto jump into Pulumi faster than if you had to learn HCL to use\nTerraform.\nBigger ecosystem and more mature tooling\nSince GPLs are used in many domains, they have far bigger\ncommunities and much more mature tooling than a typical DSL. The\nnumber and quality of Integrated Development Environments (IDEs),\nlibraries, patterns, testing tools, and so on for Java vastly exceeds what’s\navailable for Terraform.\nMore power\nGPLs, by design, can be used to do almost any programming task, so\nthey offer much more power and functionality than DSLs. Certain tasks,\nsuch as control logic (loops and conditionals), automated testing, code\nreuse, abstraction, and integration with other tools, are far easier with\nJava than with Terraform.\nMaster Versus Masterless\nBy default, Chef and Puppet require that you run a master server for storing\nthe state of your infrastructure and distributing updates. Every time you\nwant to update something in your infrastructure, you use a client (e.g., a\ncommand-line tool) to issue new commands to the master server, and the\nmaster server either pushes the updates out to all of the other servers or\n\nthose servers pull the latest updates down from the master server on a\nregular basis.\nA master server offers a few advantages. First, it’s a single, central place\nwhere you can see and manage the status of your infrastructure. Many\nconfiguration management tools even provide a web interface (e.g., the\nChef Console, Puppet Enterprise Console) for the master server to make it\neasier to see what’s going on. Second, some master servers can run\ncontinuously in the background and enforce your configuration. That way,\nif someone makes a manual change on a server, the master server can revert\nthat change to prevent configuration drift.\nHowever, having to run a master server has some serious drawbacks:\nExtra infrastructure\nYou need to deploy an extra server, or even a cluster of extra servers\n(for high availability and scalability), just to run the master.\nMaintenance\nYou need to maintain, upgrade, back up, monitor, and scale the master\nserver(s).\nSecurity\nYou need to provide a way for the client to communicate to the master\nserver(s) and a way for the master server(s) to communicate with all the\nother servers, which typically means opening extra ports and\nconfiguring extra authentication systems, all of which increases your\nsurface area to attackers.\nChef and Puppet do have varying levels of support for masterless modes\nwhere you run just their agent software on each of your servers, typically on\na periodic schedule (e.g., a cron job that runs every five minutes), and use\nthat to pull down the latest updates from version control (rather than from a\nmaster server). This significantly reduces the number of moving parts, but,\nas I discuss in the next section, this still leaves a number of unanswered' metadata={'original_pages_range': '63-64', 'source': '028_Master_Versus_Masterless', 'file_path': '/mnt/c/Users/Carlos/Documents/Github/jupiter-iaa-azure/Scripts/RAG/../../data/optimized_chunks/Libro-TF/028_Master_Versus_Masterless.pdf', 'num_pages': 2}", "page_content='questions, especially about how to provision the servers and install the\nagent software on them in the first place.\nAnsible, CloudFormation, Heat, Terraform, and Pulumi are all masterless\nby default. Or, to be more accurate, some of them rely on a master server,\nbut it’s already part of the infrastructure you’re using and not an extra piece\nthat you need to manage. For example, Terraform communicates with cloud\nproviders using the cloud provider’s APIs, so in some sense, the API\nservers are master servers, except that they don’t require any extra\ninfrastructure or any extra authentication mechanisms (i.e., just use your\nAPI keys). Ansible works by connecting directly to each server over SSH,\nso again, you don’t need to run any extra infrastructure or manage extra\nauthentication mechanisms (i.e., just use your SSH keys).\nAgent Versus Agentless\nChef and Puppet require you to install agent software (e.g., Chef Client,\nPuppet Agent) on each server that you want to configure. The agent\ntypically runs in the background on each server and is responsible for\ninstalling the latest configuration management updates.\nThis has a few drawbacks:\nBootstrapping\nHow do you provision your servers and install the agent software on\nthem in the first place? Some configuration management tools kick the\ncan down the road, assuming that some external process will take care\nof this for them (e.g., you first use Terraform to deploy a bunch of\nservers with an AMI that has the agent already installed); other\nconfiguration management tools have a special bootstrapping process in\nwhich you run one-off commands to provision the servers using the\ncloud provider APIs and install the agent software on those servers over\nSSH.\nMaintenance\n\nYou need to update the agent software on a periodic basis, being careful\nto keep it synchronized with the master server if there is one. You also\nneed to monitor the agent software and restart it if it crashes.\nSecurity\nIf the agent software pulls down configuration from a master server (or\nsome other server if you’re not using a master), you need to open\noutbound ports on every server. If the master server pushes\nconfiguration to the agent, you need to open inbound ports on every\nserver. In either case, you must figure out how to authenticate the agent\nto the server to which it’s communicating. All of this increases your\nsurface area to attackers.\nOnce again, Chef and Puppet do have varying levels of support for\nagentless modes, but these feel like they were tacked on as an afterthought\nand don’t support the full feature set of the configuration management tool.\nThat’s why in the wild, the default or idiomatic configuration for Chef and\nPuppet almost always includes an agent and usually a master, too, as shown\nin Figure 1-7.\n\n\n\nFigure 1-7. The typical architecture for Chef and Puppet involves many moving parts. For example,\nthe default setup for Chef is to run the Chef client on your computer, which talks to a Chef master\nserver, which deploys changes by communicating with Chef clients running on all your other servers.\nAll of these extra moving parts introduce a large number of new failure\nmodes into your infrastructure. Each time you get a bug report at 3 a.m.,\nyou’ll need to figure out whether it’s a bug in your application code, or your\nIaC code, or the configuration management client, or the master server(s),\nor the way the client communicates with the master server(s), or the way\nother servers communicate with the master server(s), or…\nAnsible, CloudFormation, Heat, Terraform, and Pulumi do not require you\nto install any extra agents. Or, to be more accurate, some of them require\nagents, but these are typically already installed as part of the infrastructure\nyou’re using. For example, AWS, Azure, Google Cloud, and all of the other\ncloud providers take care of installing, managing, and authenticating agent\nsoftware on each of their physical servers. As a user of Terraform, you don’t\nneed to worry about any of that: you just issue commands, and the cloud\nprovider’s agents execute them for you on all of your servers, as shown in\nFigure 1-8. With Ansible, your servers need to run the SSH daemon, which\nis common to run on most servers anyway.\n\nFigure 1-8. Terraform uses a masterless, agentless architecture. All you need to run is the Terraform\nclient, and it takes care of the rest by using the APIs of cloud providers, such as AWS.' metadata={'original_pages_range': '65-69', 'source': '029_Agent_Versus_Agentless', 'file_path': '/mnt/c/Users/Carlos/Documents/Github/jupiter-iaa-azure/Scripts/RAG/../../data/optimized_chunks/Libro-TF/029_Agent_Versus_Agentless.pdf', 'num_pages': 5}", "page_content='Paid Versus Free Offering\nCloudFormation and OpenStack Heat are completely free: the resources\nyou deploy with those tools may cost money, but you don’t pay anything to\nuse the tools themselves. Terraform, Chef, Puppet, Ansible, and Pulumi are\nall available in free versions and paid versions: for example, you can use\nthe free and open source version of Terraform by itself, or you could choose\nto use it with HashiCorp’s paid product, Terraform Cloud. The price points,\npackaging, and trade-offs with the paid versions are beyond the scope of\nthis book. The one question I want to focus on here is whether the free\nversion is so limited that you are effectively forced to use the paid offering\nfor real-world, production use cases.\nTo be clear, there’s nothing wrong with a company offering a paid service\nfor one of these tools; in fact, if you’re using these tools in production, I\nstrongly recommend looking into the paid services, as many of them are\nwell worth the money. However, you have to realize that those paid services\naren’t under your control—they could go out of business, or get acquired\n(e.g., Chef, Puppet, and Ansible have all gone through acquisitions that had\nsignificant impacts on their paid product offerings), or change their pricing\nmodel (e.g., Pulumi changed its pricing in 2021, which benefited some\nusers but increased prices by ~10x for others), or change the product, or\ndiscontinue the product entirely—so it’s important to know whether the IaC\ntool you picked would still be usable if, for some reason, you couldn’t use\none of these paid services.\nIn my experience, the free versions of Terraform, Chef, Puppet, and Ansible\ncan all be used successfully for production use cases; the paid services can\nmake these tools even better, but if they weren’t available, you could still\nget by. Pulumi, on the other hand, is harder to use in production without the\npaid offering known as Pulumi Service.\nA key part of managing infrastructure as code is managing state (you’ll\nlearn about how Terraform manages state in Chapter 3), and Pulumi, by\ndefault, uses Pulumi Service as the backend for state storage. You can\nswitch to other supported backends for state storage, such as Amazon S3,' metadata={'original_pages_range': '70', 'source': '030_Paid_Versus_Free_Offering', 'file_path': '/mnt/c/Users/Carlos/Documents/Github/jupiter-iaa-azure/Scripts/RAG/../../data/optimized_chunks/Libro-TF/030_Paid_Versus_Free_Offering.pdf', 'num_pages': 1}", "page_content='Azure Blob Storage, or Google Cloud Storage, but the Pulumi backend\ndocumentation explains that only Pulumi Service supports transactional\ncheckpointing (for fault tolerance and recovery), concurrent state locking\n(to prevent corrupting your infrastructure state in a team environment), and\nencrypted state in transit and at rest. In my opinion, without these features,\nit’s not practical to use Pulumi in any sort of production environment (i.e.,\nwith more than one developer), so if you’re going to use Pulumi, you more\nor less have to pay for Pulumi Service.\nLarge Community Versus Small Community\nWhenever you pick a technology, you are also picking a community. In\nmany cases, the ecosystem around the project can have a bigger impact on\nyour experience than the inherent quality of the technology itself. The\ncommunity determines how many people contribute to the project; how\nmany plugins, integrations, and extensions are available; how easy it is to\nfind help online (e.g., blog posts, questions on Stack Overflow); and how\neasy it is to hire someone to help you (e.g., an employee, consultant, or\nsupport company).\nIt’s difficult to do an accurate comparison between communities, but you\ncan spot some trends by searching online. Table 1-1 shows a comparison of\npopular IaC tools, with data I gathered in June 2022, including whether the\nIaC tool is open source or closed source, what cloud providers it supports,\nthe total number of contributors and stars on GitHub, how many open\nsource libraries are available for the tool, and the number of questions listed\nfor that tool on Stack Overflow.7\n\nObviously, this is not a perfect apples-to-apples comparison. For example,\nsome of the tools have more than one repository: e.g., Terraform split the\nprovider code (i.e., the code specific to AWS, Google Cloud, Azure, etc.)\nout into separate repos in 2017, so the preceding table significantly\nunderstates activity; some tools offer alternatives to Stack Overflow for\nquestions; and so on.\nThat said, a few trends are obvious. First, all of the IaC tools in this\ncomparison are open source and work with many cloud providers, except\nfor CloudFormation, which is closed source and works only with AWS.\nTable 1-1. A comparison of IaC communities\nSource Cloud Contributors Stars\nChef Open All 640 6,910\nPuppet Open All 571 6,581\nAnsible Open All 5,328 53,479\nPulumi Open All 1,402 12,723\nCloudFormationClosed AWS ? ?\nHeat Open All 395 379\nTerraform Open All 1,621 33,019\na  This is the number of cookbooks in the Chef Supermarket.\nb  This is the number of modules in Puppet Forge.\nc  This is the number of reusable roles in Ansible Galaxy.\nd  This is the number of packages in the Pulumi Registry.\ne  This is the number of templates in AWS Quick Starts.\nf  I could not find any collections of community Heat templates.\ng  This is the number of modules in the Terraform Registry.' metadata={'original_pages_range': '71-72', 'source': '031_Large_Community_Versus_Small_Community', 'file_path': '/mnt/c/Users/Carlos/Documents/Github/jupiter-iaa-azure/Scripts/RAG/../../data/optimized_chunks/Libro-TF/031_Large_Community_Versus_Small_Community.pdf', 'num_pages': 2}", "page_content='Second, Ansible and Terraform seem to be the clear leads in terms of\npopularity.\nAnother interesting trend to note is how these numbers have changed since\nthe first edition of the book. Table 1-2 shows the percentage change in each\nof the numbers from the values I gathered in the first edition back in\nSeptember 2016. (Note: Pulumi is not included in this table, as it wasn’t\npart of this comparison in the first edition of the book.)\nAgain, the data here is not perfect, but it’s good enough to spot a clear\ntrend: Terraform and Ansible are experiencing explosive growth. The\nincrease in the number of contributors, stars, open source libraries, and\nStack Overflow posts is through the roof. Both of these tools have large,\nactive communities today, and judging by these trends, it’s likely that they\nwill become even larger in the future.\nMature Versus Cutting Edge\nAnother key factor to consider when picking any technology is maturity. Is\nthis a technology that has been around for years, where all the usage\nTable 1-2. How the IaC communities have changed between September 2016\nSource Cloud Contributors Stars\nChef Open All +34% +56%\nPuppet Open All +32% +58%\nAnsible Open All +258% +183%\nCloudFormationClosed AWS ? ?\nHeat Open All +40% +34%\nTerraform Open All +148% +476%\na  In earlier editions of the book, I used CloudFormation templates in the awslabs GitHub rep\nAWS Quick Starts in this edition, so the numbers aren’t directly comparable.' metadata={'original_pages_range': '73', 'source': '032_Mature_Versus_Cutting_Edge', 'file_path': '/mnt/c/Users/Carlos/Documents/Github/jupiter-iaa-azure/Scripts/RAG/../../data/optimized_chunks/Libro-TF/032_Mature_Versus_Cutting_Edge.pdf', 'num_pages': 1}", "page_content='patterns, best practices, problems, and failure modes are well understood?\nOr is this a new technology where you’ll have to learn all those hard lessons\nfrom scratch? Table 1-3 shows the initial release dates, current version\nnumbers (as of June 2022), and my own subjective perception of the\nmaturity of each of the IaC tools.\nTable 1-3. A comparison of IaC maturity as of June 2022\nInitial releaseCurrent versionPerceived maturity\nChef 2009 17.10.3 High\nPuppet 2005 7.17.0 High\nAnsible 2012 5.9.0 Medium\nPulumi 2017 3.34.1 Low\nCloudFormation2011 ??? Medium\nHeat 2012 18.0.0 Low\nTerraform 2014 1.2.3 Medium\nAgain, this is not an apples-to-apples comparison: age alone does not\ndetermine maturity—neither does a high version number (different tools\nhave different versioning schemes). Still, some trends are clear. Pulumi is\nthe youngest IaC tool in this comparison and, arguably, the least mature:\nthis becomes apparent when you search for documentation, best practices,\ncommunity modules, etc. Terraform is a bit more mature these days: the\ntooling has improved, the best practices are better understood, there are far\nmore learning resources available (including this book!), and now that it has\nreached the 1.0.0 milestone, it is a considerably more stable and reliable\ntool than when the first and second editions of this book came out. Chef and\nPuppet are the oldest and arguably most mature tools on this list.\nUse of Multiple Tools Together' metadata={'original_pages_range': '74', 'source': '033_Use_of_Multiple_Tools_Together', 'file_path': '/mnt/c/Users/Carlos/Documents/Github/jupiter-iaa-azure/Scripts/RAG/../../data/optimized_chunks/Libro-TF/033_Use_of_Multiple_Tools_Together.pdf', 'num_pages': 1}", "page_content='Although I’ve been comparing IaC tools this entire chapter, the reality is\nthat you will likely need to use multiple tools to build your infrastructure.\nEach of the tools you’ve seen has strengths and weaknesses, so it’s your job\nto pick the right tools for the job.\nThe following sections show three common combinations I’ve seen work\nwell at a number of companies.\nProvisioning plus configuration management\nExample: Terraform and Ansible. You use Terraform to deploy all the\nunderlying infrastructure, including the network topology (i.e., virtual\nprivate clouds [VPCs], subnets, route tables), data stores (e.g., MySQL,\nRedis), load balancers, and servers. You then use Ansible to deploy your\napps on top of those servers, as depicted in Figure 1-9.\nFigure 1-9. Terraform deploys the infrastructure, including servers, and Ansible deploys apps onto\nthose servers.\nThis is an easy approach to get started with, because there is no extra\ninfrastructure to run (Terraform and Ansible are both client-only\napplications), and there are many ways to get Ansible and Terraform to\nwork together (e.g., Terraform adds special tags to your servers, and\nAnsible uses those tags to find the servers and configure them). The major\ndownside is that using Ansible typically means that you’re writing a lot of' metadata={'original_pages_range': '75', 'source': '034_Provisioning_plus_configuration_management', 'file_path': '/mnt/c/Users/Carlos/Documents/Github/jupiter-iaa-azure/Scripts/RAG/../../data/optimized_chunks/Libro-TF/034_Provisioning_plus_configuration_management.pdf', 'num_pages': 1}", "page_content='procedural code, with mutable servers, so as your codebase, infrastructure,\nand team grow, maintenance can become more difficult.\nProvisioning plus server templating\nExample: Terraform and Packer. You use Packer to package your apps as\nVM images. You then use Terraform to deploy servers with these VM\nimages and the rest of your infrastructure, including the network topology\n(i.e., VPCs, subnets, route tables), data stores (e.g., MySQL, Redis), and\nload balancers, as illustrated in Figure 1-10.\nFigure 1-10. Terraform deploys the infrastructure, including servers, and Packer creates the VMs\nthat run on those servers.\nThis is also an easy approach to get started with, because there is no extra\ninfrastructure to run (Terraform and Packer are both client-only\napplications), and you’ll get plenty of practice deploying VM images using\nTerraform later in this book. Moreover, this is an immutable infrastructure\napproach, which will make maintenance easier. However, there are two\nmajor drawbacks. First, VMs can take a long time to build and deploy,\nwhich will slow down your iteration speed. Second, as you’ll see in later\nchapters, the deployment strategies you can implement with Terraform are\nlimited (e.g., you can’t implement blue-green deployment natively in' metadata={'original_pages_range': '76', 'source': '035_Provisioning_plus_server_templating', 'file_path': '/mnt/c/Users/Carlos/Documents/Github/jupiter-iaa-azure/Scripts/RAG/../../data/optimized_chunks/Libro-TF/035_Provisioning_plus_server_templating.pdf', 'num_pages': 1}", "page_content='Terraform), so you either end up writing lots of complicated deployment\nscripts or you turn to orchestration tools, as described next.\nProvisioning plus server templating plus orchestration\nExample: Terraform, Packer, Docker, and Kubernetes. You use Packer to\ncreate a VM image that has Docker and Kubernetes agents installed. You\nthen use Terraform to deploy a cluster of servers, each of which runs this\nVM image, and the rest of your infrastructure, including the network\ntopology (i.e., VPCs, subnets, route tables), data stores (e.g., MySQL,\nRedis), and load balancers. Finally, when the cluster of servers boots up, it\nforms a Kubernetes cluster that you use to run and manage your Dockerized\napplications, as shown in Figure 1-11.\n\nFigure 1-11. Terraform deploys the infrastructure, including servers; Packer creates the VMs that\nrun on those servers; and Kubernetes manages those VMs as a cluster for running Docker\ncontainers.\nThe advantage of this approach is that Docker images build fairly quickly,\nyou can run and test them on your local computer, and you can take\nadvantage of all of the built-in functionality of Kubernetes, including\nvarious deployment strategies, auto healing, auto scaling, and so on. The\ndrawback is the added complexity, both in terms of extra infrastructure to\nrun (Kubernetes clusters are difficult and expensive to deploy and operate,\nthough most major cloud providers now provide managed Kubernetes\nservices, which can offload some of this work) and in terms of several extra\nlayers of abstraction (Kubernetes, Docker, Packer) to learn, manage, and\ndebug.' metadata={'original_pages_range': '77-78', 'source': '036_Provisioning_plus_server_templating_plus_orchestration', 'file_path': '/mnt/c/Users/Carlos/Documents/Github/jupiter-iaa-azure/Scripts/RAG/../../data/optimized_chunks/Libro-TF/036_Provisioning_plus_server_templating_plus_orchestration.pdf', 'num_pages': 2}", "page_content='You’ll see an example of this approach in Chapter 7.\nConclusion\nPutting it all together, Table 1-4 shows how the most popular IaC tools\nstack up. Note that this table shows the default or most common way the\nvarious IaC tools are used, though as discussed earlier in this chapter, these\nIaC tools are flexible enough to be used in other configurations, too (e.g.,\nyou can use Chef without a master, you can use Puppet to do immutable\ninfrastructure, etc.).\nTable 1-4. A comparison of the most common ways to use the most popular I\nChef Puppet Ansible Pulumi\nSource Open Open Open Open\nCloud All All All All\nType Config mgmt Config mgmt Config mgmt Provisioning\nInfra Mutable Mutable Mutable Immutable\nParadigm Procedural Declarative Procedural Declarative\nLanguage GPL DSL DSL GPL\nMaster Yes Yes No No\nAgent Yes Yes No No\nPaid Service Optional Optional Optional Must-have\nCommunity Large Large Huge Small\nMaturity High High Medium Low\n\nAt Gruntwork, what we wanted was an open source, cloud-agnostic\nprovisioning tool with a large community, a mature codebase, and support\nfor immutable infrastructure, a declarative language, a masterless and\nagentless architecture, and an optional paid service. Table 1-4 shows that\nTerraform, although not perfect, comes the closest to meeting all of our\ncriteria.\nDoes Terraform fit your criteria? If so, head over to Chapter 2 to learn how\nto use it.\n1 From The DevOps Handbook: How to Create World-Class Agility, Reliability, & Security in\nTechnology Organizations (IT Revolution Press, 2016) by Gene Kim, Jez Humble, Patrick\nDebois, and John Willis.\n2 On most modern operating systems, code runs in one of two “spaces”: kernel space or user\nspace. Code running in kernel space has direct, unrestricted access to all of the hardware.\nThere are no security restrictions (i.e., you can execute any CPU instruction, access any part of\nthe hard drive, write to any address in memory) or safety restrictions (e.g., a crash in kernel\nspace will typically crash the entire computer), so kernel space is generally reserved for the\nlowest-level, most trusted functions of the OS (typically called the kernel). Code running in\nuser space does not have any direct access to the hardware and must use APIs exposed by the\nOS kernel instead. These APIs can enforce security restrictions (e.g., user permissions) and\nsafety (e.g., a crash in a user space app typically affects only that app), so just about all\napplication code runs in user space.\n3 As a general rule, containers provide isolation that’s good enough to run your own code, but if\nyou need to run third-party code (e.g., you’re building your own cloud provider) that might\nactively be performing malicious actions, you’ll want the increased isolation guarantees of a\nVM.\n4 This is where the term bus factor comes from: your team’s bus factor is the number of people\nyou can lose (e.g., because they got hit by a bus) before you can no longer operate your\nbusiness. You never want to have a bus factor of 1.\n5 Check out the Gruntwork Infrastructure as Code Library for an example.\n6 Docker, Packer, and Kubernetes are not part of the comparison, because they can be used with\nany of the configuration management or provisioning tools.\n7 The data on contributors and stars comes from the open source repositories (mostly GitHub)\nfor each tool. Because CloudFormation is closed source, this information is not available.' metadata={'original_pages_range': '79-80', 'source': '037_Conclusion', 'file_path': '/mnt/c/Users/Carlos/Documents/Github/jupiter-iaa-azure/Scripts/RAG/../../data/optimized_chunks/Libro-TF/037_Conclusion.pdf', 'num_pages': 2}", "page_content='Chapter 2. Getting Started with\nTerraform\nIn this chapter, you’re going to learn the basics of how to use Terraform. It’s\nan easy tool to learn, so in the span of about 40 pages, you’ll go from\nrunning your first Terraform commands all the way up to using Terraform\nto deploy a cluster of servers with a load balancer that distributes traffic\nacross them. This infrastructure is a good starting point for running\nscalable, highly available web services. In subsequent chapters, you’ll\ndevelop this example even further.\nTerraform can provision infrastructure across public cloud providers such as\nAWS, Azure, Google Cloud, and DigitalOcean, as well as private cloud and\nvirtualization platforms such as OpenStack and VMware. For just about all\nof the code examples in this chapter and the rest of the book, you are going\nto use AWS. AWS is a good choice for learning Terraform because of the\nfollowing:\nAWS is the most popular cloud infrastructure provider, by far. It has a\n32% share in the cloud infrastructure market, which is more than the\nnext three biggest competitors (Microsoft, Google, and IBM)\ncombined.\nAWS provides a huge range of reliable and scalable cloud-hosting\nservices, including Amazon Elastic Compute Cloud (Amazon EC2),\nwhich you can use to deploy virtual servers; Auto Scaling Groups\n(ASGs), which make it easier to manage a cluster of virtual servers;\nand Elastic Load Balancers (ELBs), which you can use to distribute\ntraffic across the cluster of virtual servers.\nAWS offers a Free Tier for the first year that should allow you to run\nall of these examples for free or a very low cost. If you already used\n1\n2\n3' metadata={'original_pages_range': '81', 'source': '038_2._Getting_Started_with_Terraform', 'file_path': '/mnt/c/Users/Carlos/Documents/Github/jupiter-iaa-azure/Scripts/RAG/../../data/optimized_chunks/Libro-TF/038_2._Getting_Started_with_Terraform.pdf', 'num_pages': 1}", "page_content='your Free Tier credits, the examples in this book should still cost you\nno more than a few dollars.\nIf you’ve never used AWS or Terraform before, don’t worry; this tutorial is\ndesigned for novices to both technologies. I’ll walk you through the\nfollowing steps:\nSetting up your AWS account\nInstalling Terraform\nDeploying a single server\nDeploying a single web server\nDeploying a configurable web server\nDeploying a cluster of web servers\nDeploying a load balancer\nCleaning up\nEXAMPLE CODE\nAs a reminder, you can find all of the code examples in the book on GitHub.\nSetting Up Your AWS Account\nIf you don’t already have an AWS account, head over to\nhttps://aws.amazon.com and sign up. When you first register for AWS, you\ninitially sign in as the root user. This user account has access permissions to\ndo absolutely anything in the account, so from a security perspective, it’s\nnot a good idea to use the root user on a day-to-day basis. In fact, the only\nthing you should use the root user for is to create other user accounts with\nmore-limited permissions, and then switch to one of those accounts\nimmediately.4\n\nTo create a more-limited user account, you will need to use the Identity and\nAccess Management (IAM) service. IAM is where you manage user\naccounts as well as the permissions for each user. To create a new IAM user,\ngo to the IAM Console, click Users, and then click the Add Users button.\nEnter a name for the user, and make sure “Access key - Programmatic\naccess” is selected, as shown in Figure 2-1 (note that AWS occasionally\nmakes changes to its web console, so what you see may look slightly\ndifferent than the screenshots in this book).\n\nFigure 2-1. Use the AWS Console to create a new IAM user.\nClick the Next button. AWS will ask you to add permissions to the user. By\ndefault, new IAM users have no permissions whatsoever and cannot do\n\nanything in an AWS account. To give your IAM user the ability to do\nsomething, you need to associate one or more IAM Policies with that user’s\naccount. An IAM Policy is a JSON document that defines what a user is or\nisn’t allowed to do. You can create your own IAM Policies or use some of\nthe predefined IAM Policies built into your AWS account, which are known\nas Managed Policies.\nTo run the examples in this book, the easiest way to get started is to add the\nAdministratorAccess Managed Policy to your IAM user (search for\nit, and click the checkbox next to it), as shown in Figure 2-2.\n5\n6\n\nFigure 2-2. Add the AdministratorAccess Managed IAM Policy to your new IAM user.\nClick Next a couple more times and then the “Create user” button. AWS\nwill show you the security credentials for that user, which consist of an\n\nAccess Key ID and a Secret Access Key, as shown in Figure 2-3. You must\nsave these immediately because they will never be shown again, and you’ll\nneed them later on in this tutorial. Remember that these credentials give\naccess to your AWS account, so store them somewhere secure (e.g., a\npassword manager such as 1Password, LastPass, or macOS Keychain), and\nnever share them with anyone.\nAfter you’ve saved your credentials, click the Close button. You’re now\nready to move on to using Terraform.\nA NOTE ON DEFAULT VIRTUAL PRIVATE CLOUDS\nAll of the AWS examples in this book use the Default VPC in your AWS account. A\nVPC, or virtual private cloud, is an isolated area of your AWS account that has its own\nvirtual network and IP address space. Just about every AWS resource deploys into a\nVPC. If you don’t explicitly specify a VPC, the resource will be deployed into the\nDefault VPC, which is part of every AWS account created after 2013. If for some reason\nyou deleted the Default VPC in your account, either use a different region (each region\nhas its own Default VPC) or create a new Default VPC using the AWS Web Console.\nOtherwise, you’ll need to update almost every example to include a vpc_id or\nsubnet_id parameter pointing to a custom VPC.' metadata={'original_pages_range': '82-87', 'source': '039_Setting_Up_Your_AWS_Account', 'file_path': '/mnt/c/Users/Carlos/Documents/Github/jupiter-iaa-azure/Scripts/RAG/../../data/optimized_chunks/Libro-TF/039_Setting_Up_Your_AWS_Account.pdf', 'num_pages': 6}", "page_content='Figure 2-3. Store your AWS credentials somewhere secure. Never share them with anyone. (Don’t\nworry, the ones in the screenshot are fake.)\nInstalling Terraform\nThe easiest way to install Terraform is to use your operating system’s\npackage manager. For example, on macOS, if you are a Homebrew user,\nyou can run the following:\n$ brew tap hashicorp/tap \n$ brew install hashicorp/tap/terraform\n\nOn Windows, if you’re a Chocolatey user, you can run the following:\n$ choco install terraform\nCheck the Terraform documentation for installation instructions on other\noperating systems, including the various flavors of Linux.\nAlternatively, you can install Terraform manually by going to the Terraform\nhome page, clicking the download link, selecting the appropriate package\nfor your operating system, downloading the ZIP archive, and unzipping it\ninto the directory where you want Terraform to be installed. The archive\nwill extract a single binary called terraform, which you’ll want to add to\nyour PATH environment variable.\nTo check whether things are working, run the terraform command, and\nyou should see the usage instructions:\n$ terraform \nUsage: terraform [global options] <subcommand> [args] \n \nThe available commands for execution are listed below. \nThe primary workflow commands are given first, followed by \nless common or more advanced commands. \n \nMain commands: \n  init          Prepare your working directory for other commands \n  validate      Check whether the configuration is valid \n  plan          Show changes required by the current \nconfiguration \n  apply         Create or update infrastructure \n  destroy       Destroy previously-created infrastructure \n \n(...)\nFor Terraform to be able to make changes in your AWS account, you will\nneed to set the AWS credentials for the IAM user you created earlier as the\nenvironment variables AWS_ACCESS_KEY_ID and\nAWS_SECRET_ACCESS_KEY. For example, here is how you can do it in a\nUnix/Linux/macOS terminal:' metadata={'original_pages_range': '88-89', 'source': '040_Installing_Terraform', 'file_path': '/mnt/c/Users/Carlos/Documents/Github/jupiter-iaa-azure/Scripts/RAG/../../data/optimized_chunks/Libro-TF/040_Installing_Terraform.pdf', 'num_pages': 2}", "page_content='$ export AWS_ACCESS_KEY_ID=(your access key id) \n$ export AWS_SECRET_ACCESS_KEY=(your secret access key)\nAnd here is how you can do it in a Windows command terminal:\n$ set AWS_ACCESS_KEY_ID=(your access key id) \n$ set AWS_SECRET_ACCESS_KEY=(your secret access key)\nNote that these environment variables apply only to the current shell, so if\nyou reboot your computer or open a new terminal window, you’ll need to\nexport these variables again.\nOTHER AWS AUTHENTICATION OPTIONS\nIn addition to environment variables, Terraform supports the same authentication\nmechanisms as all AWS CLI and SDK tools. Therefore, it’ll also be able to use\ncredentials in $HOME/.aws/credentials, which are automatically generated if you run\naws configure, or IAM roles, which you can add to almost any resource in AWS.\nFor more info, see A Comprehensive Guide to Authenticating to AWS on the Command\nLine. You’ll also see more information on authenticating to Terraform providers in\nChapter 6.\nDeploying a Single Server\nTerraform code is written in the HashiCorp Configuration Language (HCL)\nin files with the extension .tf.  It is a declarative language, so your goal is to\ndescribe the infrastructure you want, and Terraform will figure out how to\ncreate it. Terraform can create infrastructure across a wide variety of\nplatforms, or what it calls providers, including AWS, Azure, Google Cloud,\nDigitalOcean, and many others.\nYou can write Terraform code in just about any text editor. If you search\naround, you can find Terraform syntax highlighting support for most editors\n(note that you may have to search for the word HCL instead of Terraform),\nincluding vim, emacs, Sublime Text, Atom, Visual Studio Code, and IntelliJ\n(the latter even has support for refactoring, find usages, and go to\ndeclaration).\n7\n\nThe first step to using Terraform is typically to configure the provider(s)\nyou want to use. Create an empty folder and put a file in it called main.tf\nthat contains the following contents:\nprovider \"aws\" {\n  region = \"us-east-2\"\n}\nThis tells Terraform that you are going to be using AWS as your provider\nand that you want to deploy your infrastructure into the us-east-2\nregion. AWS has datacenters all over the world, grouped into regions. An\nAWS region is a separate geographic area, such as us-east-2 (Ohio),\neu-west-1 (Ireland), and ap-southeast-2 (Sydney). Within each\nregion, there are multiple isolated datacenters known as Availability Zones\n(AZs), such as us-east-2a, us-east-2b, and so on. There are many\nother settings you can configure on this provider, but for now, let’s keep it\nsimple, and we’ll take a deeper look at provider configuration in Chapter 7.\nFor each type of provider, there are many different kinds of resources that\nyou can create, such as servers, databases, and load balancers. The general\nsyntax for creating a resource in Terraform is as follows:\nresource \"<PROVIDER>_<TYPE>\" \"<NAME>\" { \n  [CONFIG ...]\n}\nwhere PROVIDER is the name of a provider (e.g., aws), TYPE is the type\nof resource to create in that provider (e.g., instance), NAME is an\nidentifier you can use throughout the Terraform code to refer to this\nresource (e.g., my_instance), and CONFIG consists of one or more\narguments that are specific to that resource.\nFor example, to deploy a single (virtual) server in AWS, known as an EC2\nInstance, use the aws_instance resource in main.tf as follows:\nresource \"aws_instance\" \"example\" {\n  ami           = \"ami-0fb653ca2d3203ac1\"\n8\n\ninstance_type = \"t2.micro\"\n}\nThe aws_instance resource supports many different arguments, but for\nnow, you only need to set the two required ones:\nami\nThe Amazon Machine Image (AMI) to run on the EC2 Instance. You\ncan find free and paid AMIs in the AWS Marketplace or create your\nown using tools such as Packer. The preceding code sets the ami\nparameter to the ID of an Ubuntu 20.04 AMI in us-east-2. This\nAMI is free to use. Please note that AMI IDs are different in every AWS\nregion, so if you change the region parameter to something other than\nus-east-2, you’ll need to manually look up the corresponding\nUbuntu AMI ID for that region, and copy it into the ami parameter. In\nChapter 7, you’ll see how to fetch the AMI ID completely\nautomatically.\ninstance_type\nThe type of EC2 Instance to run. Each type of EC2 Instance provides a\ndifferent amount of CPU, memory, disk space, and networking capacity.\nThe EC2 Instance Types page lists all the available options. The\npreceding example uses t2.micro, which has one virtual CPU, 1 GB\nof memory, and is part of the AWS Free Tier.\nUSE THE DOCS!\nTerraform supports dozens of providers, each of which supports dozens of resources,\nand each resource has dozens of arguments. There is no way to remember them all.\nWhen you’re writing Terraform code, you should be regularly referring to the Terraform\ndocumentation to look up what resources are available and how to use each one. For\nexample, here’s the documentation for the aws_instance resource. I’ve been using\nTerraform for years, and I still refer to these docs multiple times per day!\n9\n\nIn a terminal, go into the folder where you created main.tf and run the\nterraform init command:\n$ terraform init \n \nInitializing the backend... \n \nInitializing provider plugins... \n- Reusing previous version of hashicorp/aws from the dependency \nlock file \n- Using hashicorp/aws v4.19.0 from the shared cache directory \n \nTerraform has been successfully initialized!\nThe terraform binary contains the basic functionality for Terraform, but\nit does not come with the code for any of the providers (e.g., the AWS\nProvider, Azure provider, GCP provider, etc.), so when you’re first starting\nto use Terraform, you need to run terraform init to tell Terraform to\nscan the code, figure out which providers you’re using, and download the\ncode for them. By default, the provider code will be downloaded into a\n.terraform folder, which is Terraform’s scratch directory (you may want to\nadd it to .gitignore). Terraform will also record information about the\nprovider code it downloaded into a .terraform.lock.hcl file (you’ll learn\nmore about this file in “Versioned Modules”). You’ll see a few other uses\nfor the init command and .terraform folder in later chapters. For now,\njust be aware that you need to run init anytime you start with new\nTerraform code and that it’s safe to run init multiple times (the command\nis idempotent).\nNow that you have the provider code downloaded, run the terraform\nplan command:\n$ terraform plan \n \n(...) \n \nTerraform will perform the following actions: \n \n  # aws_instance.example will be created\n\n+ resource \"aws_instance\" \"example\" { \n      + ami                          = \"ami-0fb653ca2d3203ac1\" \n      + arn                          = (known after apply) \n      + associate_public_ip_address  = (known after apply) \n      + availability_zone            = (known after apply) \n      + cpu_core_count               = (known after apply) \n      + cpu_threads_per_core         = (known after apply) \n      + get_password_data            = false \n      + host_id                      = (known after apply) \n      + id                           = (known after apply) \n      + instance_state               = (known after apply) \n      + instance_type                = \"t2.micro\" \n      + ipv6_address_count           = (known after apply) \n      + ipv6_addresses               = (known after apply) \n      + key_name                     = (known after apply) \n      (...) \n  } \n \nPlan: 1 to add, 0 to change, 0 to destroy.\nThe plan command lets you see what Terraform will do before actually\nmaking any changes. This is a great way to sanity-check your code before\nunleashing it onto the world. The output of the plan command is similar to\nthe output of the diff command that is part of Unix, Linux, and git:\nanything with a plus sign (+) will be created, anything with a minus sign (–)\nwill be deleted, and anything with a tilde sign (~) will be modified in place.\nIn the preceding output, you can see that Terraform is planning on creating\na single EC2 Instance and nothing else, which is exactly what you want.\nTo actually create the Instance, run the terraform apply command:\n$ terraform apply \n \n(...) \n \nTerraform will perform the following actions: \n \n  # aws_instance.example will be created \n  + resource \"aws_instance\" \"example\" { \n      + ami                          = \"ami-0fb653ca2d3203ac1\" \n      + arn                          = (known after apply) \n      + associate_public_ip_address  = (known after apply) \n      + availability_zone            = (known after apply)\n\n+ cpu_core_count               = (known after apply) \n      + cpu_threads_per_core         = (known after apply) \n      + get_password_data            = false \n      + host_id                      = (known after apply) \n      + id                           = (known after apply) \n      + instance_state               = (known after apply) \n      + instance_type                = \"t2.micro\" \n      + ipv6_address_count           = (known after apply) \n      + ipv6_addresses               = (known after apply) \n      + key_name                     = (known after apply) \n      (...) \n  } \n \nPlan: 1 to add, 0 to change, 0 to destroy. \n \nDo you want to perform these actions? \n  Terraform will perform the actions described above. \n  Only 'yes' will be accepted to approve. \n \n  Enter a value:\nYou’ll notice that the apply command shows you the same plan output\nand asks you to confirm whether you actually want to proceed with this\nplan. So, while plan is available as a separate command, it’s mainly useful\nfor quick sanity checks and during code reviews (a topic you’ll see more of\nin Chapter 10), and most of the time you’ll run apply directly and review\nthe plan output it shows you.\nType yes and hit Enter to deploy the EC2 Instance:\nDo you want to perform these actions? \n  Terraform will perform the actions described above. \n  Only 'yes' will be accepted to approve. \n \n  Enter a value: yes \n \naws_instance.example: Creating... \naws_instance.example: Still creating... [10s elapsed] \naws_instance.example: Still creating... [20s elapsed] \naws_instance.example: Still creating... [30s elapsed] \naws_instance.example: Creation complete after 38s [id=i-\n07e2a3e006d785906] \n \nApply complete! Resources: 1 added, 0 changed, 0 destroyed.\n\nCongrats, you’ve just deployed an EC2 Instance in your AWS account\nusing Terraform! To verify this, head over to the EC2 console, and you\nshould see something similar to Figure 2-4.\n\nFigure 2-4. The AWS Console shows the EC2 Instance you deployed.\nSure enough, the Instance is there, though admittedly, this isn’t the most\nexciting example. Let’s make it a bit more interesting. First, notice that the\n\nEC2 Instance doesn’t have a name. To add one, you can add tags to the\naws_instance resource:\nresource \"aws_instance\" \"example\" {\n  ami           = \"ami-0fb653ca2d3203ac1\"\n  instance_type = \"t2.micro\" \n \n  tags = {\n    Name = \"terraform-example\" \n  }\n}\nRun terraform apply again to see what this would do:\n$ terraform apply \n \naws_instance.example: Refreshing state... \n(...) \n \nTerraform will perform the following actions: \n \n  # aws_instance.example will be updated in-place \n  ~ resource \"aws_instance\" \"example\" { \n        ami                          = \"ami-0fb653ca2d3203ac1\" \n        availability_zone            = \"us-east-2b\" \n        instance_state               = \"running\" \n        (...) \n      + tags                         = { \n          + \"Name\" = \"terraform-example\" \n        } \n        (...) \n    } \n \nPlan: 0 to add, 1 to change, 0 to destroy. \n \nDo you want to perform these actions? \n  Terraform will perform the actions described above. \n  Only 'yes' will be accepted to approve. \n \n  Enter a value:\nTerraform keeps track of all the resources it already created for this set of\nconfiguration files, so it knows your EC2 Instance already exists (notice\n\nTerraform says Refreshing state… when you run the apply\ncommand), and it can show you a diff between what’s currently deployed\nand what’s in your Terraform code (this is one of the advantages of using a\ndeclarative language over a procedural one, as discussed in “How Does\nTerraform Compare to Other IaC Tools?”). The preceding diff shows that\nTerraform wants to create a single tag called Name, which is exactly what\nyou need, so type yes and hit Enter.\nWhen you refresh your EC2 console, you’ll see something similar to\nFigure 2-5.\n\nFigure 2-5. The EC2 Instance now has a name tag.\nNow that you have some working Terraform code, you may want to store it\nin version control. This allows you to share your code with other team\n\nmembers, track the history of all infrastructure changes, and use the commit\nlog for debugging. For example, here is how you can create a local Git\nrepository and use it to store your Terraform configuration file and the lock\nfile (you’ll learn all about the lock file in Chapter 8; for now, all you need to\nknow is it should be added to version control along with your code):\ngit init \ngit add main.tf .terraform.lock.hcl \ngit commit -m \"Initial commit\"\nYou should also create a .gitignore file with the following contents:\n.terraform \n*.tfstate \n*.tfstate.backup\nThe preceding .gitignore file instructs Git to ignore the .terraform folder,\nwhich Terraform uses as a temporary scratch directory, as well as *.tfstate\nfiles, which Terraform uses to store state (in Chapter 3, you’ll see why state\nfiles shouldn’t be checked in). You should commit the .gitignore file, too:\ngit add .gitignore \ngit commit -m \"Add a .gitignore file\"\nTo share this code with your teammates, you’ll want to create a shared Git\nrepository that you can all access. One way to do this is to use GitHub.\nHead over to GitHub, create an account if you don’t have one already, and\ncreate a new repository. Configure your local Git repository to use the new\nGitHub repository as a remote endpoint named origin, as follows:\ngit remote add origin git@github.com:\n<YOUR_USERNAME>/<YOUR_REPO_NAME>.git\nNow, whenever you want to share your commits with your teammates, you\ncan push them to origin:\ngit push origin main' metadata={'original_pages_range': '90-101', 'source': '041_Deploying_a_Single_Server', 'file_path': '/mnt/c/Users/Carlos/Documents/Github/jupiter-iaa-azure/Scripts/RAG/../../data/optimized_chunks/Libro-TF/041_Deploying_a_Single_Server.pdf', 'num_pages': 12}", "page_content='And whenever you want to see changes your teammates have made, you\ncan pull them from origin:\ngit pull origin main\nAs you go through the rest of this book, and as you use Terraform in\ngeneral, make sure to regularly git commit and git push your\nchanges. This way, you’ll not only be able to collaborate with team\nmembers on this code, but all of your infrastructure changes will also be\ncaptured in the commit log, which is very handy for debugging. You’ll learn\nmore about using Terraform as a team in Chapter 10.\nDeploying a Single Web Server\nThe next step is to run a web server on this Instance. The goal is to deploy\nthe simplest web architecture possible: a single web server that can respond\nto HTTP requests, as shown in Figure 2-6.\n\nFigure 2-6. Start with a simple architecture: a single web server running in AWS that responds to\nHTTP requests.\nIn a real-world use case, you’d probably build the web server using a web\nframework like Ruby on Rails or Django, but to keep this example simple,\nlet’s run a dirt-simple web server that always returns the text “Hello,\nWorld”:\n#!/bin/bash\necho \"Hello, World\" > index.xhtml \nnohup busybox httpd -f -p 8080 &\nThis is a Bash script that writes the text “Hello, World” into index.xhtml and\nruns a tool called busybox (which is installed by default on Ubuntu) to\n10\n\nfire up a web server on port 8080 to serve that file. I wrapped the busybox\ncommand with nohup and an ampersand (&) so that the web server runs\npermanently in the background, whereas the Bash script itself can exit.\nPORT NUMBERS\nThe reason this example uses port 8080, rather than the default HTTP port 80, is that\nlistening on any port less than 1024 requires root user privileges. This is a security risk\nsince any attacker who manages to compromise your server would get root privileges,\ntoo.\nTherefore, it’s a best practice to run your web server with a non-root user that has\nlimited permissions. That means you have to listen on higher-numbered ports, but as\nyou’ll see later in this chapter, you can configure a load balancer to listen on port 80 and\nroute traffic to the high-numbered ports on your server(s).\nHow do you get the EC2 Instance to run this script? Normally, as discussed\nin “Server Templating Tools”, you would use a tool like Packer to create a\ncustom AMI that has the web server installed on it. Since the dummy web\nserver in this example is just a one-liner that uses busybox, you can use a\nplain Ubuntu 20.04 AMI and run the “Hello, World” script as part of the\nEC2 Instance’s User Data configuration. When you launch an EC2\nInstance, you have the option of passing either a shell script or cloud-init\ndirective to User Data, and the EC2 Instance will execute it during its very\nfirst boot. You pass a shell script to User Data by setting the user_data\nargument in your Terraform code as follows:\nresource \"aws_instance\" \"example\" {\n  ami                    = \"ami-0fb653ca2d3203ac1\"\n  instance_type          = \"t2.micro\" \n \n  user_data = <<-EOF\n              #!/bin/bash \n              echo \"Hello, World\" > index.xhtml \n              nohup busybox httpd -f -p 8080 & \n              EOF \n \n  user_data_replace_on_change = true\n\ntags = {\n    Name = \"terraform-example\" \n  }\n}\nTwo things to notice about the preceding code:\nThe <<-EOF and EOF are Terraform’s heredoc syntax, which allows\nyou to create multiline strings without having to insert \\n characters\nall over the place.\nThe user_data_replace_on_change parameter is set to true\nso that when you change the user_data parameter and run apply,\nTerraform will terminate the original instance and launch a totally new\none. Terraform’s default behavior is to update the original instance in\nplace, but since User Data runs only on the very first boot, and your\noriginal instance already went through that boot process, you need to\nforce the creation of a new instance to ensure your new User Data\nscript actually gets executed.\nYou need to do one more thing before this web server works. By default,\nAWS does not allow any incoming or outgoing traffic from an EC2\nInstance. To allow the EC2 Instance to receive traffic on port 8080, you\nneed to create a security group:\nresource \"aws_security_group\" \"instance\" {\n  name = \"terraform-example-instance\" \n \n  ingress {\n    from_port   = 8080\n    to_port     = 8080\n    protocol    = \"tcp\"\n    cidr_blocks = [\"0.0.0.0/0\"] \n  }\n}\nThis code creates a new resource called aws_security_group (notice\nhow all resources for the AWS Provider begin with aws_) and specifies\nthat this group allows incoming TCP requests on port 8080 from the CIDR\n\nblock 0.0.0.0/0. CIDR blocks are a concise way to specify IP address\nranges. For example, a CIDR block of 10.0.0.0/24 represents all IP\naddresses between 10.0.0.0 and 10.0.0.255. The CIDR block 0.0.0.0/0 is an\nIP address range that includes all possible IP addresses, so this security\ngroup allows incoming requests on port 8080 from any IP.\nSimply creating a security group isn’t enough; you need to tell the EC2\nInstance to actually use it by passing the ID of the security group into the\nvpc_security _group_ids argument of the aws_instance\nresource. To do that, you first need to learn about Terraform expressions.\nAn expression in Terraform is anything that returns a value. You’ve already\nseen the simplest type of expressions, literals, such as strings (e.g., \"ami-\n0fb653ca2d3203ac1\") and numbers (e.g., 5). Terraform supports\nmany other types of expressions that you’ll see throughout the book.\nOne particularly useful type of expression is a reference, which allows you\nto access values from other parts of your code. To access the ID of the\nsecurity group resource, you are going to need to use a resource attribute\nreference, which uses the following syntax:\n<PROVIDER>_<TYPE>.<NAME>.<ATTRIBUTE>\nwhere PROVIDER is the name of the provider (e.g., aws), TYPE is the type\nof resource (e.g., security_group), NAME is the name of that resource\n(e.g., the security group is named \"instance\"), and ATTRIBUTE is\neither one of the arguments of that resource (e.g., name) or one of the\nattributes exported by the resource (you can find the list of available\nattributes in the documentation for each resource). The security group\nexports an attribute called id, so the expression to reference it will look\nlike this:\naws_security_group.instance.id\nYou can use this security group ID in the vpc_security_group_ids\nargument of the aws_instance as follows:\n11\n\nresource \"aws_instance\" \"example\" {\n  ami                    = \"ami-0fb653ca2d3203ac1\"\n  instance_type          = \"t2.micro\"\n  vpc_security_group_ids = [aws_security_group.instance.id] \n \n  user_data = <<-EOF\n              #!/bin/bash \n              echo \"Hello, World\" > index.xhtml \n              nohup busybox httpd -f -p 8080 & \n              EOF \n \n  user_data_replace_on_change = true \n \n  tags = {\n    Name = \"terraform-example\" \n  }\n}\nWhen you add a reference from one resource to another, you create an\nimplicit dependency. Terraform parses these dependencies, builds a\ndependency graph from them, and uses that to automatically determine in\nwhich order it should create resources. For example, if you were deploying\nthis code from scratch, Terraform would know that it needs to create the\nsecurity group before the EC2 Instance, because the EC2 Instance\nreferences the ID of the security group. You can even get Terraform to show\nyou the dependency graph by running the graph command:\n$ terraform graph \n \ndigraph { \n compound = \"true\" \n newrank = \"true\" \n subgraph \"root\" { \n  \"[root] aws_instance.example\" \n    [label = \"aws_instance.example\", shape = \"box\"] \n  \"[root] aws_security_group.instance\" \n    [label = \"aws_security_group.instance\", shape = \n\"box\"] \n  \"[root] provider.aws\" \n    [label = \"provider.aws\", shape = \"diamond\"] \n  \"[root] aws_instance.example\" -> \n    \"[root] aws_security_group.instance\" \n  \"[root] aws_security_group.instance\" -> \n    \"[root] provider.aws\"\n\n\"[root] meta.count-boundary (EachMode fixup)\" -> \n    \"[root] aws_instance.example\" \n  \"[root] provider.aws (close)\" -> \n    \"[root] aws_instance.example\" \n  \"[root] root\" -> \n    \"[root] meta.count-boundary (EachMode fixup)\" \n  \"[root] root\" -> \n    \"[root] provider.aws (close)\" \n } \n}\nThe output is in a graph description language called DOT, which you can\nturn into an image, similar to the dependency graph shown in Figure 2-7, by\nusing a desktop app such as Graphviz or web app like GraphvizOnline.12\n\nFigure 2-7. This is what the dependency graph for the EC2 Instance and its security group looks like\nwhen rendered with Graphviz.\nWhen Terraform walks your dependency tree, it creates as many resources\nin parallel as it can, which means that it can apply your changes fairly\nefficiently. That’s the beauty of a declarative language: you just specify\n\nwhat you want, and Terraform determines the most efficient way to make it\nhappen.\nIf you run the apply command, you’ll see that Terraform wants to create a\nsecurity group and replace the EC2 Instance with a new one that has the\nnew user data:\n$ terraform apply \n \n(...) \n \nTerraform will perform the following actions: \n \n  # aws_instance.example must be replaced \n-/+ resource \"aws_instance\" \"example\" { \n        ami                          = \"ami-0fb653ca2d3203ac1\" \n      ~ availability_zone            = \"us-east-2c\" -> (known \nafter apply) \n      ~ instance_state               = \"running\" -> (known after \napply) \n        instance_type                = \"t2.micro\" \n        (...) \n      + user_data                    = \"c765373...\" # forces \nreplacement \n      ~ volume_tags                  = {} -> (known after apply) \n      ~ vpc_security_group_ids       = [ \n          - \"sg-871fa9ec\", \n        ] -> (known after apply) \n        (...) \n    } \n \n  # aws_security_group.instance will be created \n  + resource \"aws_security_group\" \"instance\" { \n      + arn                    = (known after apply) \n      + description            = \"Managed by Terraform\" \n      + egress                 = (known after apply) \n      + id                     = (known after apply) \n      + ingress                = [ \n          + { \n              + cidr_blocks      = [ \n                  + \"0.0.0.0/0\", \n                ] \n              + description      = \"\" \n              + from_port        = 8080 \n              + ipv6_cidr_blocks = [] \n              + prefix_list_ids  = []\n\n+ protocol         = \"tcp\" \n              + security_groups  = [] \n              + self             = false \n              + to_port          = 8080 \n            }, \n        ] \n      + name                   = \"terraform-example-instance\" \n      + owner_id               = (known after apply) \n      + revoke_rules_on_delete = false \n      + vpc_id                 = (known after apply) \n    } \n \nPlan: 2 to add, 0 to change, 1 to destroy. \n \nDo you want to perform these actions? \n  Terraform will perform the actions described above. \n  Only 'yes' will be accepted to approve. \n \n  Enter a value:\nThe -/+ in the plan output means “replace”; look for the text “forces\nreplacement” in the plan output to figure out what is forcing Terraform to\ndo a replacement. Since you set user_data_replace_on_change to\ntrue and changed the user_data parameter, this will force a\nreplacement, which means that the original EC2 Instance will be terminated\nand a completely new Instance will be created. This is an example of the\nimmutable infrastructure paradigm discussed in “Server Templating Tools”.\nIt’s worth mentioning that while the web server is being replaced, any users\nof that web server would experience downtime; you’ll see how to do a zero-\ndowntime deployment with Terraform in Chapter 5.\nSince the plan looks good, enter yes, and you’ll see your new EC2\nInstance deploying, as shown in Figure 2-8.\n\nFigure 2-8. The new EC2 Instance with the web server code replaces the old Instance.\nIf you click your new Instance, you can find its public IP address in the\ndescription panel at the bottom of the screen. Give the Instance a minute or' metadata={'original_pages_range': '102-112', 'source': '042_Deploying_a_Single_Web_Server', 'file_path': '/mnt/c/Users/Carlos/Documents/Github/jupiter-iaa-azure/Scripts/RAG/../../data/optimized_chunks/Libro-TF/042_Deploying_a_Single_Web_Server.pdf', 'num_pages': 11}", "page_content='two to boot up, and then use a web browser or a tool like curl to make an\nHTTP request to this IP address at port 8080:\n$ curl http://<EC2_INSTANCE_PUBLIC_IP>:8080 \nHello, World\nYay! You now have a working web server running in AWS!\nNETWORK SECURITY\nTo keep all of the examples in this book simple, they deploy not only\ninto your Default VPC (as mentioned earlier) but also into the default\nsubnets of that VPC. A VPC is partitioned into one or more subnets,\neach with its own IP addresses. The subnets in the Default VPC are all\npublic subnets, which means they get IP addresses that are accessible\nfrom the public internet. This is why you are able to test your EC2\nInstance from your home computer.\nRunning a server in a public subnet is fine for a quick experiment, but\nin real-world usage, it’s a security risk. Hackers all over the world are\nconstantly scanning IP addresses at random for any weakness. If your\nservers are exposed publicly, all it takes is accidentally leaving a single\nport unprotected or running out-of-date code with a known\nvulnerability, and someone can break in.\nTherefore, for production systems, you should deploy all of your\nservers, and certainly all of your data stores, in private subnets, which\nhave IP addresses that can be accessed only from within the VPC and\nnot from the public internet. The only servers you should run in public\nsubnets are a small number of reverse proxies and load balancers that\nyou lock down as much as possible (you’ll see an example of how to\ndeploy a load balancer later in this chapter).\nDeploying a Configurable Web Server\n\nYou might have noticed that the web server code has the port 8080\nduplicated in both the security group and the User Data configuration. This\nviolates the Don’t Repeat Yourself (DRY) principle: every piece of\nknowledge must have a single, unambiguous, authoritative representation\nwithin a system. If you have the port number in two places, it’s easy to\nupdate it in one place but forget to make the same change in the other place.\nTo allow you to make your code more DRY and more configurable,\nTerraform allows you to define input variables. Here’s the syntax for\ndeclaring a variable:\nvariable \"NAME\" { \n  [CONFIG ...]\n}\nThe body of the variable declaration can contain the following optional\nparameters:\ndescription\nIt’s always a good idea to use this parameter to document how a\nvariable is used. Your teammates will be able to see this description not\nonly while reading the code but also when running the plan or apply\ncommands (you’ll see an example of this shortly).\ndefault\nThere are a number of ways to provide a value for the variable,\nincluding passing it in at the command line (using the -var option), via\na file (using the -var-file option), or via an environment variable\n(Terraform looks for environment variables of the name\nTF_VAR_<variable_name>). If no value is passed in, the variable\nwill fall back to this default value. If there is no default value, Terraform\nwill interactively prompt the user for one.\ntype\n13\n\nThis allows you to enforce type constraints on the variables a user\npasses in. Terraform supports a number of type constraints, including\nstring, number, bool, list, map, set, object, tuple, and\nany. It’s always a good idea to define a type constraint to catch simple\nerrors. If you don’t specify a type, Terraform assumes the type is any.\nvalidation\nThis allows you to define custom validation rules for the input variable\nthat go beyond basic type checks, such as enforcing minimum or\nmaximum values on a number. You’ll see an example of validations in\nChapter 8.\nsensitive\nIf you set this parameter to true on an input variable, Terraform will\nnot log it when you run plan or apply. You should use this on any\nsecrets you pass into your Terraform code via variables: e.g., passwords,\nAPI keys, etc. I’ll talk more about secrets in Chapter 6.\nHere is an example of an input variable that checks to verify that the value\nyou pass in is a number:\nvariable \"number_example\" {\n  description = \"An example of a number variable in Terraform\"\n  type        = number\n  default     = 42\n}\nAnd here’s an example of a variable that checks whether the value is a list:\nvariable \"list_example\" {\n  description = \"An example of a list in Terraform\"\n  type        = list\n  default     = [\"a\", \"b\", \"c\"]\n}\n\nYou can combine type constraints, too. For example, here’s a list input\nvariable that requires all of the items in the list to be numbers:\nvariable \"list_numeric_example\" {\n  description = \"An example of a numeric list in Terraform\"\n  type        = list(number)\n  default     = [1, 2, 3]\n}\nAnd here’s a map that requires all of the values to be strings:\nvariable \"map_example\" {\n  description = \"An example of a map in Terraform\"\n  type        = map(string) \n \n  default = {\n    key1 = \"value1\"\n    key2 = \"value2\"\n    key3 = \"value3\" \n  }\n}\nYou can also create more complicated structural types using the object\ntype constraint:\nvariable \"object_example\" {\n  description = \"An example of a structural type in Terraform\"\n  type        = object({\n    name    = string\n    age     = number\n    tags    = list(string)\n    enabled = bool \n  }) \n \n  default = {\n    name    = \"value1\"\n    age     = 42\n    tags    = [\"a\", \"b\", \"c\"]\n    enabled = true \n  }\n}\n\nThe preceding example creates an input variable that will require the value\nto be an object with the keys name (which must be a string), age (which\nmust be a number), tags (which must be a list of strings), and enabled\n(which must be a Boolean). If you try to set this variable to a value that\ndoesn’t match this type, Terraform immediately gives you a type error. The\nfollowing example demonstrates trying to set enabled to a string instead\nof a Boolean:\nvariable \"object_example_with_error\" {\n  description = \"An example of a structural type in Terraform \nwith an error\"\n  type        = object({\n    name    = string\n    age     = number\n    tags    = list(string)\n    enabled = bool \n  }) \n \n  default = {\n    name    = \"value1\"\n    age     = 42\n    tags    = [\"a\", \"b\", \"c\"]\n    enabled = \"invalid\" \n  }\n}\nYou get the following error:\n$ terraform apply \n \nError: Invalid default value for variable \n \n  on variables.tf line 78, in variable \n\"object_example_with_error\": \n  78:   default = { \n  79:     name    = \"value1\" \n  80:     age     = 42 \n  81:     tags    = [\"a\", \"b\", \"c\"] \n  82:     enabled = \"invalid\" \n  83:   } \n \nThis default value is not compatible with the variable's type\n\nconstraint: a \nbool is required.\nComing back to the web server example, what you need is a variable that\nstores the port number:\nvariable \"server_port\" {\n  description = \"The port the server will use for HTTP requests\"\n  type        = number\n}\nNote that the server_port input variable has no default, so if you\nrun the apply command now, Terraform will interactively prompt you to\nenter a value for server_port and show you the description of the\nvariable:\n$ terraform apply \n \nvar.server_port \n  The port the server will use for HTTP requests \n \n  Enter a value:\nIf you don’t want to deal with an interactive prompt, you can provide a\nvalue for the variable via the -var command-line option:\n$ terraform plan -var \"server_port=8080\"\nYou could also set the variable via an environment variable named\nTF_VAR_<name>, where <name> is the name of the variable you’re\ntrying to set:\n$ export TF_VAR_server_port=8080 \n$ terraform plan\nAnd if you don’t want to deal with remembering extra command-line\narguments every time you run plan or apply, you can specify a\ndefault value:\n\nvariable \"server_port\" {\n  description = \"The port the server will use for HTTP requests\"\n  type        = number\n  default     = 8080\n}\nTo use the value from an input variable in your Terraform code, you can use\na new type of expression called a variable reference, which has the\nfollowing syntax:\nvar.<VARIABLE_NAME>\nFor example, here is how you can set the from_port and to_port\nparameters of the security group to the value of the server_port\nvariable:\nresource \"aws_security_group\" \"instance\" {\n  name = \"terraform-example-instance\" \n \n  ingress {\n    from_port   = var.server_port\n    to_port     = var.server_port\n    protocol    = \"tcp\"\n    cidr_blocks = [\"0.0.0.0/0\"] \n  }\n}\nIt’s also a good idea to use the same variable when setting the port in the\nUser Data script. To use a reference inside of a string literal, you need to\nuse a new type of expression called an interpolation, which has the\nfollowing syntax:\n\"${...}\"\nYou can put any valid reference within the curly braces, and Terraform will\nconvert it to a string. For example, here’s how you can use\nvar.server_port inside of the User Data string:\n\nuser_data = <<-EOF\n              #!/bin/bash \n              echo \"Hello, World\" > index.xhtml \n              nohup busybox httpd -f -p ${var.server_port} & \n              EOF\nIn addition to input variables, Terraform also allows you to define output\nvariables by using the following syntax:\noutput \"<NAME>\" {\n  value = <VALUE> \n  [CONFIG ...]\n}\nThe NAME is the name of the output variable, and VALUE can be any\nTerraform expression that you would like to output. The CONFIG can\ncontain the following optional parameters:\ndescription\nIt’s always a good idea to use this parameter to document what type of\ndata is contained in the output variable.\nsensitive\nSet this parameter to true to instruct Terraform not to log this output at\nthe end of plan or apply. This is useful if the output variable\ncontains secrets such as passwords or private keys. Note that if your\noutput variable references an input variable or resource attribute marked\nwith sensitive = true, you are required to mark the output\nvariable with sensitive = true as well to indicate you are\nintentionally outputting a secret.\ndepends_on\nNormally, Terraform automatically figures out your dependency graph\nbased on the references within your code, but in rare situations, you\nhave to give it extra hints. For example, perhaps you have an output\nvariable that returns the IP address of a server, but that IP won’t be\n\naccessible until a security group (firewall) is properly configured for\nthat server. In that case, you may explicitly tell Terraform there is a\ndependency between the IP address output variable and the security\ngroup resource using depends_on.\nFor example, instead of having to manually poke around the EC2 console to\nfind the IP address of your server, you can provide the IP address as an\noutput variable:\noutput \"public_ip\" {\n  value       = aws_instance.example.public_ip\n  description = \"The public IP address of the web server\"\n}\nThis code uses an attribute reference again, this time referencing the\npublic_ip attribute of the aws_instance resource. If you run the\napply command again, Terraform will not apply any changes (because\nyou haven’t changed any resources), but it will show you the new output at\nthe very end:\n$ terraform apply \n \n(...) \n \naws_security_group.instance: Refreshing state... [id=sg-\n078ccb4f9533d2c1a] \naws_instance.example: Refreshing state... [id=i-\n028cad2d4e6bddec6] \n \nApply complete! Resources: 0 added, 0 changed, 0 destroyed. \n \nOutputs: \n \npublic_ip = \"54.174.13.5\"\nAs you can see, output variables show up in the console after you run\nterraform apply, which users of your Terraform code might find\nuseful (e.g., you now know what IP to test after the web server is deployed).' metadata={'original_pages_range': '113-121', 'source': '043_Deploying_a_Configurable_Web_Server', 'file_path': '/mnt/c/Users/Carlos/Documents/Github/jupiter-iaa-azure/Scripts/RAG/../../data/optimized_chunks/Libro-TF/043_Deploying_a_Configurable_Web_Server.pdf', 'num_pages': 9}", "page_content='You can also use the terraform output command to list all outputs\nwithout applying any changes:\n$ terraform output \npublic_ip = \"54.174.13.5\"\nAnd you can run terraform output <OUTPUT_NAME> to see the\nvalue of a specific output called <OUTPUT_NAME>:\n$ terraform output public_ip \n\"54.174.13.5\"\nThis is particularly handy for scripting. For example, you could create a\ndeployment script that runs terraform apply to deploy the web server,\nuses terraform output public_ip to grab its public IP, and runs\ncurl on the IP as a quick smoke test to validate that the deployment\nworked.\nInput and output variables are also essential ingredients in creating\nconfigurable and reusable infrastructure code, a topic you’ll see more of in\nChapter 4.\nDeploying a Cluster of Web Servers\nRunning a single server is a good start, but in the real world, a single server\nis a single point of failure. If that server crashes, or if it becomes overloaded\nfrom too much traffic, users will be unable to access your site. The solution\nis to run a cluster of servers, routing around servers that go down and\nadjusting the size of the cluster up or down based on traffic.\nManaging such a cluster manually is a lot of work. Fortunately, you can let\nAWS take care of it for you by using an Auto Scaling Group (ASG), as\nshown in Figure 2-9. An ASG takes care of a lot of tasks for you\ncompletely automatically, including launching a cluster of EC2 Instances,\nmonitoring the health of each Instance, replacing failed Instances, and\nadjusting the size of the cluster in response to load.\n14\n\nFigure 2-9. Instead of a single web server, run a cluster of web servers using an Auto Scaling Group.\nThe first step in creating an ASG is to create a launch configuration, which\nspecifies how to configure each EC2 Instance in the ASG. The15\n\naws_launch_configuration resource uses almost the same\nparameters as the aws_instance resource, although it doesn’t\nsupport tags (you’ll handle these in the aws_autoscaling_group\nresource later) or the user_data_replace_on_change parameter\n(ASGs launch new instances by default, so you don’t need this parameter),\nand two of the parameters have different names (ami is now image_id,\nand vpc_security_group_ids is now security_groups), so\nreplace aws_instance with aws_launch_configuration as\nfollows:\nresource \"aws_launch_configuration\" \"example\" {\n  image_id        = \"ami-0fb653ca2d3203ac1\"\n  instance_type   = \"t2.micro\"\n  security_groups = [aws_security_group.instance.id] \n \n  user_data = <<-EOF\n              #!/bin/bash \n              echo \"Hello, World\" > index.xhtml \n              nohup busybox httpd -f -p ${var.server_port} & \n              EOF\n}\nNow you can create the ASG itself using the\naws_autoscaling_group resource:\nresource \"aws_autoscaling_group\" \"example\" {\n  launch_configuration = aws_launch_configuration.example.name \n \n  min_size = 2\n  max_size = 10 \n \n  tag {\n    key                 = \"Name\"\n    value               = \"terraform-asg-example\"\n    propagate_at_launch = true \n  }\n}\nThis ASG will run between 2 and 10 EC2 Instances (defaulting to 2 for the\ninitial launch), each tagged with the name terraform-asg-example.\n\nNote that the ASG uses a reference to fill in the launch configuration name.\nThis leads to a problem: launch configurations are immutable, so if you\nchange any parameter of your launch configuration, Terraform will try to\nreplace it. Normally, when replacing a resource, Terraform would delete the\nold resource first and then creates its replacement, but because your ASG\nnow has a reference to the old resource, Terraform won’t be able to delete\nit.\nTo solve this problem, you can use a lifecycle setting. Every Terraform\nresource supports several lifecycle settings that configure how that resource\nis created, updated, and/or deleted. A particularly useful lifecycle setting is\ncreate_before_destroy. If you set create_before_destroy\nto true, Terraform will invert the order in which it replaces resources,\ncreating the replacement resource first (including updating any references\nthat were pointing at the old resource to point to the replacement) and then\ndeleting the old resource. Add the lifecycle block to your\naws_launch _configuration as follows:\nresource \"aws_launch_configuration\" \"example\" {\n  image_id        = \"ami-0fb653ca2d3203ac1\"\n  instance_type   = \"t2.micro\"\n  security_groups = [aws_security_group.instance.id] \n \n  user_data = <<-EOF\n              #!/bin/bash \n              echo \"Hello, World\" > index.xhtml \n              nohup busybox httpd -f -p ${var.server_port} & \n              EOF \n \n  # Required when using a launch configuration with an auto \nscaling group. \n  lifecycle {\n    create_before_destroy = true \n  }\n}\nThere’s also one other parameter that you need to add to your ASG to make\nit work: subnet_ids. This parameter specifies to the ASG into which\nVPC subnets the EC2 Instances should be deployed (see “Network\n\nSecurity” for background info on subnets). Each subnet lives in an isolated\nAWS AZ (that is, isolated datacenter), so by deploying your Instances\nacross multiple subnets, you ensure that your service can keep running even\nif some of the datacenters have an outage. You could hardcode the list of\nsubnets, but that won’t be maintainable or portable, so a better option is to\nuse data sources to get the list of subnets in your AWS account.\nA data source represents a piece of read-only information that is fetched\nfrom the provider (in this case, AWS) every time you run Terraform.\nAdding a data source to your Terraform configurations does not create\nanything new; it’s just a way to query the provider’s APIs for data and to\nmake that data available to the rest of your Terraform code. Each Terraform\nprovider exposes a variety of data sources. For example, the AWS Provider\nincludes data sources to look up VPC data, subnet data, AMI IDs, IP\naddress ranges, the current user’s identity, and much more.\nThe syntax for using a data source is very similar to the syntax of a\nresource:\ndata \"<PROVIDER>_<TYPE>\" \"<NAME>\" { \n  [CONFIG ...]\n}\nHere, PROVIDER is the name of a provider (e.g., aws), TYPE is the type of\ndata source you want to use (e.g., vpc), NAME is an identifier you can use\nthroughout the Terraform code to refer to this data source, and CONFIG\nconsists of one or more arguments that are specific to that data source. For\nexample, here is how you can use the aws_vpc data source to look up the\ndata for your Default VPC (see “A Note on Default Virtual Private Clouds”\nfor background information):\ndata \"aws_vpc\" \"default\" {\n  default = true\n}\nNote that with data sources, the arguments you pass in are typically search\nfilters that indicate to the data source what information you’re looking for.\n\nWith the aws_vpc data source, the only filter you need is default =\ntrue, which directs Terraform to look up the Default VPC in your AWS\naccount.\nTo get the data out of a data source, you use the following attribute\nreference syntax:\ndata.<PROVIDER>_<TYPE>.<NAME>.<ATTRIBUTE>\nFor example, to get the ID of the VPC from the aws_vpc data source, you\nwould use the following:\ndata.aws_vpc.default.id\nYou can combine this with another data source, aws_subnets, to look up\nthe subnets\nwithin that VPC:\ndata \"aws_subnets\" \"default\" { \n  filter {\n    name   = \"vpc-id\"\n    values = [data.aws_vpc.default.id] \n  }\n}\nFinally, you can pull the subnet IDs out of the aws_subnets data source\nand tell your ASG to use those subnets via the (somewhat oddly named)\nvpc_zone_identifier argument:\nresource \"aws_autoscaling_group\" \"example\" {\n  launch_configuration = aws_launch_configuration.example.name\n  vpc_zone_identifier  = data.aws_subnets.default.ids \n \n  min_size = 2\n  max_size = 10 \n \n  tag {\n    key                 = \"Name\"\n    value               = \"terraform-asg-example\"' metadata={'original_pages_range': '122-127', 'source': '044_Deploying_a_Cluster_of_Web_Servers', 'file_path': '/mnt/c/Users/Carlos/Documents/Github/jupiter-iaa-azure/Scripts/RAG/../../data/optimized_chunks/Libro-TF/044_Deploying_a_Cluster_of_Web_Servers.pdf', 'num_pages': 6}", "page_content='propagate_at_launch = true \n  }\n}\nDeploying a Load Balancer\nAt this point, you can deploy your ASG, but you’ll have a small problem:\nyou now have multiple servers, each with its own IP address, but you\ntypically want to give your end users only a single IP to use. One way to\nsolve this problem is to deploy a load balancer to distribute traffic across\nyour servers and to give all your users the IP (actually, the DNS name) of\nthe load balancer. Creating a load balancer that is highly available and\nscalable is a lot of work. Once again, you can let AWS take care of it for\nyou, this time by using Amazon’s Elastic Load Balancer (ELB) service, as\nshown in Figure 2-10.\n\nFigure 2-10. Use Amazon ELB to distribute traffic across the Auto Scaling Group.\nAWS offers three types of load balancers:\nApplication Load Balancer (ALB)\n\nBest suited for load balancing of HTTP and HTTPS traffic. Operates at\nthe application layer (Layer 7) of the Open Systems Interconnection\n(OSI) model.\nNetwork Load Balancer (NLB)\nBest suited for load balancing of TCP, UDP, and TLS traffic. Can scale\nup and down in response to load faster than the ALB (the NLB is\ndesigned to scale to tens of millions of requests per second). Operates at\nthe transport layer (Layer 4) of the OSI model.\nClassic Load Balancer (CLB)\nThis is the “legacy” load balancer that predates both the ALB and NLB.\nIt can handle HTTP, HTTPS, TCP, and TLS traffic but with far fewer\nfeatures than either the ALB or NLB. Operates at both the application\nlayer (L7) and transport layer (L4) of the OSI model.\nMost applications these days should use either the ALB or the NLB.\nBecause the simple web server example you’re working on is an HTTP app\nwithout any extreme performance requirements, the ALB is going to be the\nbest fit.\nThe ALB consists of several parts, as shown in Figure 2-11:\nListener\nListens on a specific port (e.g., 80) and protocol (e.g., HTTP).\nListener rule\nTakes requests that come into a listener and sends those that match\nspecific paths (e.g., /foo and /bar) or hostnames (e.g.,\nfoo.example.com and bar.example.com) to specific target\ngroups.\nTarget groups\n\nOne or more servers that receive requests from the load balancer. The\ntarget group also performs health checks on these servers and sends\nrequests only to healthy nodes.\nFigure 2-11. An ALB consists of listeners, listener rules, and target groups.\nThe first step is to create the ALB itself using the aws_lb resource:\nresource \"aws_lb\" \"example\" {\n  name               = \"terraform-asg-example\"\n  load_balancer_type = \"application\"\n  subnets            = data.aws_subnets.default.ids\n}\nNote that the subnets parameter configures the load balancer to use all\nthe subnets in your Default VPC by using the aws_subnets data\nsource. AWS load balancers don’t consist of a single server, but of\nmultiple servers that can run in separate subnets (and, therefore, separate\ndatacenters). AWS automatically scales the number of load balancer servers\nup and down based on traffic and handles failover if one of those servers\ngoes down, so you get scalability and high availability out of the box.\n16\n\nThe next step is to define a listener for this ALB using the\naws_lb_listener resource:\nresource \"aws_lb_listener\" \"http\" {\n  load_balancer_arn = aws_lb.example.arn\n  port              = 80\n  protocol          = \"HTTP\" \n \n  # By default, return a simple 404 page \n  default_action {\n    type = \"fixed-response\" \n \n    fixed_response {\n      content_type = \"text/plain\"\n      message_body = \"404: page not found\"\n      status_code  = 404 \n    } \n  }\n}\nThis listener configures the ALB to listen on the default HTTP port, port\n80, use HTTP as the protocol, and send a simple 404 page as the default\nresponse for requests that don’t match any listener rules.\nNote that, by default, all AWS resources, including ALBs, don’t allow any\nincoming or outgoing traffic, so you need to create a new security group\nspecifically for the ALB. This security group should allow incoming\nrequests on port 80 so that you can access the load balancer over HTTP, and\nallow outgoing requests on all ports so that the load balancer can perform\nhealth checks:\nresource \"aws_security_group\" \"alb\" {\n  name = \"terraform-example-alb\" \n \n  # Allow inbound HTTP requests \n  ingress {\n    from_port   = 80\n    to_port     = 80\n    protocol    = \"tcp\"\n    cidr_blocks = [\"0.0.0.0/0\"] \n  }\n\n# Allow all outbound requests \n  egress {\n    from_port   = 0\n    to_port     = 0\n    protocol    = \"-1\"\n    cidr_blocks = [\"0.0.0.0/0\"] \n  }\n}\nYou’ll need to tell the aws_lb resource to use this security group via the\nsecurity_groups argument:\nresource \"aws_lb\" \"example\" {\n  name               = \"terraform-asg-example\"\n  load_balancer_type = \"application\"\n  subnets            = data.aws_subnets.default.ids\n  security_groups    = [aws_security_group.alb.id]\n}\nNext, you need to create a target group for your ASG using the\naws_lb_target_group resource:\nresource \"aws_lb_target_group\" \"asg\" {\n  name     = \"terraform-asg-example\"\n  port     = var.server_port\n  protocol = \"HTTP\"\n  vpc_id   = data.aws_vpc.default.id \n \n  health_check {\n    path                = \"/\"\n    protocol            = \"HTTP\"\n    matcher             = \"200\"\n    interval            = 15\n    timeout             = 3\n    healthy_threshold   = 2\n    unhealthy_threshold = 2 \n  }\n}\nThis target group will health check your Instances by periodically sending\nan HTTP request to each Instance and will consider the Instance “healthy”\nonly if the Instance returns a response that matches the configured\n\nmatcher (e.g., you can configure a matcher to look for a 200 OK\nresponse). If an Instance fails to respond, perhaps because that Instance has\ngone down or is overloaded, it will be marked as “unhealthy,” and the target\ngroup will automatically stop sending traffic to it to minimize disruption for\nyour users.\nHow does the target group know which EC2 Instances to send requests to?\nYou could attach a static list of EC2 Instances to the target group using the\naws_lb_target_group_attachment resource, but with an ASG,\nInstances can launch or terminate at any time, so a static list won’t work.\nInstead, you can take advantage of the first-class integration between the\nASG and the ALB. Go back to the aws _autoscaling_group resource,\nand set its target_group_arns argument to point at your new target\ngroup:\nresource \"aws_autoscaling_group\" \"example\" {\n  launch_configuration = aws_launch_configuration.example.name\n  vpc_zone_identifier  = data.aws_subnets.default.ids \n \n  target_group_arns = [aws_lb_target_group.asg.arn]\n  health_check_type = \"ELB\" \n \n  min_size = 2\n  max_size = 10 \n \n  tag {\n    key                 = \"Name\"\n    value               = \"terraform-asg-example\"\n    propagate_at_launch = true \n  }\n}\nYou should also update the health_check_type to \"ELB\". The\ndefault health_check_type is \"EC2\", which is a minimal health\ncheck that considers an Instance unhealthy only if the AWS hypervisor says\nthe VM is completely down or unreachable. The \"ELB\" health check is\nmore robust, because it instructs the ASG to use the target group’s health\ncheck to determine whether an Instance is healthy and to automatically\nreplace Instances if the target group reports them as unhealthy. That way,\n\nInstances will be replaced not only if they are completely down but also if,\nfor example, they’ve stopped serving requests because they ran out of\nmemory or a critical process crashed.\nFinally, it’s time to tie all these pieces together by creating listener rules\nusing the aws_lb_listener_rule resource:\nresource \"aws_lb_listener_rule\" \"asg\" {\n  listener_arn = aws_lb_listener.http.arn\n  priority     = 100 \n \n  condition { \n    path_pattern {\n      values = [\"*\"] \n    } \n  } \n \n  action {\n    type             = \"forward\"\n    target_group_arn = aws_lb_target_group.asg.arn \n  }\n}\nThe preceding code adds a listener rule that sends requests that match any\npath to the target group that contains your ASG.\nThere’s one last thing to do before you deploy the load balancer—replace\nthe old public_ip output of the single EC2 Instance you had before with\nan output that shows the DNS name of the ALB:\noutput \"alb_dns_name\" {\n  value       = aws_lb.example.dns_name\n  description = \"The domain name of the load balancer\"\n}\nRun terraform apply, and read through the plan output. You should\nsee that your original single EC2 Instance is being removed, and in its\nplace, Terraform will create a launch configuration, ASG, ALB, and a\nsecurity group. If the plan looks good, type yes and hit Enter. When\napply completes, you should see the alb_dns_name output:\n\nOutputs: \nalb_dns_name = \"terraform-asg-example-123.us-east-\n2.elb.amazonaws.com\"\nCopy down this URL. It’ll take a couple minutes for the Instances to boot\nand show up as healthy in the ALB. In the meantime, you can inspect what\nyou’ve deployed. Open up the ASG section of the EC2 console, and you\nshould see that the ASG has been created, as shown in Figure 2-12.\n\nFigure 2-12. The AWS Console shows all the ASGs you’ve created.\nIf you switch over to the Instances tab, you’ll see the two EC2 Instances\nlaunching, as shown in Figure 2-13.\n\nFigure 2-13. The EC2 Instances in the ASG are launching.\nIf you click the Load Balancers tab, you’ll see your ALB, as shown in\nFigure 2-14.\n\nFigure 2-14. The AWS Console shows all the ALBs you’ve created.\nFinally, if you click the Target Groups tab, you can find your target group,\nas shown in Figure 2-15.\n\nFigure 2-15. The AWS Console shows all the target groups you’ve created.\nIf you click your target group and find the Targets tab in the bottom half of\nthe screen, you can see your Instances registering with the target group and' metadata={'original_pages_range': '128-140', 'source': '045_Deploying_a_Load_Balancer', 'file_path': '/mnt/c/Users/Carlos/Documents/Github/jupiter-iaa-azure/Scripts/RAG/../../data/optimized_chunks/Libro-TF/045_Deploying_a_Load_Balancer.pdf', 'num_pages': 13}", "page_content='going through health checks. Wait for the Status indicator to indicate\n“healthy” for both of them. This typically takes one to two minutes. When\nyou see it, test the alb_dns_name output you copied earlier:\n$ curl http://<alb_dns_name> \nHello, World\nSuccess! The ALB is routing traffic to your EC2 Instances. Each time you\naccess the URL, it’ll pick a different Instance to handle the request. You\nnow have a fully working cluster of web servers!\nAt this point, you can see how your cluster responds to firing up new\nInstances or shutting down old ones. For example, go to the Instances tab\nand terminate one of the Instances by selecting its checkbox, clicking the\nActions button at the top, and then setting the Instance State to Terminate.\nContinue to test the ALB URL, and you should get a 200 OK for each\nrequest, even while terminating an Instance, because the ALB will\nautomatically detect that the Instance is down and stop routing to it. Even\nmore interesting, a short time after the Instance shuts down, the ASG will\ndetect that fewer than two Instances are running and automatically launch a\nnew one to replace it (self-healing!). You can also see how the ASG resizes\nitself by adding a desired_capacity parameter to your Terraform\ncode and rerunning apply.\nCleanup\nWhen you’re done experimenting with Terraform, either at the end of this\nchapter, or at the end of future chapters, it’s a good idea to remove all of the\nresources you created so that AWS doesn’t charge you for them. Because\nTerraform keeps track of what resources you created, cleanup is simple. All\nyou need to do is run the destroy command:\n$ terraform destroy \n \n(...)\n\nTerraform will perform the following actions: \n \n  # aws_autoscaling_group.example will be destroyed \n  - resource \"aws_autoscaling_group\" \"example\" { \n      (...) \n    } \n \n  # aws_launch_configuration.example will be destroyed \n  - resource \"aws_launch_configuration\" \"example\" { \n      (...) \n    } \n \n  # aws_lb.example will be destroyed \n  - resource \"aws_lb\" \"example\" { \n      (...) \n    } \n \n  (...) \n \nPlan: 0 to add, 0 to change, 8 to destroy. \n \nDo you really want to destroy all resources? \n  Terraform will destroy all your managed infrastructure, as \nshown above. \n  There is no undo. Only 'yes' will be accepted to confirm. \n \n  Enter a value:\nIt goes without saying that you should rarely, if ever, run destroy in a\nproduction environment! There’s no “undo” for the destroy command, so\nTerraform gives you one final chance to review what you’re doing, showing\nyou the list of all the resources you’re about to delete, and prompting you to\nconfirm the deletion. If everything looks good, type yes and hit Enter;\nTerraform will build the dependency graph and delete all of the resources in\nthe correct order, using as much parallelism as possible. In a minute or two,\nyour AWS account should be clean again.\nNote that later in the book, you will continue to develop this example, so\ndon’t delete the Terraform code! However, feel free to run destroy on the\nactual deployed resources whenever you want. After all, the beauty of\ninfrastructure as code is that all of the information about those resources is\ncaptured in code, so you can re-create all of them at any time with a single' metadata={'original_pages_range': '141-142', 'source': '046_Cleanup', 'file_path': '/mnt/c/Users/Carlos/Documents/Github/jupiter-iaa-azure/Scripts/RAG/../../data/optimized_chunks/Libro-TF/046_Cleanup.pdf', 'num_pages': 2}", "page_content='command: terraform apply. In fact, you might want to commit your\nlatest changes to Git so that you can keep track of the history of your\ninfrastructure.\nConclusion\nYou now have a basic grasp of how to use Terraform. The declarative\nlanguage makes it easy to describe exactly the infrastructure you want to\ncreate. The plan command allows you to verify your changes and catch\nbugs before deploying them. Variables, references, and dependencies allow\nyou to remove duplication from your code and make it highly configurable.\nHowever, you’ve only scratched the surface. In Chapter 3, you’ll learn how\nTerraform keeps track of what infrastructure it has already created, and the\nprofound impact that has on how you should structure your Terraform code.\nIn Chapter 4, you’ll see how to create reusable infrastructure with\nTerraform modules.\n1 Source: “Global Cloud Services Spend Hits Record US$49.4 Billion in Q3 2021”, Canalys,\nOctober 28, 2021.\n2 If you find the AWS terminology confusing, be sure to check out Amazon Web Services in\nPlain English.\n3 Check out the AWS Free Tier documentation for details.\n4 For more details on AWS user management best practices, see the documentation.\n5 You can learn more about IAM Policies on the AWS website.\n6 I’m assuming that you’re running the examples in this book in an AWS account dedicated\nsolely to learning and testing so that the broad permissions of the AdministratorAccess\nManaged Policy are not a big risk. If you are running these examples in a more sensitive\nenvironment—which, for the record, I don’t recommend!—and you’re comfortable with\ncreating custom IAM Policies, you can find a more pared-down set of permissions in this\nbook’s code examples repo.\n7 You can also write Terraform code in pure JSON in files with the extension .tf.json. You can\nlearn more about Terraform’s HCL and JSON syntax in the Terraform documentation.\n8 You can learn more about AWS regions and Availability Zones on the AWS website.\n9 Finding AMI IDs is surprisingly complicated, as documented in this Gruntwork blog post.\n\n10 You can find a handy list of HTTP server one-liners on GitHub.\n11 To learn more about how CIDR works, see its Wikipedia page. For a handy calculator that\nconverts between IP address ranges and CIDR notation, use either https://cidr.xyz/ in your\nbrowser or install the ipcalc command in your terminal.\n12 Note that while the graph command can be useful for visualizing the relationships between\na small number of resources, with dozens or hundreds of resources, the graphs tend to become\ntoo large and messy to be useful.\n13 From The Pragmatic Programmer by Andy Hunt and Dave Thomas (Addison-Wesley\nProfessional).\n14 For a deeper look at how to build highly available and scalable systems on AWS, see “A\nComprehensive Guide to Building a Scalable Web App on Amazon Web Services - Part 1” by\nJosh Padnick.\n15 These days, you should actually be using a launch template (and the\naws_launch_template resource) with ASGs rather than a launch configuration. However,\nI’ve stuck with the launch configuration in the examples in this book as it is convenient for\nteaching some of the concepts in the zero-downtime deployment section of Chapter 5.\n16 To keep these examples simple, the EC2 Instances and ALB are running in the same subnets.\nIn production usage, you’d most likely run them in different subnets, with the EC2 Instances in\nprivate subnets (so they aren’t directly accessible from the public internet) and the ALBs in\npublic subnets (so users can access them directly).' metadata={'original_pages_range': '143-144', 'source': '047_Conclusion', 'file_path': '/mnt/c/Users/Carlos/Documents/Github/jupiter-iaa-azure/Scripts/RAG/../../data/optimized_chunks/Libro-TF/047_Conclusion.pdf', 'num_pages': 2}", "page_content='Chapter 3. How to Manage\nTerraform State\nIn Chapter 2, as you were using Terraform to create and update resources,\nyou might have noticed that every time you ran terraform plan or\nterraform apply, Terraform was able to find the resources it created\npreviously and update them accordingly. But how did Terraform know\nwhich resources it was supposed to manage? You could have all sorts of\ninfrastructure in your AWS account, deployed through a variety of\nmechanisms (some manually, some via Terraform, some via the CLI), so\nhow does Terraform know which infrastructure it’s responsible for?\nIn this chapter, you’re going to see how Terraform tracks the state of your\ninfrastructure and the impact that has on file layout, isolation, and locking\nin a Terraform project. Here are the key topics I’ll go over:\nWhat is Terraform state?\nShared storage for state files\nLimitations with Terraform’s backends\nState file isolation\nIsolation via workspaces\nIsolation via file layout\nThe terraform_remote_state data source\nEXAMPLE CODE\nAs a reminder, you can find all of the code examples in the book on GitHub.' metadata={'original_pages_range': '145', 'source': '048_3._How_to_Manage_Terraform_State', 'file_path': '/mnt/c/Users/Carlos/Documents/Github/jupiter-iaa-azure/Scripts/RAG/../../data/optimized_chunks/Libro-TF/048_3._How_to_Manage_Terraform_State.pdf', 'num_pages': 1}", "page_content='What Is Terraform State?\nEvery time you run Terraform, it records information about what\ninfrastructure it created in a Terraform state file. By default, when you run\nTerraform in the folder /foo/bar, Terraform creates the file\n/foo/bar/terraform.tfstate. This file contains a custom JSON format that\nrecords a mapping from the Terraform resources in your configuration files\nto the representation of those resources in the real world. For example, let’s\nsay your Terraform configuration contained the following:\nresource \"aws_instance\" \"example\" {\n  ami           = \"ami-0fb653ca2d3203ac1\"\n  instance_type = \"t2.micro\"\n}\nAfter running terraform apply, here is a small snippet of the contents\nof the terraform.tfstate file (truncated for readability):\n{\n  \"version\": 4,\n  \"terraform_version\": \"1.2.3\",\n  \"serial\": 1,\n  \"lineage\": \"86545604-7463-4aa5-e9e8-a2a221de98d2\",\n  \"outputs\": {},\n  \"resources\": [\n    {\n      \"mode\": \"managed\",\n      \"type\": \"aws_instance\",\n      \"name\": \"example\",\n      \"provider\": \n\"provider[\\\"registry.terraform.io/hashicorp/aws\\\"]\",\n      \"instances\": [\n        {\n          \"schema_version\": 1,\n          \"attributes\": {\n            \"ami\": \"ami-0fb653ca2d3203ac1\",\n            \"availability_zone\": \"us-east-2b\",\n            \"id\": \"i-0bc4bbe5b84387543\",\n            \"instance_state\": \"running\",\n            \"instance_type\": \"t2.micro\",\n            \"(...)\": \"(truncated)\"\n          }\n\n}\n      ]\n    }\n  ]\n}\nUsing this JSON format, Terraform knows that a resource with type\naws_instance and name example corresponds to an EC2 Instance in\nyour AWS account with ID i-0bc4bbe5b84387543. Every time you\nrun Terraform, it can fetch the latest status of this EC2 Instance from AWS\nand compare that to what’s in your Terraform configurations to determine\nwhat changes need to be applied. In other words, the output of the plan\ncommand is a diff between the code on your computer and the\ninfrastructure deployed in the real world, as discovered via IDs in the state\nfile.\nTHE STATE FILE IS A PRIVATE API\nThe state file format is a private API that is meant only for internal use within\nTerraform. You should never edit the Terraform state files by hand or write code that\nreads them directly.\nIf for some reason you need to manipulate the state file—which should be a relatively\nrare occurrence—use the terraform import or terraform state commands\n(you’ll see examples of both in Chapter 5).\nIf you’re using Terraform for a personal project, storing state in a single\nterraform.tfstate file that lives locally on your computer works just fine. But\nif you want to use Terraform as a team on a real product, you run into\nseveral problems:\nShared storage for state files\nTo be able to use Terraform to update your infrastructure, each of your\nteam members needs access to the same Terraform state files. That\nmeans you need to store those files in a shared location.\nLocking state files' metadata={'original_pages_range': '146-147', 'source': '049_What_Is_Terraform_State', 'file_path': '/mnt/c/Users/Carlos/Documents/Github/jupiter-iaa-azure/Scripts/RAG/../../data/optimized_chunks/Libro-TF/049_What_Is_Terraform_State.pdf', 'num_pages': 2}", "page_content='As soon as data is shared, you run into a new problem: locking. Without\nlocking, if two team members are running Terraform at the same time,\nyou can run into race conditions as multiple Terraform processes make\nconcurrent updates to the state files, leading to conflicts, data loss, and\nstate file corruption.\nIsolating state files\nWhen making changes to your infrastructure, it’s a best practice to\nisolate different environments. For example, when making a change in a\ntesting or staging environment, you want to be sure that there is no way\nyou can accidentally break production. But how can you isolate your\nchanges if all of your infrastructure is defined in the same Terraform\nstate file?\nIn the following sections, I’ll dive into each of these problems and show\nyou how to solve them.\nShared Storage for State Files\nThe most common technique for allowing multiple team members to access\na common set of files is to put them in version control (e.g., Git). Although\nyou should definitely store your Terraform code in version control, storing\nTerraform state in version control is a bad idea for the following reasons:\nManual error\nIt’s too easy to forget to pull down the latest changes from version\ncontrol before running Terraform or to push your latest changes to\nversion control after running Terraform. It’s just a matter of time before\nsomeone on your team runs Terraform with out-of-date state files and,\nas a result, accidentally rolls back or duplicates previous deployments.\nLocking\n\nMost version control systems do not provide any form of locking that\nwould prevent two team members from running terraform apply\non the same state file at the same time.\nSecrets\nAll data in Terraform state files is stored in plain text. This is a problem\nbecause certain Terraform resources need to store sensitive data. For\nexample, if you use the aws_db_instance resource to create a\ndatabase, Terraform will store the username and password for the\ndatabase in a state file in plain text, and you shouldn’t store plain text\nsecrets in version control.\nInstead of using version control, the best way to manage shared storage for\nstate files is to use Terraform’s built-in support for remote backends. A\nTerraform backend determines how Terraform loads and stores state. The\ndefault backend, which you’ve been using this entire time, is the local\nbackend, which stores the state file on your local disk. Remote backends\nallow you to store the state file in a remote, shared store. A number of\nremote backends are supported, including Amazon S3, Azure Storage,\nGoogle Cloud Storage, and HashiCorp’s Terraform Cloud and Terraform\nEnterprise.\nRemote backends solve the three issues just listed:\nManual error\nAfter you configure a remote backend, Terraform will automatically\nload the state file from that backend every time you run plan or\napply, and it’ll automatically store the state file in that backend after\neach apply, so there’s no chance of manual error.\nLocking\nMost of the remote backends natively support locking. To run\nterraform apply, Terraform will automatically acquire a lock; if\nsomeone else is already running apply, they will already have the\n\nlock, and you will have to wait. You can run apply with the -lock-\ntimeout=<TIME> parameter to instruct Terraform to wait up to\nTIME for a lock to be released (e.g., -lock-timeout=10m will wait\nfor 10 minutes).\nSecrets\nMost of the remote backends natively support encryption in transit and\nencryption at rest of the state file. Moreover, those backends usually\nexpose ways to configure access permissions (e.g., using IAM policies\nwith an Amazon S3 bucket), so you can control who has access to your\nstate files and the secrets they might contain. It would be better still if\nTerraform natively supported encrypting secrets within the state file, but\nthese remote backends reduce most of the security concerns, given that\nat least the state file isn’t stored in plain text on disk anywhere.\nIf you’re using Terraform with AWS, Amazon S3 (Simple Storage Service),\nwhich is Amazon’s managed file store, is typically your best bet as a remote\nbackend for the following reasons:\nIt’s a managed service, so you don’t need to deploy and manage extra\ninfrastructure to use it.\nIt’s designed for 99.999999999% durability and 99.99% availability,\nwhich means you don’t need to worry too much about data loss or\noutages.\nIt supports encryption, which reduces worries about storing sensitive\ndata in state files. You still have to be very careful who on your team\ncan access the S3 bucket, but at least the data will be encrypted at rest\n(Amazon S3 supports server-side encryption using AES-256) and in\ntransit (Terraform uses TLS when talking to Amazon S3).\nIt supports locking via DynamoDB. (More on this later.)\nIt supports versioning, so every revision of your state file is stored, and\nyou can roll back to an older version if something goes wrong.\n1\n\nIt’s inexpensive, with most Terraform usage easily fitting into the\nAWS Free Tier.\nTo enable remote state storage with Amazon S3, the first step is to create an\nS3 bucket. Create a main.tf file in a new folder (it should be a different\nfolder from where you store the configurations from Chapter 2), and at the\ntop of the file, specify AWS as the provider:\nprovider \"aws\" {\n  region = \"us-east-2\"\n}\nNext, create an S3 bucket by using the aws_s3_bucket resource:\nresource \"aws_s3_bucket\" \"terraform_state\" {\n  bucket = \"terraform-up-and-running-state\" \n \n  # Prevent accidental deletion of this S3 bucket \n  lifecycle {\n    prevent_destroy = true \n  }\n}\nThis code sets the following arguments:\nbucket\nThis is the name of the S3 bucket. Note that S3 bucket names must be\nglobally unique among all AWS customers. Therefore, you will need to\nchange the bucket parameter from \"terraform-up-and-\nrunning-state\" (which I already created) to your own name. Make\nsure to remember this name and take note of what AWS region you’re\nusing because you’ll need both pieces of information again a little later\non.\nprevent_destroy\nprevent_destroy is the second lifecycle setting you’ve seen (the\nfirst was create_before_destroy in Chapter 2). When you set\n2\n\nprevent_destroy to true on a resource, any attempt to delete that\nresource (e.g., by running terraform destroy) will cause\nTerraform to exit with an error. This is a good way to prevent accidental\ndeletion of an important resource, such as this S3 bucket, which will\nstore all of your Terraform state. Of course, if you really mean to delete\nit, you can just comment that setting out.\nLet’s now add several extra layers of protection to this S3 bucket.\nFirst, use the aws_s3_bucket_versioning resource to enable\nversioning on the S3 bucket so that every update to a file in the bucket\nactually creates a new version of that file. This allows you to see older\nversions of the file and revert to those older versions at any time, which can\nbe a useful fallback mechanism if something goes wrong:\n# Enable versioning so you can see the full revision history of \nyour\n# state files\nresource \"aws_s3_bucket_versioning\" \"enabled\" {\n  bucket = aws_s3_bucket.terraform_state.id \n  versioning_configuration {\n    status = \"Enabled\" \n  }\n}\nSecond, use the\naws_s3_bucket_server_side_encryption_configuration\nresource to turn server-side encryption on by default for all data written to\nthis S3 bucket. This ensures that your state files, and any secrets they might\ncontain, are always encrypted on disk when stored in S3:\n# Enable server-side encryption by default\nresource \"aws_s3_bucket_server_side_encryption_configuration\" \n\"default\" {\n  bucket = aws_s3_bucket.terraform_state.id \n \n  rule { \n    apply_server_side_encryption_by_default {\n      sse_algorithm = \"AES256\"\n\n} \n  }\n}\nThird, use the aws_s3_bucket_public_access_block resource to\nblock all public access to the S3 bucket. S3 buckets are private by default,\nbut as they are often used to serve static content—e.g., images, fonts, CSS,\nJS, HTML—it is possible, even easy, to make the buckets public. Since\nyour Terraform state files may contain sensitive data and secrets, it’s worth\nadding this extra layer of protection to ensure no one on your team can ever\naccidentally make this S3 bucket public:\n# Explicitly block all public access to the S3 bucket\nresource \"aws_s3_bucket_public_access_block\" \"public_access\" {\n  bucket                  = aws_s3_bucket.terraform_state.id\n  block_public_acls       = true\n  block_public_policy     = true\n  ignore_public_acls      = true\n  restrict_public_buckets = true\n}\nNext, you need to create a DynamoDB table to use for locking. DynamoDB\nis Amazon’s distributed key-value store. It supports strongly consistent\nreads and conditional writes, which are all the ingredients you need for a\ndistributed lock system. Moreover, it’s completely managed, so you don’t\nhave any infrastructure to run yourself, and it’s inexpensive, with most\nTerraform usage easily fitting into the AWS Free Tier.\nTo use DynamoDB for locking with Terraform, you must create a\nDynamoDB table that has a primary key called LockID (with this exact\nspelling and capitalization). You can create such a table using the\naws_dynamodb_table resource:\nresource \"aws_dynamodb_table\" \"terraform_locks\" {\n  name         = \"terraform-up-and-running-locks\"\n  billing_mode = \"PAY_PER_REQUEST\"\n  hash_key     = \"LockID\" \n \n  attribute {\n    name = \"LockID\"\n3\n\ntype = \"S\" \n  }\n}\nRun terraform init to download the provider code, and then run\nterraform apply to deploy. After everything is deployed, you will\nhave an S3 bucket and DynamoDB table, but your Terraform state will still\nbe stored locally. To configure Terraform to store the state in your S3\nbucket (with encryption and locking), you need to add a backend\nconfiguration to your Terraform code. This is configuration for Terraform\nitself, so it resides within a terraform block and has the following\nsyntax:\nterraform { \n  backend \"<BACKEND_NAME>\" { \n    [CONFIG...] \n  }\n}\nwhere BACKEND_NAME is the name of the backend you want to use (e.g.,\n\"s3\") and CONFIG consists of one or more arguments that are specific to\nthat backend (e.g., the name of the S3 bucket to use). Here’s what the\nbackend configuration looks like for an S3 bucket:\nterraform { \n  backend \"s3\" {\n    # Replace this with your bucket name!\n    bucket         = \"terraform-up-and-running-state\"\n    key            = \"global/s3/terraform.tfstate\"\n    region         = \"us-east-2\" \n \n    # Replace this with your DynamoDB table name!\n    dynamodb_table = \"terraform-up-and-running-locks\"\n    encrypt        = true \n  }\n}\nLet’s go through these settings one at a time:\nbucket\n\nThe name of the S3 bucket to use. Make sure to replace this with the\nname of the S3 bucket you created earlier.\nkey\nThe filepath within the S3 bucket where the Terraform state file should\nbe written. You’ll see a little later on why the preceding example code\nsets this to global/s3/terraform.tfstate.\nregion\nThe AWS region where the S3 bucket lives. Make sure to replace this\nwith the region of the S3 bucket you created earlier.\ndynamodb_table\nThe DynamoDB table to use for locking. Make sure to replace this with\nthe name of the DynamoDB table you created earlier.\nencrypt\nSetting this to true ensures that your Terraform state will be encrypted\non disk when stored in S3. We already enabled default encryption in the\nS3 bucket itself, so this is here as a second layer to ensure that the data\nis always encrypted.\nTo instruct Terraform to store your state file in this S3 bucket, you’re going\nto use the terraform init command again. This command not only\ncan download provider code, but also configure your Terraform backend\n(and you’ll see yet another use later on, too). Moreover, the init\ncommand is idempotent, so it’s safe to run it multiple times:\n$ terraform init \n \nInitializing the backend... \nAcquiring state lock. This may take a few moments... \nDo you want to copy existing state to the new backend? \n  Pre-existing state was found while migrating the previous \n\"local\" backend\n\nto the newly configured \"s3\" backend. No existing state was \nfound in the \n  newly configured \"s3\" backend. Do you want to copy this state \nto the new \n  \"s3\" backend? Enter \"yes\" to copy and \"no\" to start with an \nempty state. \n \n  Enter a value:\nTerraform will automatically detect that you already have a state file locally\nand prompt you to copy it to the new S3 backend. If you type yes, you\nshould see the following:\nSuccessfully configured the backend \"s3\"! Terraform will \nautomatically \nuse this backend unless the backend configuration changes.\nAfter running this command, your Terraform state will be stored in the S3\nbucket. You can check this by heading over to the S3 Management Console\nin your browser and clicking your bucket. You should see something similar\nto Figure 3-1.\nFigure 3-1. You can use the AWS Console to see how your state file is stored in an S3 bucket.\n\nWith this backend enabled, Terraform will automatically pull the latest state\nfrom this S3 bucket before running a command and automatically push the\nlatest state to the S3 bucket after running a command. To see this in action,\nadd the following output variables:\noutput \"s3_bucket_arn\" {\n  value       = aws_s3_bucket.terraform_state.arn\n  description = \"The ARN of the S3 bucket\"\n} \n \noutput \"dynamodb_table_name\" {\n  value       = aws_dynamodb_table.terraform_locks.name\n  description = \"The name of the DynamoDB table\"\n}\nThese variables will print out the Amazon Resource Name (ARN) of your\nS3 bucket and the name of your DynamoDB table. Run terraform\napply to see it:\n$ terraform apply \n \n(...) \n \nAcquiring state lock. This may take a few moments... \n \naws_dynamodb_table.terraform_locks: Refreshing state... \naws_s3_bucket.terraform_state: Refreshing state... \n \nApply complete! Resources: 0 added, 0 changed, 0 destroyed. \n \nReleasing state lock. This may take a few moments... \n \nOutputs: \n \ndynamodb_table_name = \"terraform-up-and-running-locks\" \ns3_bucket_arn = \"arn:aws:s3:::terraform-up-and-running-state\"\nNote how Terraform is now acquiring a lock before running apply and\nreleasing the lock after!\nNow, head over to the S3 console again, refresh the page, and click the gray\nShow button next to Versions. You should now see several versions of your' metadata={'original_pages_range': '148-157', 'source': '050_Shared_Storage_for_State_Files', 'file_path': '/mnt/c/Users/Carlos/Documents/Github/jupiter-iaa-azure/Scripts/RAG/../../data/optimized_chunks/Libro-TF/050_Shared_Storage_for_State_Files.pdf', 'num_pages': 10}", "page_content='terraform.tfstate file in the S3 bucket, as shown in Figure 3-2.\nFigure 3-2. If you enable versioning for your S3 bucket, every change to the state file will be stored\nas a separate version.\nThis means that Terraform is automatically pushing and pulling state data to\nand from S3, and S3 is storing every revision of the state file, which can be\nuseful for debugging and rolling back to older versions if something goes\nwrong.\nLimitations with Terraform’s Backends\nTerraform’s backends have a few limitations and gotchas that you need to\nbe aware of. The first limitation is the chicken-and-egg situation of using\nTerraform to create the S3 bucket where you want to store your Terraform\nstate. To make this work, you had to use a two-step process:\n1. Write Terraform code to create the S3 bucket and DynamoDB table,\nand deploy that code with a local backend.\n\n2. Go back to the Terraform code, add a remote backend configuration\nto it to use the newly created S3 bucket and DynamoDB table, and run\nterraform init to copy your local state to S3.\nIf you ever wanted to delete the S3 bucket and DynamoDB table, you’d\nhave to do this two-step process in reverse:\n1. Go to the Terraform code, remove the backend configuration, and\nrerun terraform init to copy the Terraform state back to your\nlocal disk.\n2. Run terraform destroy to delete the S3 bucket and DynamoDB\ntable.\nThis two-step process is a bit awkward, but the good news is that you can\nshare a single S3 bucket and DynamoDB table across all of your Terraform\ncode, so you’ll probably only need to do it once (or once per AWS account\nif you have multiple accounts). After the S3 bucket exists, in the rest of\nyour Terraform code, you can specify the backend configuration right\nfrom the start without any extra steps.\nThe second limitation is more painful: the backend block in Terraform\ndoes not allow you to use any variables or references. The following code\nwill not work:\n# This will NOT work. Variables aren't allowed in a backend \nconfiguration.\nterraform { \n  backend \"s3\" {\n    bucket         = var.bucket\n    region         = var.region\n    dynamodb_table = var.dynamodb_table\n    key            = \"example/terraform.tfstate\"\n    encrypt        = true \n  }\n}\nThis means that you need to manually copy and paste the S3 bucket name,\nregion, DynamoDB table name, etc., into every one of your Terraform\n\nmodules (you’ll learn all about Terraform modules in Chapters 4 and 8; for\nnow, it’s enough to understand that modules are a way to organize and reuse\nTerraform code and that real-world Terraform code typically consists of\nmany small modules). Even worse, you must very carefully not copy and\npaste the key value but ensure a unique key for every Terraform module\nyou deploy so that you don’t accidentally overwrite the state of some other\nmodule! Having to do lots of copy-and-pastes and lots of manual changes is\nerror prone, especially if you need to deploy and manage many Terraform\nmodules across many environments.\nOne option for reducing copy-and-paste is to use partial configurations,\nwhere you omit certain parameters from the backend configuration in\nyour Terraform code and instead pass those in via -backend-config\ncommand-line arguments when calling terraform init. For example,\nyou could extract the repeated backend arguments, such as bucket and\nregion, into a separate file called backend.hcl:\n# backend.hcl\nbucket         = \"terraform-up-and-running-state\"\nregion         = \"us-east-2\"\ndynamodb_table = \"terraform-up-and-running-locks\"\nencrypt        = true\nOnly the key parameter remains in the Terraform code, since you still need\nto set a different key value for each module:\n# Partial configuration. The other settings (e.g., bucket, \nregion) will be\n# passed in from a file via -backend-config arguments to \n'terraform init'\nterraform { \n  backend \"s3\" {\n    key = \"example/terraform.tfstate\" \n  }\n}\nTo put all your partial configurations together, run terraform init\nwith the -backend-config argument:' metadata={'original_pages_range': '158-160', 'source': '051_Limitations_with_Terraform’s_Backends', 'file_path': '/mnt/c/Users/Carlos/Documents/Github/jupiter-iaa-azure/Scripts/RAG/../../data/optimized_chunks/Libro-TF/051_Limitations_with_Terraform’s_Backends.pdf', 'num_pages': 3}", "page_content='$ terraform init -backend-config=backend.hcl\nTerraform merges the partial configuration in backend.hcl with the partial\nconfiguration in your Terraform code to produce the full configuration used\nby your module. You can use the same backend.hcl file with all of your\nmodules, which reduces duplication considerably; however, you’ll still need\nto manually set a unique key value in every module.\nAnother option for reducing copy-and-paste is to use Terragrunt, an open\nsource tool that tries to fill in a few gaps in Terraform. Terragrunt can help\nyou keep your entire backend configuration DRY (Don’t Repeat\nYourself) by defining all the basic backend settings (bucket name, region,\nDynamoDB table name) in one file and automatically setting the key\nargument to the relative folder path of the module.\nYou’ll see an example of how to use Terragrunt in Chapter 10.\nState File Isolation\nWith a remote backend and locking, collaboration is no longer a problem.\nHowever, there is still one more problem remaining: isolation. When you\nfirst start using Terraform, you might be tempted to define all of your\ninfrastructure in a single Terraform file or a single set of Terraform files in\none folder. The problem with this approach is that all of your Terraform\nstate is now stored in a single file, too, and a mistake anywhere could break\neverything.\nFor example, while trying to deploy a new version of your app in staging,\nyou might break the app in production. Or, worse yet, you might corrupt\nyour entire state file, either because you didn’t use locking or due to a rare\nTerraform bug, and now all of your infrastructure in all environments is\nbroken.\nThe whole point of having separate environments is that they are isolated\nfrom one another, so if you are managing all the environments from a single\nset of Terraform configurations, you are breaking that isolation. Just as a\n4\n\nship has bulkheads that act as barriers to prevent a leak in one part of the\nship from immediately flooding all the others, you should have “bulkheads”\nbuilt into your Terraform design, as shown in Figure 3-3.\nFigure 3-3. Create isolation (“bulkheads”) between your environments by defining each environment\nin a separate Terraform configuration.\nAs Figure 3-3 illustrates, instead of defining all your environments in a\nsingle set of Terraform configurations (top), you want to define each' metadata={'original_pages_range': '161-162', 'source': '052_State_File_Isolation', 'file_path': '/mnt/c/Users/Carlos/Documents/Github/jupiter-iaa-azure/Scripts/RAG/../../data/optimized_chunks/Libro-TF/052_State_File_Isolation.pdf', 'num_pages': 2}", "page_content='environment in a separate set of configurations (bottom), so a problem in\none environment is completely isolated from the others. There are two ways\nyou could isolate state files:\nIsolation via workspaces\nUseful for quick, isolated tests on the same configuration\nIsolation via file layout\nUseful for production use cases for which you need strong separation\nbetween environments\nLet’s dive into each of these in the next two sections.\nIsolation via Workspaces\nTerraform workspaces allow you to store your Terraform state in multiple,\nseparate, named workspaces. Terraform starts with a single workspace\ncalled “default,” and if you never explicitly specify a workspace, the default\nworkspace is the one you’ll use the entire time. To create a new workspace\nor switch between workspaces, you use the terraform workspace\ncommands. Let’s experiment with workspaces on some Terraform code that\ndeploys a single EC2 Instance:\nresource \"aws_instance\" \"example\" {\n  ami           = \"ami-0fb653ca2d3203ac1\"\n  instance_type = \"t2.micro\"\n}\nConfigure a backend for this Instance using the S3 bucket and DynamoDB\ntable you created earlier in the chapter but with the key set to\nworkspaces-example/ terraform.tfstate:\nterraform { \n  backend \"s3\" {\n    # Replace this with your bucket name!\n    bucket         = \"terraform-up-and-running-state\"\n    key            = \"workspaces-example/terraform.tfstate\"\n\nregion         = \"us-east-2\" \n \n    # Replace this with your DynamoDB table name!\n    dynamodb_table = \"terraform-up-and-running-locks\"\n    encrypt        = true \n  }\n}\nRun terraform init and terraform apply to deploy this code:\n$ terraform init \n \n \nInitializing the backend... \n \nSuccessfully configured the backend \"s3\"! Terraform will \nautomatically \nuse this backend unless the backend configuration changes. \n \nInitializing provider plugins... \n \n(...) \n \nTerraform has been successfully initialized! \n \n \n \n$ terraform apply \n \n(...) \n \nApply complete! Resources: 1 added, 0 changed, 0 destroyed.\nThe state for this deployment is stored in the default workspace. You can\nconfirm this by running the terraform workspace show command,\nwhich will identify which workspace you’re currently in:\n$ terraform workspace show \ndefault\nThe default workspace stores your state in exactly the location you specify\nvia the key configuration. As shown in Figure 3-4, if you take a look in\n\nyour S3 bucket, you’ll find a terraform.tfstate file in the workspaces-\nexample folder.\nFigure 3-4. When using the default workspace, the S3 bucket will have just a single folder and state\nfile in it.\nLet’s create a new workspace called “example1” using the terraform\nworkspace new command:\n$ terraform workspace new example1 \nCreated and switched to workspace \"example1\"!\n\nYou're now on a new, empty workspace. Workspaces isolate their \nstate, \nso if you run \"terraform plan\" Terraform will not see any \nexisting state \nfor this configuration.\nNow, note what happens if you try to run terraform plan:\n$ terraform plan \n \nTerraform will perform the following actions: \n \n  # aws_instance.example will be created \n  + resource \"aws_instance\" \"example\" { \n      + ami                          = \"ami-0fb653ca2d3203ac1\" \n      + instance_type                = \"t2.micro\" \n      (...) \n    } \n \nPlan: 1 to add, 0 to change, 0 to destroy.\nTerraform wants to create a totally new EC2 Instance from scratch! That’s\nbecause the state files in each workspace are isolated from one another, and\nbecause you’re now in the example1 workspace, Terraform isn’t using the\nstate file from the default workspace and therefore doesn’t see the EC2\nInstance was already created there.\nTry running terraform apply to deploy this second EC2 Instance in\nthe new workspace:\n$ terraform apply \n \n(...) \n \nApply complete! Resources: 1 added, 0 changed, 0 destroyed.\nRepeat the exercise one more time and create another workspace called\n“example2”:\n$ terraform workspace new example2 \nCreated and switched to workspace \"example2\"!\n\nYou're now on a new, empty workspace. Workspaces isolate their \nstate, \nso if you run \"terraform plan\" Terraform will not see any \nexisting state \nfor this configuration.\nRun terraform apply again to deploy a third EC2 Instance:\n$ terraform apply \n \n(...) \n \nApply complete! Resources: 1 added, 0 changed, 0 destroyed.\nYou now have three workspaces available, which you can see by using the\nterraform workspace list command:\n$ terraform workspace list \n  default \n  example1 \n* example2\nAnd you can switch between them at any time using the terraform\nworkspace select command:\n$ terraform workspace select example1 \nSwitched to workspace \"example1\".\nTo understand how this works under the hood, take a look again in your S3\nbucket; you should now see a new folder called env:, as shown in Figure 3-\n5.\n\nFigure 3-5. When using custom workspaces, the S3 bucket will have multiple folders and state files in\nit.\nInside the env: folder, you’ll find one folder for each of your workspaces, as\nshown in Figure 3-6.\n\nFigure 3-6. Terraform creates one folder per workspace.\nInside each of those workspaces, Terraform uses the key you specified in\nyour backend configuration, so you should find an example1/workspaces-\nexample/terraform.tfstate and an example2/workspaces-\nexample/terraform.tfstate. In other words, switching to a different\nworkspace is equivalent to changing the path where your state file is stored.\n\nThis is handy when you already have a Terraform module deployed and you\nwant to do some experiments with it (e.g., try to refactor the code) but you\ndon’t want your experiments to affect the state of the already-deployed\ninfrastructure. Terraform workspaces allow you to run terraform\nworkspace new and deploy a new copy of the exact same infrastructure,\nbut storing the state in a separate file.\nIn fact, you can even change how that module behaves based on the\nworkspace you’re in by reading the workspace name using the expression\nterraform.workspace. For example, here’s how to set the Instance\ntype to t2.medium in the default workspace and t2.micro in all other\nworkspaces (e.g., to save money when experimenting):\nresource \"aws_instance\" \"example\" {\n  ami           = \"ami-0fb653ca2d3203ac1\"\n  instance_type = terraform.workspace == \"default\" ? \"t2.medium\" \n: \"t2.micro\"\n}\nThe preceding code uses ternary syntax to conditionally set\ninstance_type to either t2.medium or t2.micro, depending on the\nvalue of terraform.workspace. You’ll see the full details of ternary\nsyntax and conditional logic in Terraform in Chapter 5.\nTerraform workspaces can be a great way to quickly spin up and tear down\ndifferent versions of your code, but they have a few drawbacks:\nThe state files for all of your workspaces are stored in the same\nbackend (e.g., the same S3 bucket). That means you use the same\nauthentication and access controls for all the workspaces, which is one\nmajor reason workspaces are an unsuitable mechanism for isolating\nenvironments (e.g., isolating staging from production).\nWorkspaces are not visible in the code or on the terminal unless you\nrun terraform workspace commands. When browsing the code,\na module that has been deployed in one workspace looks exactly the\nsame as a module deployed in 10 workspaces. This makes' metadata={'original_pages_range': '163-170', 'source': '053_Isolation_via_Workspaces', 'file_path': '/mnt/c/Users/Carlos/Documents/Github/jupiter-iaa-azure/Scripts/RAG/../../data/optimized_chunks/Libro-TF/053_Isolation_via_Workspaces.pdf', 'num_pages': 8}", "page_content='maintenance more difficult, because you don’t have a good picture of\nyour infrastructure.\nPutting the two previous items together, the result is that workspaces\ncan be fairly error prone. The lack of visibility makes it easy to forget\nwhat workspace you’re in and accidentally deploy changes in the\nwrong one (e.g., accidentally running terraform destroy in a\n“production” workspace rather than a “staging” workspace), and\nbecause you must use the same authentication mechanism for all\nworkspaces, you have no other layers of defense to protect against\nsuch errors.\nDue to these drawbacks, workspaces are not a suitable mechanism for\nisolating one environment from another: e.g., isolating staging from\nproduction. To get proper isolation between environments, instead of\nworkspaces, you’ll most likely want to use file layout, which is the topic of\nthe next section.\nBefore moving on, make sure to clean up the three EC2 Instances you just\ndeployed by running terraform workspace select <name> and\nterraform destroy in each of the three workspaces.\nIsolation via File Layout\nTo achieve full isolation between environments, you need to do the\nfollowing:\nPut the Terraform configuration files for each environment into a\nseparate folder. For example, all of the configurations for the staging\nenvironment can be in a folder called stage and all the configurations\nfor the production environment can be in a folder called prod.\nConfigure a different backend for each environment, using different\nauthentication mechanisms and access controls: e.g., each environment\ncould live in a separate AWS account with a separate S3 bucket as a\nbackend.\n5\n\nWith this approach, the use of separate folders makes it much clearer which\nenvironments you’re deploying to, and the use of separate state files, with\nseparate authentication mechanisms, makes it significantly less likely that a\nscrew-up in one environment can have any impact on another.\nIn fact, you might want to take the isolation concept beyond environments\nand down to the “component” level, where a component is a coherent set of\nresources that you typically deploy together. For example, after you’ve set\nup the basic network topology for your infrastructure—in AWS lingo, your\nVirtual Private Cloud (VPC) and all the associated subnets, routing rules,\nVPNs, and network ACLs—you will probably change it only once every\nfew months, at most. On the other hand, you might deploy a new version of\na web server multiple times per day. If you manage the infrastructure for\nboth the VPC component and the web server component in the same set of\nTerraform configurations, you are unnecessarily putting your entire network\ntopology at risk of breakage (e.g., from a simple typo in the code or\nsomeone accidentally running the wrong command) multiple times per day.\nTherefore, I recommend using separate Terraform folders (and therefore\nseparate state files) for each environment (staging, production, etc.) and for\neach component (VPC, services, databases) within that environment. To see\nwhat this looks like in practice, let’s go through the recommended file\nlayout for Terraform projects.\nFigure 3-7 shows the file layout for my typical Terraform project.\n\n\n\nFigure 3-7. The typical file layout for a Terraform project uses separate folders for each environment\nand for each component within that environment.\nAt the top level, there are separate folders for each “environment.” The\nexact environments differ for every project, but the typical ones are as\nfollows:\nstage\nAn environment for pre-production workloads (i.e., testing)\nprod\nAn environment for production workloads (i.e., user-facing apps)\nmgmt\nAn environment for DevOps tooling (e.g., bastion host, CI server)\nglobal\nA place to put resources that are used across all environments (e.g., S3,\nIAM)\nWithin each environment, there are separate folders for each “component.”\nThe components differ for every project, but here are the typical ones:\nvpc\nThe network topology for this environment.\nservices\nThe apps or microservices to run in this environment, such as a Ruby on\nRails frontend or a Scala backend. Each app could even live in its own\nfolder to isolate it from all the other apps.\ndata-storage\nThe data stores to run in this environment, such as MySQL or Redis.\nEach data store could even reside in its own folder to isolate it from all\n\nother data stores.\nWithin each component, there are the actual Terraform configuration files,\nwhich are organized according to the following naming conventions:\nvariables.tf\nInput variables\noutputs.tf\nOutput variables\nmain.tf\nResources and data sources\nWhen you run Terraform, it simply looks for files in the current directory\nwith the .tf extension, so you can use whatever filenames you want.\nHowever, although Terraform may not care about filenames, your\nteammates probably do. Using a consistent, predictable naming convention\nmakes your code easier to browse: e.g., you’ll always know where to look\nto find a variable, output, or resource.\nNote that the preceding convention is the minimum convention you should\nfollow, because in virtually all uses of Terraform, it’s useful to be able to\njump to the input variables, output variables, and resources very quickly,\nbut you may want to go beyond this convention. Here are just a few\nexamples:\ndependencies.tf\nIt’s common to put all your data sources in a dependencies.tf file to\nmake it easier to see what external things the code depends on.\nproviders.tf\nYou may want to put your provider blocks into a providers.tf file so\nyou can see, at a glance, what providers the code talks to and what\n\nauthentication you’ll have to provide.\nmain-xxx.tf\nIf the main.tf file is getting really long because it contains a large\nnumber of resources, you could break it down into smaller files that\ngroup the resources in some logical way: e.g., main-iam.tf could contain\nall the IAM resources, main-s3.tf could contain all the S3 resources, and\nso on. Using the main- prefix makes it easier to scan the list of files in a\nfolder when they are organized alphabetically, as all the resources will\nbe grouped together. It’s also worth noting that if you find yourself\nmanaging a very large number of resources and struggling to break\nthem down across many files, that might be a sign that you should break\nyour code into smaller modules instead, which is a topic I’ll dive into in\nChapter 4.\nLet’s take the web server cluster code you wrote in Chapter 2, plus the\nAmazon S3 and DynamoDB code you wrote in this chapter, and rearrange it\nusing the folder structure in Figure 3-8.\n\nFigure 3-8. Move the web server cluster code into a stage/services/webserver-cluster folder to\nindicate that this is a testing or staging version of the web server.\nThe S3 bucket you created in this chapter should be moved into the\nglobal/s3 folder. Move the output variables (s3_bucket_arn and\ndynamodb_table_name) into outputs.tf. When moving the folder, make\nsure that you don’t miss the (hidden) .terraform folder when copying files\nto the new location so you don’t need to reinitialize everything.\nThe web server cluster you created in Chapter 2 should be moved into\nstage/services/webserver-cluster (think of this as the “testing” or “staging”\nversion of that web server cluster; you’ll add a “production” version in the\n\nnext chapter). Again, make sure to copy over the .terraform folder, move\ninput variables into variables.tf, and move output variables into outputs.tf.\nYou should also update the web server cluster to use S3 as a backend.\nYou can copy and paste the backend config from global/s3/main.tf more\nor less verbatim, but make sure to change the key to the same folder path\nas the web server Terraform code: stage/services/webserver-\ncluster/terraform.tfstate. This gives you a 1:1 mapping between the layout\nof your Terraform code in version control and your Terraform state files in\nS3, so it’s obvious how the two are connected. The s3 module already sets\nthe key using this convention.\nThis file layout has a number of advantages:\nClear code / environment layout\nIt’s easy to browse the code and understand exactly what components\nare deployed in each environment.\nIsolation\nThis layout provides a good amount of isolation between environments\nand between components within an environment, ensuring that if\nsomething goes wrong, the damage is contained as much as possible to\njust one small part of your entire infrastructure.\nIn some ways, these advantages are drawbacks, too:\nWorking with multiple folders\nSplitting components into separate folders prevents you from\naccidentally blowing up your entire infrastructure in one command, but\nit also prevents you from creating your entire infrastructure in one\ncommand. If all of the components for a single environment were\ndefined in a single Terraform configuration, you could spin up an entire\nenvironment with a single call to terraform apply. But if all of the\ncomponents are in separate folders, then you need to run terraform\napply separately in each one.' metadata={'original_pages_range': '171-178', 'source': '054_Isolation_via_File_Layout', 'file_path': '/mnt/c/Users/Carlos/Documents/Github/jupiter-iaa-azure/Scripts/RAG/../../data/optimized_chunks/Libro-TF/054_Isolation_via_File_Layout.pdf', 'num_pages': 8}", "page_content='Solution: If you use Terragrunt, you can run commands across multiple\nfolders concurrently using the run-all command.\nCopy/paste\nThe file layout described in this section has a lot of duplication. For\nexample, the same frontend-app and backend-app live in both\nthe stage and prod folders.\nSolution: You won’t actually need to copy and paste all of that code! In\nChapter 4, you’ll see how to use Terraform modules to keep all of this\ncode DRY.\nResource dependencies\nBreaking the code into multiple folders makes it more difficult to use\nresource dependencies. If your app code was defined in the same\nTerraform configuration files as the database code, that app code could\ndirectly access attributes of the database using an attribute reference\n(e.g., access the database address via\naws_db_instance.foo.address). But if the app code and\ndatabase code live in different folders, as I’ve recommended, you can\nno longer do that.\nSolution: One option is to use dependency blocks in Terragrunt, as\nyou’ll see in Chapter 10. Another option is to use the\nterraform_remote_state data source, as described in the next\nsection.\nThe terraform_remote_state Data Source\nIn Chapter 2, you used data sources to fetch read-only information from\nAWS, such as the aws_subnets data source, which returns a list of\nsubnets in your VPC. There is another data source that is particularly useful\nwhen working with state: terraform_remote_state. You can use\n\nthis data source to fetch the Terraform state file stored by another set of\nTerraform configurations.\nLet’s go through an example. Imagine that your web server cluster needs to\ncommunicate with a MySQL database. Running a database that is scalable,\nsecure, durable, and highly available is a lot of work. Again, you can let\nAWS take care of it for you, this time by using Amazon’s Relational\nDatabase Service (RDS), as shown in Figure 3-9. RDS supports a variety of\ndatabases, including MySQL, PostgreSQL, SQL Server, and Oracle.\nYou might not want to define the MySQL database in the same set of\nconfiguration files as the web server cluster, because you’ll be deploying\nupdates to the web server cluster far more frequently and don’t want to risk\naccidentally breaking the database each time you do so.\n\nFigure 3-9. The web server cluster communicates with MySQL, which is deployed on top of Amazon\nRDS.\nTherefore, your first step should be to create a new folder at stage/data-\nstores/mysql and create the basic Terraform files (main.tf, variables.tf,\n\noutputs.tf) within it, as shown in Figure 3-10.\n\n\n\nFigure 3-10. Create the database code in the stage/data-stores folder.\nNext, create the database resources in stage/data-stores/mysql/main.tf:\nprovider \"aws\" {\n  region = \"us-east-2\"\n} \n \nresource \"aws_db_instance\" \"example\" {\n  identifier_prefix   = \"terraform-up-and-running\"\n  engine              = \"mysql\"\n  allocated_storage   = 10\n  instance_class      = \"db.t2.micro\"\n  skip_final_snapshot = true\n  db_name             = \"example_database\" \n \n  # How should we set the username and password?\n  username = \"???\"\n  password = \"???\"\n}\nAt the top of the file, you see the typical provider block, but just below\nthat is a new resource: aws_db_instance. This resource creates a\ndatabase in RDS with the following settings:\nMySQL as the database engine.\n10 GB of storage.\nA db.t2.micro Instance, which has one virtual CPU, 1 GB of\nmemory, and is part of the AWS Free Tier.\nThe final snapshot is disabled, as this code is just for learning and\ntesting (if you don’t disable the snapshot, or don’t provide a name for\nthe snapshot via the final_snapshot_identifier parameter,\ndestroy will fail).\nNote that two of the parameters that you must pass to the\naws_db_instance resource are the master username and master\npassword. Because these are secrets, you should not put them directly into\nyour code in plain text! In Chapter 6, I’ll discuss a variety of options for\n\nhow to securely handle secrets with Terraform. For now, let’s use an option\nthat avoids storing any secrets in plain text and is easy to use: you store\nyour secrets, such as database passwords, outside of Terraform (e.g., in a\npassword manager such as 1Password, LastPass, or macOS Keychain), and\nyou pass those secrets into Terraform via environment variables.\nTo do that, declare variables called db_username and db_password in\nstage/data-stores/mysql/variables.tf:\nvariable \"db_username\" {\n  description = \"The username for the database\"\n  type        = string\n  sensitive   = true\n} \n \nvariable \"db_password\" {\n  description = \"The password for the database\"\n  type        = string\n  sensitive   = true\n}\nFirst, note that these variables are marked with sensitive = true to\nindicate they contain secrets. This ensures Terraform won’t log the values\nwhen you run plan or apply. Second, note that these variables do not\nhave a default. This is intentional. You should not store your database\ncredentials or any sensitive information in plain text. Instead, you’ll set\nthese variables using environment variables.\nBefore doing that, let’s finish the code. First, pass the two new input\nvariables through to the aws_db_instance resource:\nresource \"aws_db_instance\" \"example\" {\n  identifier_prefix   = \"terraform-up-and-running\"\n  engine              = \"mysql\"\n  allocated_storage   = 10\n  instance_class      = \"db.t2.micro\"\n  skip_final_snapshot = true\n  db_name             = \"example_database\" \n \n  username = var.db_username\n\npassword = var.db_password\n}\nNext, configure this module to store its state in the S3 bucket you created\nearlier at the path stage/data-stores/mysql/terraform.tfstate:\nterraform { \n  backend \"s3\" {\n    # Replace this with your bucket name!\n    bucket         = \"terraform-up-and-running-state\"\n    key            = \"stage/data-stores/mysql/terraform.tfstate\"\n    region         = \"us-east-2\" \n \n    # Replace this with your DynamoDB table name!\n    dynamodb_table = \"terraform-up-and-running-locks\"\n    encrypt        = true \n  }\n}\nFinally, add two output variables in stage/data-stores/mysql/outputs.tf to\nreturn the database’s address and port:\noutput \"address\" {\n  value       = aws_db_instance.example.address\n  description = \"Connect to the database at this endpoint\"\n} \n \noutput \"port\" {\n  value       = aws_db_instance.example.port\n  description = \"The port the database is listening on\"\n}\nYou’re now ready to pass in the database username and password using\nenvironment variables. As a reminder, for each input variable foo defined\nin your Terraform configurations, you can provide Terraform the value of\nthis variable using the environment variable TF_VAR_foo. For the\ndb_username and db_password input variables, here is how you can\nset the TF_VAR_db_username and TF_VAR_db_password\nenvironment variables on Linux/Unix/macOS systems:\n\n$ export TF_VAR_db_username=\"(YOUR_DB_USERNAME)\" \n$ export TF_VAR_db_password=\"(YOUR_DB_PASSWORD)\"\nAnd here is how you do it on Windows systems:\n$ set TF_VAR_db_username=\"(YOUR_DB_USERNAME)\" \n$ set TF_VAR_db_password=\"(YOUR_DB_PASSWORD)\"\nRun terraform init and terraform apply to create the database.\nNote that Amazon RDS can take roughly 10 minutes to provision even a\nsmall database, so be patient. After apply completes, you should see the\noutputs in the terminal:\n$ terraform apply \n \n(...) \n \nApply complete! Resources: 1 added, 0 changed, 0 destroyed. \n \nOutputs: \n \naddress = \"terraform-up-and-running.cowu6mts6srx.us-east-\n2.rds.amazonaws.com\" \nport = 3306\nThese outputs are now also stored in the Terraform state for the database,\nwhich is in your S3 bucket at the path stage/data-\nstores/mysql/terraform.tfstate.\nIf you go back to your web server cluster code, you can get the web server\nto read those outputs from the database’s state file by adding the\nterraform_remote_state data source in stage/services/webserver-\ncluster/main.tf:\ndata \"terraform_remote_state\" \"db\" {\n  backend = \"s3\" \n \n  config = {\n    bucket = \"(YOUR_BUCKET_NAME)\"\n    key    = \"stage/data-stores/mysql/terraform.tfstate\"\n    region = \"us-east-2\"\n\n}\n}\nThis terraform_remote_state data source configures the web\nserver cluster code to read the state file from the same S3 bucket and folder\nwhere the database stores its state, as shown in Figure 3-11.\n\n\n\nFigure 3-11. The database writes its state to an S3 bucket (top), and the web server cluster reads that\nstate from the same bucket (bottom).\nIt’s important to understand that, like all Terraform data sources, the data\nreturned by terraform_remote_state is read-only. Nothing you do\nin your web server cluster Terraform code can modify that state, so you can\npull in the database’s state data with no risk of causing any problems in the\ndatabase itself.\nAll of the database’s output variables are stored in the state file, and you can\nread them from the terraform_remote_state data source using an\nattribute reference of the form:\ndata.terraform_remote_state.<NAME>.outputs.<ATTRIBUTE>\nFor example, here is how you can update the User Data of the web server\ncluster Instances to pull the database address and port out of the\nterraform_remote_state data source and expose that information in\nthe HTTP response:\nuser_data = <<EOF\n#!/bin/bash\necho \"Hello, World\" >> index.xhtml\necho \"${data.terraform_remote_state.db.outputs.address}\" >> \nindex.xhtml\necho \"${data.terraform_remote_state.db.outputs.port}\" >> \nindex.xhtml\nnohup busybox httpd -f -p ${var.server_port} &\nEOF\nAs the User Data script is growing longer, defining it inline is becoming\nmessier and messier. In general, embedding one programming language\n(Bash) inside another (Terraform) makes it more difficult to maintain each\none, so let’s pause here for a moment to externalize the Bash script. To do\nthat, you can use the templatefile built-in function.\nTerraform includes a number of built-in functions that you can execute\nusing an expression of the form:\n\nfunction_name(...)\nFor example, consider the format function:\nformat(<FMT>, <ARGS>, ...)\nThis function formats the arguments in ARGS according to the sprintf\nsyntax in the string FMT.  A great way to experiment with built-in functions\nis to run the terraform console command to get an interactive\nconsole where you can try out Terraform syntax, query the state of your\ninfrastructure, and see the results instantly:\n$ terraform console \n \n> format(\"%.3f\", 3.14159265359) \n3.142\nNote that the Terraform console is read-only, so you don’t need to worry\nabout accidentally changing infrastructure or state.\nThere are a number of other built-in functions that you can use to\nmanipulate strings, numbers, lists, and maps. One of them is the\ntemplatefile function:\ntemplatefile(<PATH>, <VARS>)\nThis function reads the file at PATH, renders it as a template, and returns\nthe result as a string. When I say “renders it as a template,” what I mean is\nthat the file at PATH can use the string interpolation syntax in Terraform\n(${...}), and Terraform will render the contents of that file, filling\nvariable references from VARS.\nTo see this in action, put the contents of the User Data script into the file\nstage/services/webserver-cluster/user-data.sh as follows:\n#!/bin/bash \n \ncat > index.xhtml <<EOF\n6\n7\n\n<h1>Hello, World</h1>\n<p>DB address: ${db_address}</p>\n<p>DB port: ${db_port}</p>\nEOF \n \nnohup busybox httpd -f -p ${server_port} &\nNote that this Bash script has a few changes from the original:\nIt looks up variables using Terraform’s standard interpolation syntax,\nexcept the only variables it has access to are those you pass in via the\nsecond parameter to templatefile (as you’ll see shortly), so you\ndon’t need any prefix to access them: for example, you should use\n${server_port} and not ${var.server_port}.\nThe script now includes some HTML syntax (e.g., <h1>) to make the\noutput a bit more readable in a web browser.\nThe final step is to update the user_data parameter of the aws_launch \n_configuration resource to call the templatefile function and\npass in the variables it needs as a map:\nresource \"aws_launch_configuration\" \"example\" {\n  image_id        = \"ami-0fb653ca2d3203ac1\"\n  instance_type   = \"t2.micro\"\n  security_groups = [aws_security_group.instance.id] \n \n  # Render the User Data script as a template\n  user_data = templatefile(\"user-data.sh\", {\n    server_port = var.server_port\n    db_address  = data.terraform_remote_state.db.outputs.address\n    db_port     = data.terraform_remote_state.db.outputs.port \n  }) \n \n  # Required when using a launch configuration with an auto \nscaling group. \n  lifecycle {\n    create_before_destroy = true \n  }\n}\nAh, that’s much cleaner than writing Bash scripts inline!' metadata={'original_pages_range': '179-192', 'source': '055_The_terraform_remote_state_Data_Source', 'file_path': '/mnt/c/Users/Carlos/Documents/Github/jupiter-iaa-azure/Scripts/RAG/../../data/optimized_chunks/Libro-TF/055_The_terraform_remote_state_Data_Source.pdf', 'num_pages': 14}", "page_content='If you deploy this cluster using terraform apply, wait for the\nInstances to register in the ALB, and open the ALB URL in a web browser,\nyou’ll see something similar to Figure 3-12.\nCongrats, your web server cluster can now programmatically access the\ndatabase address and port via Terraform. If you were using a real web\nframework (e.g., Ruby on Rails), you could set the address and port as\nenvironment variables or write them to a config file so that they could be\nused by your database library (e.g., ActiveRecord) to communicate with the\ndatabase.\nFigure 3-12. The web server cluster can programmatically access the database address and port.\nConclusion\nThe reason you need to put so much thought into isolation, locking, and\nstate is that infrastructure as code (IaC) has different trade-offs than normal\ncoding. When you’re writing code for a typical app, most bugs are\nrelatively minor and break only a small part of a single app. When you’re\nwriting code that controls your infrastructure, bugs tend to be more severe,\ngiven that they can break all of your apps—and all of your data stores, and\n\nyour entire network topology, and just about everything else. Therefore, I\nrecommend including more “safety mechanisms” when working on IaC\nthan with typical code.\nA common concern of using the recommended file layout is that it leads to\ncode duplication. If you want to run the web server cluster in both staging\nand production, how do you avoid having to copy and paste a lot of code\nbetween stage/services/webserver-cluster and prod/services/webserver-\ncluster? The answer is that you need to use Terraform modules, which are\nthe main topic of Chapter 4.\n1 Learn more about S3’s guarantees on the AWS website.\n2 See pricing information for S3 on the AWS website.\n3 Pricing information for DynamoDB is available on the AWS website.\n4 Here’s a colorful example of what happens when you don’t isolate Terraform state.\n5 The workspaces documentation makes this same exact point, but it’s buried among several\nparagraphs of text, and as workspaces used to be called “environments,” I find many users are\nstill confused about when and when not to use workspaces.\n6 You can find documentation for the sprintf syntax on the Go website.\n7 The full list of built-in functions is available on the Terraform website.\n8 For more information on software safety mechanisms, see Agility Requires Safety.\n8' metadata={'original_pages_range': '193-194', 'source': '056_Conclusion', 'file_path': '/mnt/c/Users/Carlos/Documents/Github/jupiter-iaa-azure/Scripts/RAG/../../data/optimized_chunks/Libro-TF/056_Conclusion.pdf', 'num_pages': 2}", "page_content='Chapter 4. How to Create\nReusable Infrastructure with\nTerraform Modules\nAt the end of Chapter 3, you deployed the architecture shown in Figure 4-1.\n\nFigure 4-1. The architecture you deployed in previous chapters included a load balancer, web server\ncluster, and database.\nThis works great as a first environment, but you typically need at least two\nenvironments: one for your team’s internal testing (“staging”) and one that\n\nreal users can access (“production”), as shown in Figure 4-2. Ideally, the\ntwo environments are nearly identical, though you might run slightly\nfewer/smaller servers in staging to save money.\n\n\n\nFigure 4-2. The architecture you’ll deploy in this chapter will have two environments, each with its\nown load balancer, web server cluster, and database.\nHow do you add this production environment without having to copy and\npaste all of the code from staging? For example, how do you avoid having\nto copy and paste all the code in stage/services/webserver-cluster into\nprod/services/webserver-cluster and all the code in stage/data-stores/mysql\ninto prod/data-stores/mysql?\nIn a general-purpose programming language such as Ruby, if you had the\nsame code copied and pasted in several places, you could put that code\ninside of a function and reuse that function everywhere:\n# Define the function in one place\ndef example_function() \n  puts \"Hello, World\"\nend \n \n# Use the function in multiple other places\nexample_function()\nWith Terraform, you can put your code inside of a Terraform module and\nreuse that module in multiple places throughout your code. Instead of\nhaving the same code copied and pasted in the staging and production\nenvironments, you’ll be able to have both environments reuse code from the\nsame module, as shown in Figure 4-3.\n\n' metadata={'original_pages_range': '195-200', 'source': '057_4._How_to_Create_Reusable_Infrastructure_with_Terraform_Modules', 'file_path': '/mnt/c/Users/Carlos/Documents/Github/jupiter-iaa-azure/Scripts/RAG/../../data/optimized_chunks/Libro-TF/057_4._How_to_Create_Reusable_Infrastructure_with_Terraform_Modules.pdf', 'num_pages': 6}", "page_content='Figure 4-3. Putting code into modules allows you to reuse that code from multiple environments.\nThis is a big deal. Modules are the key ingredient to writing reusable,\nmaintainable, and testable Terraform code. Once you start using them,\nthere’s no going back. You’ll start building everything as a module, creating\na library of modules to share within your company, using modules that you\nfind online, and thinking of your entire infrastructure as a collection of\nreusable modules.\nIn this chapter, I’ll show you how to create and use Terraform modules by\ncovering the following topics:\nModule basics\nModule inputs\nModule locals\nModule outputs\nModule gotchas\nModule versioning\nEXAMPLE CODE\nAs a reminder, you can find all of the code examples in the book on GitHub.\nModule Basics\nA Terraform module is very simple: any set of Terraform configuration files\nin a folder is a module. All of the configurations you’ve written so far have\ntechnically been modules, although not particularly interesting ones, since\nyou deployed them directly: if you run apply directly on a module, it’s\nreferred to as a root module. To see what modules are really capable of, you\nneed to create a reusable module, which is a module that is meant to be\nused within other modules.\n\nAs an example, let’s turn the code in stage/services/webserver-cluster,\nwhich includes an Auto Scaling Group (ASG), Application Load Balancer\n(ALB), security groups, and many other resources, into a reusable module.\nAs a first step, run terraform destroy in the\nstage/services/webserver-cluster to clean up any resources that you created\nearlier. Next, create a new top-level folder called modules, and move all of\nthe files from stage/services/webserver-cluster to\nmodules/services/webserver-cluster. You should end up with a folder\nstructure that looks something like Figure 4-4.\nOpen up the main.tf file in modules/services/webserver-cluster, and remove\nthe provider definition. Providers should be configured only in root\nmodules and not in reusable modules (you’ll learn a lot more about working\nwith providers in Chapter 7).\n\n\n\nFigure 4-4. Move your reusable web server cluster code into a modules/services/web server-cluster\nfolder.\nYou can now make use of this module in the staging environment. Here’s\nthe syntax for using a module:\nmodule \"<NAME>\" {\n  source = \"<SOURCE>\" \n \n  [CONFIG ...]\n}\nwhere NAME is an identifier you can use throughout the Terraform code to\nrefer to this module (e.g., webserver_cluster), SOURCE is the path\nwhere the module code can be found (e.g., modules/services/webserver-\ncluster), and CONFIG consists of arguments that are specific to that\nmodule. For example, you can create a new file in\nstage/services/webserver-cluster/main.tf and use the webserver-\ncluster module in it as follows:\nprovider \"aws\" {\n  region = \"us-east-2\"\n} \n \nmodule \"webserver_cluster\" {\n  source = \"../../../modules/services/webserver-cluster\"\n}\nYou can then reuse the exact same module in the production environment\nby creating a new prod/services/webserver-cluster/main.tf file with the\nfollowing contents:\nprovider \"aws\" {\n  region = \"us-east-2\"\n} \n \nmodule \"webserver_cluster\" {\n  source = \"../../../modules/services/webserver-cluster\"\n}' metadata={'original_pages_range': '201-204', 'source': '058_Module_Basics', 'file_path': '/mnt/c/Users/Carlos/Documents/Github/jupiter-iaa-azure/Scripts/RAG/../../data/optimized_chunks/Libro-TF/058_Module_Basics.pdf', 'num_pages': 4}", "page_content='And there you have it: code reuse in multiple environments that involves\nminimal duplication. Note that whenever you add a module to your\nTerraform configurations or modify the source parameter of a module,\nyou need to run the init command before you run plan or apply:\n$ terraform init \nInitializing modules... \n- webserver_cluster in ../../../modules/services/webserver-\ncluster \n \nInitializing the backend... \n \nInitializing provider plugins... \n \nTerraform has been successfully initialized!\nNow you’ve seen all the tricks the init command has up its sleeve: it\ninstalls providers, it configures your backends, and it downloads modules,\nall in one handy command.\nBefore you run the apply command on this code, be aware that there is a\nproblem with the webserver-cluster module: all of the names are\nhardcoded. That is, the name of the security groups, ALB, and other\nresources are all hardcoded, so if you use this module more than once in the\nsame AWS account, you’ll get name conflict errors. Even the details for\nhow to read the database’s state are hardcoded because the main.tf file you\ncopied into modules/services/webserver-cluster is using a\nterraform_remote_state data source to figure out the database\naddress and port, and that terraform_remote_state is hardcoded to\nlook at the staging environment.\nTo fix these issues, you need to add configurable inputs to the\nwebserver-cluster module so that it can behave differently in\ndifferent environments.\nModule Inputs\n\nTo make a function configurable in a general-purpose programming\nlanguage such as Ruby, you can add input parameters to that function:\n# A function with two input parameters\ndef example_function(param1, param2) \n  puts \"Hello, #{param1} #{param2}\"\nend \n \n# Pass two input parameters to the function\nexample_function(\"foo\", \"bar\")\nIn Terraform, modules can have input parameters, too. To define them, you\nuse a mechanism you’re already familiar with: input variables. Open up\nmodules/services/webserver-cluster/variables.tf and add three new input\nvariables:\nvariable \"cluster_name\" {\n  description = \"The name to use for all the cluster resources\"\n  type        = string\n} \n \nvariable \"db_remote_state_bucket\" {\n  description = \"The name of the S3 bucket for the database's \nremote state\"\n  type        = string\n} \n \nvariable \"db_remote_state_key\" {\n  description = \"The path for the database's remote state in S3\"\n  type        = string\n}\nNext, go through modules/services/webserver-cluster/main.tf, and use\nvar.cluster_name instead of the hardcoded names (e.g., instead of\n\"terraform-asg-example\"). For example, here is how you do it for\nthe ALB security group:\nresource \"aws_security_group\" \"alb\" {\n  name = \"${var.cluster_name}-alb\" \n \n  ingress {\n    from_port   = 80\n\nto_port     = 80\n    protocol    = \"tcp\"\n    cidr_blocks = [\"0.0.0.0/0\"] \n  } \n \n  egress {\n    from_port   = 0\n    to_port     = 0\n    protocol    = \"-1\"\n    cidr_blocks = [\"0.0.0.0/0\"] \n  }\n}\nNotice how the name parameter is set to \"${var.cluster_name}-\nalb\". You’ll need to make a similar change to the other\naws_security_group resource (e.g., give it the name\n\"${var.cluster_name}-instance\"), the aws_alb resource, and\nthe tag section of the aws_autoscaling_group resource.\nYou should also update the terraform_remote_state data source to\nuse the db_remote_state_bucket and db_remote_state_key\nas its bucket and key parameter, respectively, to ensure you’re reading\nthe state file from the right environment:\ndata \"terraform_remote_state\" \"db\" {\n  backend = \"s3\" \n \n  config = {\n    bucket = var.db_remote_state_bucket\n    key    = var.db_remote_state_key\n    region = \"us-east-2\" \n  }\n}\nNow, in the staging environment, in stage/services/webserver-\ncluster/main.tf, you can set these new input variables accordingly:\nmodule \"webserver_cluster\" {\n  source = \"../../../modules/services/webserver-cluster\" \n \n  cluster_name           = \"webservers-stage\"\n  db_remote_state_bucket = \"(YOUR_BUCKET_NAME)\"\n\ndb_remote_state_key    = \"stage/data-\nstores/mysql/terraform.tfstate\"\n}\nYou should also set these variables in the production environment in\nprod/services/webserver-cluster/main.tf but to different values that\ncorrespond to that environment:\nmodule \"webserver_cluster\" {\n  source = \"../../../modules/services/webserver-cluster\" \n \n  cluster_name           = \"webservers-prod\"\n  db_remote_state_bucket = \"(YOUR_BUCKET_NAME)\"\n  db_remote_state_key    = \"prod/data-\nstores/mysql/terraform.tfstate\"\n}\nNOTE\nThe production database doesn’t actually exist yet. As an exercise, I leave it up to you to\nadd a production database similar to the staging one.\nAs you can see, you set input variables for a module by using the same\nsyntax as setting arguments for a resource. The input variables are the API\nof the module, controlling how it will behave in different environments.\nSo far, you’ve added input variables for the name and database remote state,\nbut you may want to make other parameters configurable in your module,\ntoo. For example, in staging, you might want to run a small web server\ncluster to save money, but in production, you might want to run a larger\ncluster to handle lots of traffic. To do that, you can add three more input\nvariables to modules/services/webserver-cluster/variables.tf:\nvariable \"instance_type\" {\n  description = \"The type of EC2 Instances to run (e.g. \nt2.micro)\"\n  type        = string\n}\n\nvariable \"min_size\" {\n  description = \"The minimum number of EC2 Instances in the ASG\"\n  type        = number\n} \n \nvariable \"max_size\" {\n  description = \"The maximum number of EC2 Instances in the ASG\"\n  type        = number\n}\nNext, update the launch configuration in modules/services/webserver-\ncluster/main.tf to set its instance_type parameter to the new\nvar.instance_type input variable:\nresource \"aws_launch_configuration\" \"example\" {\n  image_id        = \"ami-0fb653ca2d3203ac1\"\n  instance_type   = var.instance_type\n  security_groups = [aws_security_group.instance.id] \n \n  user_data = templatefile(\"user-data.sh\", {\n    server_port = var.server_port\n    db_address  = data.terraform_remote_state.db.outputs.address\n    db_port     = data.terraform_remote_state.db.outputs.port \n  }) \n \n  # Required when using a launch configuration with an auto \nscaling group. \n  lifecycle {\n    create_before_destroy = true \n  }\n}\nSimilarly, you should update the ASG definition in the same file to set its\nmin_size and max_size parameters to the new var.min_size and\nvar.max_size input variables, respectively:\nresource \"aws_autoscaling_group\" \"example\" {\n  launch_configuration = aws_launch_configuration.example.name\n  vpc_zone_identifier  = data.aws_subnets.default.ids\n  target_group_arns    = [aws_lb_target_group.asg.arn]\n  health_check_type    = \"ELB\" \n \n  min_size = var.min_size\n\nmax_size = var.max_size \n \n  tag {\n    key                 = \"Name\"\n    value               = var.cluster_name\n    propagate_at_launch = true \n  }\n}\nNow, in the staging environment (stage/services/webserver-cluster/main.tf),\nyou can keep the cluster small and inexpensive by setting\ninstance_type to \"t2.micro\" and min_size and max_size to\n2:\nmodule \"webserver_cluster\" {\n  source = \"../../../modules/services/webserver-cluster\" \n \n  cluster_name           = \"webservers-stage\"\n  db_remote_state_bucket = \"(YOUR_BUCKET_NAME)\"\n  db_remote_state_key    = \"stage/data-\nstores/mysql/terraform.tfstate\" \n \n  instance_type = \"t2.micro\"\n  min_size      = 2\n  max_size      = 2\n}\nOn the other hand, in the production environment, you can use a larger\ninstance_type with more CPU and memory, such as m4.large (be\naware that this Instance type is not part of the AWS Free Tier, so if you’re\njust using this for learning and don’t want to be charged, stick with\n\"t2.micro\" for the instance_type), and you can set max_size to\n10 to allow the cluster to shrink or grow depending on the load (don’t\nworry, the cluster will launch with two Instances initially):\nmodule \"webserver_cluster\" {\n  source = \"../../../modules/services/webserver-cluster\" \n \n  cluster_name           = \"webservers-prod\"\n  db_remote_state_bucket = \"(YOUR_BUCKET_NAME)\"\n  db_remote_state_key    = \"prod/data-' metadata={'original_pages_range': '205-210', 'source': '059_Module_Inputs', 'file_path': '/mnt/c/Users/Carlos/Documents/Github/jupiter-iaa-azure/Scripts/RAG/../../data/optimized_chunks/Libro-TF/059_Module_Inputs.pdf', 'num_pages': 6}", "page_content='stores/mysql/terraform.tfstate\" \n \n  instance_type = \"m4.large\"\n  min_size      = 2\n  max_size      = 10\n}\nModule Locals\nUsing input variables to define your module’s inputs is great, but what if\nyou need a way to define a variable in your module to do some\nintermediary calculation, or just to keep your code DRY, but you don’t want\nto expose that variable as a configurable input? For example, the load\nbalancer in the webserver-cluster module in\nmodules/services/webserver-cluster/main.tf listens on port 80, the default\nport for HTTP. This port number is currently copied and pasted in multiple\nplaces, including the load balancer listener:\nresource \"aws_lb_listener\" \"http\" {\n  load_balancer_arn = aws_lb.example.arn\n  port              = 80\n  protocol          = \"HTTP\" \n \n  # By default, return a simple 404 page \n  default_action {\n    type = \"fixed-response\" \n \n    fixed_response {\n      content_type = \"text/plain\"\n      message_body = \"404: page not found\"\n      status_code  = 404 \n    } \n  }\n}\nAnd the load balancer security group:\nresource \"aws_security_group\" \"alb\" {\n  name = \"${var.cluster_name}-alb\" \n \n  ingress {\n\nfrom_port   = 80\n    to_port     = 80\n    protocol    = \"tcp\"\n    cidr_blocks = [\"0.0.0.0/0\"] \n  } \n \n  egress {\n    from_port   = 0\n    to_port     = 0\n    protocol    = \"-1\"\n    cidr_blocks = [\"0.0.0.0/0\"] \n  }\n}\nThe values in the security group, including the “all IPs” CIDR block\n0.0.0.0/0, the “any port” value of 0, and the “any protocol” value of\n\"-1\" are also copied and pasted in several places throughout the module.\nHaving these magical values hardcoded in multiple places makes the code\nmore difficult to read and maintain. You could extract values into input\nvariables, but then users of your module will be able to (accidentally)\noverride these values, which you might not want. Instead of using input\nvariables, you can define these as local values in a locals block:\nlocals {\n  http_port    = 80\n  any_port     = 0\n  any_protocol = \"-1\"\n  tcp_protocol = \"tcp\"\n  all_ips      = [\"0.0.0.0/0\"]\n}\nLocal values allow you to assign a name to any Terraform expression and to\nuse that name throughout the module. These names are visible only within\nthe module, so they will have no impact on other modules, and you can’t\noverride these values from outside of the module. To read the value of a\nlocal, you need to use a local reference, which uses the following syntax:\nlocal.<NAME>\n\nUse this syntax to update the port parameter of your load-balancer\nlistener:\nresource \"aws_lb_listener\" \"http\" {\n  load_balancer_arn = aws_lb.example.arn\n  port              = local.http_port\n  protocol          = \"HTTP\" \n \n  # By default, return a simple 404 page \n  default_action {\n    type = \"fixed-response\" \n \n    fixed_response {\n      content_type = \"text/plain\"\n      message_body = \"404: page not found\"\n      status_code  = 404 \n    } \n  }\n}\nSimilarly, update virtually all the parameters in the security groups in the\nmodule, including the load-balancer security group:\nresource \"aws_security_group\" \"alb\" {\n  name = \"${var.cluster_name}-alb\" \n \n  ingress {\n    from_port   = local.http_port\n    to_port     = local.http_port\n    protocol    = local.tcp_protocol\n    cidr_blocks = local.all_ips \n  } \n \n  egress {\n    from_port   = local.any_port\n    to_port     = local.any_port\n    protocol    = local.any_protocol\n    cidr_blocks = local.all_ips \n  }\n}\nLocals make your code easier to read and maintain, so use them often.' metadata={'original_pages_range': '211-213', 'source': '060_Module_Locals', 'file_path': '/mnt/c/Users/Carlos/Documents/Github/jupiter-iaa-azure/Scripts/RAG/../../data/optimized_chunks/Libro-TF/060_Module_Locals.pdf', 'num_pages': 3}", "page_content='Module Outputs\nA powerful feature of ASGs is that you can configure them to increase or\ndecrease the number of servers you have running in response to load. One\nway to do this is to use a scheduled action, which can change the size of the\ncluster at a scheduled time during the day. For example, if traffic to your\ncluster is much higher during normal business hours, you can use a\nscheduled action to increase the number of servers at 9 a.m. and decrease it\nat 5 p.m.\nIf you define the scheduled action in the webserver-cluster module,\nit would apply to both staging and production. Because you don’t need to\ndo this sort of scaling in your staging environment, for the time being, you\ncan define the auto scaling schedule directly in the production\nconfigurations (in Chapter 5, you’ll see how to conditionally define\nresources, which lets you move the scheduled action into the webserver-\ncluster module).\nTo define a scheduled action, add the following two\naws_autoscaling_schedule resources to prod/services/webserver-\ncluster/main.tf:\nresource \"aws_autoscaling_schedule\" \n\"scale_out_during_business_hours\" {\n  scheduled_action_name = \"scale-out-during-business-hours\"\n  min_size              = 2\n  max_size              = 10\n  desired_capacity      = 10\n  recurrence            = \"0 9 * * *\"\n} \n \nresource \"aws_autoscaling_schedule\" \"scale_in_at_night\" {\n  scheduled_action_name = \"scale-in-at-night\"\n  min_size              = 2\n  max_size              = 10\n  desired_capacity      = 2\n  recurrence            = \"0 17 * * *\"\n}\n\nThis code uses one aws_autoscaling_schedule resource to increase\nthe number of servers to 10 during the morning hours (the recurrence\nparameter uses cron syntax, so \"0 9 * * *\" means “9 a.m. every day”)\nand a second aws_autoscaling_schedule resource to decrease the\nnumber of servers at night (\"0 17 * * *\" means “5 p.m. every day”).\nHowever, both usages of aws_autoscaling_schedule are missing a\nrequired parameter, autoscaling_group_name, which specifies the\nname of the ASG. The ASG itself is defined within the webserver-\ncluster module, so how do you access its name? In a general-purpose\nprogramming language such as Ruby, functions can return values:\n# A function that returns a value\ndef example_function(param1, param2) \n  return \"Hello, #{param1} #{param2}\"\nend \n \n# Call the function and get the return value\nreturn_value = example_function(\"foo\", \"bar\")\nIn Terraform, a module can also return values. Again, you do this using a\nmechanism you already know: output variables. You can add the ASG name\nas an output variable in /modules/services/webserver-cluster/outputs.tf as\nfollows:\noutput \"asg_name\" {\n  value       = aws_autoscaling_group.example.name\n  description = \"The name of the Auto Scaling Group\"\n}\nYou can access module output variables using the following syntax:\nmodule.<MODULE_NAME>.<OUTPUT_NAME>\nFor example:\nmodule.frontend.asg_name\n\nIn prod/services/webserver-cluster/main.tf, you can use this syntax to set\nthe autoscaling_group_name parameter in each of the\naws_autoscaling_schedule resources:\nresource \"aws_autoscaling_schedule\" \n\"scale_out_during_business_hours\" {\n  scheduled_action_name = \"scale-out-during-business-hours\"\n  min_size              = 2\n  max_size              = 10\n  desired_capacity      = 10\n  recurrence            = \"0 9 * * *\" \n \n  autoscaling_group_name = module.webserver_cluster.asg_name\n} \n \nresource \"aws_autoscaling_schedule\" \"scale_in_at_night\" {\n  scheduled_action_name = \"scale-in-at-night\"\n  min_size              = 2\n  max_size              = 10\n  desired_capacity      = 2\n  recurrence            = \"0 17 * * *\" \n \n  autoscaling_group_name = module.webserver_cluster.asg_name\n}\nYou might want to expose one other output in the webserver-cluster\nmodule: the DNS name of the ALB, so you know what URL to test when\nthe cluster is deployed. To do that, you again add an output variable in\n/modules/services/webserver-cluster/outputs.tf:\noutput \"alb_dns_name\" {\n  value       = aws_lb.example.dns_name\n  description = \"The domain name of the load balancer\"\n}\nYou can then “pass through” this output in stage/services/webserver-\ncluster/outputs.tf and prod/services/webserver-cluster/outputs.tf as follows:\noutput \"alb_dns_name\" {\n  value       = module.webserver_cluster.alb_dns_name\n  description = \"The domain name of the load balancer\"\n}' metadata={'original_pages_range': '214-216', 'source': '061_Module_Outputs', 'file_path': '/mnt/c/Users/Carlos/Documents/Github/jupiter-iaa-azure/Scripts/RAG/../../data/optimized_chunks/Libro-TF/061_Module_Outputs.pdf', 'num_pages': 3}", "page_content='Your web server cluster is almost ready to deploy. The only thing left is to\ntake a few gotchas into account.\nModule Gotchas\nWhen creating modules, watch out for these gotchas:\nFile paths\nInline blocks\nFile Paths\nIn Chapter 3, you moved the User Data script for the web server cluster into\nan external file, user-data.sh, and used the templatefile built-in\nfunction to read this file from disk. The catch with the templatefile\nfunction is that the filepath you use must be a relative path (you don’t want\nto use absolute file paths, as your Terraform code may run on many\ndifferent computers, each with a different disk layout)—but what is it\nrelative to?\nBy default, Terraform interprets the path relative to the current working\ndirectory. That works if you’re using the templatefile function in a\nTerraform configuration file that’s in the same directory as where you’re\nrunning terraform apply (that is, if you’re using the\ntemplatefile function in the root module), but that won’t work when\nyou’re using templatefile in a module that’s defined in a separate\nfolder (a reusable module).\nTo solve this issue, you can use an expression known as a path reference,\nwhich is of the form path.<TYPE>. Terraform supports the following\ntypes of path references:\npath.module\nReturns the filesystem path of the module where the expression is\ndefined.' metadata={'original_pages_range': '217', 'source': '062_Module_Gotchas_y_1_mas', 'file_path': '/mnt/c/Users/Carlos/Documents/Github/jupiter-iaa-azure/Scripts/RAG/../../data/optimized_chunks/Libro-TF/062_Module_Gotchas_y_1_mas.pdf', 'num_pages': 1}", "page_content='path.root\nReturns the filesystem path of the root module.\npath.cwd\nReturns the filesystem path of the current working directory. In normal\nuse of Terraform, this is the same as path.root, but some advanced\nuses of Terraform run it from a directory other than the root module\ndirectory, causing these paths to be different.\nFor the User Data script, you need a path relative to the module itself, so\nyou should use path.module when calling the templatefile\nfunction in modules/services/webserver-cluster/main.tf:\n  user_data = templatefile(\"${path.module}/user-data.sh\", {\n    server_port = var.server_port\n    db_address  = data.terraform_remote_state.db.outputs.address\n    db_port     = data.terraform_remote_state.db.outputs.port \n  })\nInline Blocks\nThe configuration for some Terraform resources can be defined either as\ninline blocks or as separate resources. An inline block is an argument you\nset within a resource of the format:\nresource \"xxx\" \"yyy\" { \n  <NAME> { \n    [CONFIG...] \n  }\n}\nwhere NAME is the name of the inline block (e.g., ingress) and CONFIG\nconsists of one or more arguments that are specific to that inline block (e.g.,\nfrom_port and to_port). For example, with the\naws_security_group_resource, you can define ingress and egress\n\nrules using either inline blocks (e.g., ingress { … }) or separate\naws_security_group_rule resources.\nIf you try to use a mix of both inline blocks and separate resources, due to\nhow Terraform is designed, you will get errors where the configurations\nconflict and overwrite one another. Therefore, you must use one or the\nother. Here’s my advice: when creating a module, you should always prefer\nusing separate resources.\nThe advantage of using separate resources is that they can be added\nanywhere, whereas an inline block can only be added within the module\nthat creates a resource. So using solely separate resources makes your\nmodule more flexible and configurable.\nFor example, in the webserver-cluster module\n(modules/services/webserver-cluster/main.tf), you used inline blocks to\ndefine ingress and egress rules:\nresource \"aws_security_group\" \"alb\" {\n  name = \"${var.cluster_name}-alb\" \n \n  ingress {\n    from_port   = local.http_port\n    to_port     = local.http_port\n    protocol    = local.tcp_protocol\n    cidr_blocks = local.all_ips \n  } \n \n  egress {\n    from_port   = local.any_port\n    to_port     = local.any_port\n    protocol    = local.any_protocol\n    cidr_blocks = local.all_ips \n  }\n}\nWith these inline blocks, a user of this module has no way to add additional\ningress or egress rules from outside the module. To make your module more\nflexible, you should change it to define the exact same ingress and egress\n\nrules by using separate aws_security_group_rule resources (make\nsure to do this for both security groups in the module):\nresource \"aws_security_group\" \"alb\" {\n  name = \"${var.cluster_name}-alb\"\n} \n \nresource \"aws_security_group_rule\" \"allow_http_inbound\" {\n  type              = \"ingress\"\n  security_group_id = aws_security_group.alb.id \n \n  from_port   = local.http_port\n  to_port     = local.http_port\n  protocol    = local.tcp_protocol\n  cidr_blocks = local.all_ips\n} \n \nresource \"aws_security_group_rule\" \"allow_all_outbound\" {\n  type              = \"egress\"\n  security_group_id = aws_security_group.alb.id \n \n  from_port   = local.any_port\n  to_port     = local.any_port\n  protocol    = local.any_protocol\n  cidr_blocks = local.all_ips\n}\nYou should also export the ID of the aws_security_group as an\noutput variable in modules/services/webserver-cluster/outputs.tf:\noutput \"alb_security_group_id\" {\n  value       = aws_security_group.alb.id\n  description = \"The ID of the Security Group attached to the \nload balancer\"\n}\nNow, if you needed to expose an extra port in just the staging environment\n(e.g., for testing), you can do this by adding an\naws_security_group_rule resource to stage/services/webserver-\ncluster/main.tf:\n\nmodule \"webserver_cluster\" {\n  source = \"../../../modules/services/webserver-cluster\" \n \n  # (parameters hidden for clarity)\n} \n \nresource \"aws_security_group_rule\" \"allow_testing_inbound\" {\n  type              = \"ingress\"\n  security_group_id = \nmodule.webserver_cluster.alb_security_group_id \n \n  from_port   = 12345\n  to_port     = 12345\n  protocol    = \"tcp\"\n  cidr_blocks = [\"0.0.0.0/0\"]\n}\nHad you defined even a single ingress or egress rule as an inline block, this\ncode would not work. Note that this same type of problem affects a number\nof Terraform resources, such as the following:\naws_security_group and aws_security_group_rule\naws_route_table and aws_route\naws_network_acl and aws_network_acl_rule\nAt this point, you are finally ready to deploy your web server cluster in both\nstaging and production. Run terraform apply as usual, and enjoy\nusing two separate copies of your infrastructure.' metadata={'original_pages_range': '218-221', 'source': '063_Inline_Blocks', 'file_path': '/mnt/c/Users/Carlos/Documents/Github/jupiter-iaa-azure/Scripts/RAG/../../data/optimized_chunks/Libro-TF/063_Inline_Blocks.pdf', 'num_pages': 4}", "page_content='NETWORK ISOLATION\nThe examples in this chapter create two environments that are isolated\nin your Terraform code, as well as isolated in terms of having separate\nload balancers, servers, and databases, but they are not isolated at the\nnetwork level. To keep all the examples in this book simple, all of the\nresources deploy into the same VPC. This means that a server in the\nstaging environment can communicate with a server in the production\nenvironment, and vice versa.\nIn real-world usage, running both environments in one VPC opens you\nup to two risks. First, a mistake in one environment could affect the\nother. For example, if you’re making changes in staging and\naccidentally mess up the configuration of the route tables, all the\nrouting in production can be affected, too. Second, if an attacker gains\naccess to one environment, they also have access to the other. If you’re\nmaking rapid changes in staging and accidentally leave a port exposed,\nany hacker that broke in would have access to not only your staging\ndata but also your production data.\nTherefore, outside of simple examples and experiments, you should run\neach environment in a separate VPC. In fact, to be extra sure, you might\neven run each environment in a totally separate AWS account.\nModule Versioning\nIf both your staging and production environment are pointing to the same\nmodule folder, as soon as you make a change in that folder, it will affect\nboth environments on the very next deployment. This sort of coupling\nmakes it more difficult to test a change in staging without any chance of\naffecting production. A better approach is to create versioned modules so\nthat you can use one version in staging (e.g., v0.0.2) and a different version\nin production (e.g., v0.0.1), as shown in Figure 4-5.\n\nIn all of the module examples you’ve seen so far, whenever you used a\nmodule, you set the source parameter of the module to a local filepath. In\naddition to file paths, Terraform supports other types of module sources,\nsuch as Git URLs, Mercurial URLs, and arbitrary HTTP URLs.1\n\n\n\nFigure 4-5. By versioning your modules, you can use different versions in different environments:\ne.g., v0.0.1 in prod and v0.0.2 in stage.\nThe easiest way to create a versioned module is to put the code for the\nmodule in a separate Git repository and to set the source parameter to that\nrepository’s URL. That means your Terraform code will be spread out\nacross (at least) two repositories:\nmodules\nThis repo defines reusable modules. Think of each module as a\n“blueprint” that defines a specific part of your infrastructure.\nlive\nThis repo defines the live infrastructure you’re running in each\nenvironment (stage, prod, mgmt, etc.). Think of this as the “houses” you\nbuilt from the “blueprints” in the modules repo.\nThe updated folder structure for your Terraform code now looks something\nlike Figure 4-6.\n\n\n\nFigure 4-6. You should store reusable, versioned modules in one repo (modules) and the\nconfiguration for your live environments in another repo (live).\nTo set up this folder structure, you’ll first need to move the stage, prod, and\nglobal folders into a folder called live. Next, configure the live and modules\nfolders as separate Git repositories. Here is an example of how to do that for\nthe modules folder:\n$ cd modules \n$ git init \n$ git add . \n$ git commit -m \"Initial commit of modules repo\" \n$ git remote add origin \"(URL OF REMOTE GIT REPOSITORY)\" \n$ git push origin main\nYou can also add a tag to the modules repo to use as a version number. If\nyou’re using GitHub, you can use the GitHub UI to create a release, which\nwill create a tag under the hood.\nIf you’re not using GitHub, you can use the Git CLI:\n$ git tag -a \"v0.0.1\" -m \"First release of webserver-cluster \nmodule\" \n$ git push --follow-tags\nNow you can use this versioned module in both staging and production by\nspecifying a Git URL in the source parameter. Here is what that would\nlook like in live/stage/services/webserver-cluster/main.tf if your modules\nrepo was in the GitHub repo github.com/foo/modules (note that the double-\nslash in the following Git URL is required):\nmodule \"webserver_cluster\" {\n  source = \"github.com/foo/modules//services/webserver-cluster?\nref=v0.0.1\" \n \n  cluster_name           = \"webservers-stage\"\n  db_remote_state_bucket = \"(YOUR_BUCKET_NAME)\"\n  db_remote_state_key    = \"stage/data-\nstores/mysql/terraform.tfstate\" \n \n  instance_type = \"t2.micro\"\n\nmin_size      = 2\n  max_size      = 2\n}\nIf you want to try out versioned modules without messing with Git repos,\nyou can use a module from the code examples GitHub repo for this book (I\nhad to break up the URL to make it fit in the book, but it should all be on\none line):\nsource = \"github.com/brikis98/terraform-up-and-running-code// \n  code/terraform/04-terraform-module/module-example/modules/ \n  services/webserver-cluster?ref=v0.3.0\"\nThe ref parameter allows you to specify a particular Git commit via its\nsha1 hash, a branch name, or, as in this example, a specific Git tag. I\ngenerally recommend using Git tags as version numbers for modules.\nBranch names are not stable, as you always get the latest commit on a\nbranch, which may change every time you run the init command, and the\nsha1 hashes are not very human friendly. Git tags are as stable as a commit\n(in fact, a tag is just a pointer to a commit), but they allow you to use a\nfriendly, readable name.\nA particularly useful naming scheme for tags is semantic versioning. This is\na versioning scheme of the format MAJOR.MINOR.PATCH (e.g., 1.0.4)\nwith specific rules on when you should increment each part of the version\nnumber. In particular, you should increment the following:\nThe MAJOR version when you make incompatible API changes\nThe MINOR version when you add functionality in a backward-\ncompatible manner\nThe PATCH version when you make backward-compatible bug fixes\nSemantic versioning gives you a way to communicate to users of your\nmodule what kinds of changes you’ve made and the implications of\nupgrading.\n\nBecause you’ve updated your Terraform code to use a versioned module\nURL, you need to instruct Terraform to download the module code by\nrerunning terraform init:\n$ terraform init \nInitializing modules... \nDownloading git@github.com:brikis98/terraform-up-and-running-\ncode.git?ref=v0.3.0 \nfor webserver_cluster... \n \n(...)\nThis time, you can see that Terraform downloads the module code from Git\nrather than your local filesystem. After the module code has been\ndownloaded, you can run the apply command as usual.\nPRIVATE GIT REPOS\nIf your Terraform module is in a private Git repository, to use that repo as a module\nsource, you need to give Terraform a way to authenticate to that Git repository. I\nrecommend using SSH auth so that you don’t need to hardcode the credentials for your\nrepo in the code itself. With SSH authentication, each developer can create an SSH key,\nassociate it with their Git user, add it to ssh-agent, and Terraform will automatically\nuse that key for authentication if you use an SSH source URL.\nThe source URL should be of the form:\ngit@github.com:<OWNER>/<REPO>.git//<PATH>?ref=<VERSION>\nFor example:\ngit@github.com:acme/modules.git//example?ref=v0.1.2\nTo check that you’ve formatted the URL correctly, try to git clone the base URL\nfrom your terminal:\n$ git clone git@github.com:acme/modules.git\nIf that command succeeds, Terraform should be able to use the private repo, too.\n2\n\nNow that you’re using versioned modules, let’s walk through the process of\nmaking changes. Let’s say you made some changes to the webserver-\ncluster module, and you want to test them out in staging. First, you’d\ncommit those changes to the modules repo:\n$ cd modules \n$ git add . \n$ git commit -m \"Made some changes to webserver-cluster\" \n$ git push origin main\nNext, you would create a new tag in the modules repo:\n$ git tag -a \"v0.0.2\" -m \"Second release of webserver-cluster\" \n$ git push --follow-tags\nAnd now you can update just the source URL used in the staging\nenvironment (live/stage/services/webserver-cluster/main.tf) to use this new\nversion:\nmodule \"webserver_cluster\" {\n  source = \"github.com/foo/modules//services/webserver-cluster?\nref=v0.0.2\" \n \n  cluster_name           = \"webservers-stage\"\n  db_remote_state_bucket = \"(YOUR_BUCKET_NAME)\"\n  db_remote_state_key    = \"stage/data-\nstores/mysql/terraform.tfstate\" \n \n  instance_type = \"t2.micro\"\n  min_size      = 2\n  max_size      = 2\n}\nIn production (live/prod/services/webserver-cluster/main.tf), you can\nhappily continue to run v0.0.1 unchanged:\nmodule \"webserver_cluster\" {\n  source = \"github.com/foo/modules//services/webserver-cluster?\nref=v0.0.1\" \n \n  cluster_name           = \"webservers-prod\"' metadata={'original_pages_range': '222-230', 'source': '064_Module_Versioning', 'file_path': '/mnt/c/Users/Carlos/Documents/Github/jupiter-iaa-azure/Scripts/RAG/../../data/optimized_chunks/Libro-TF/064_Module_Versioning.pdf', 'num_pages': 9}", "page_content='db_remote_state_bucket = \"(YOUR_BUCKET_NAME)\"\n  db_remote_state_key    = \"prod/data-\nstores/mysql/terraform.tfstate\" \n \n  instance_type = \"m4.large\"\n  min_size      = 2\n  max_size      = 10\n}\nAfter v0.0.2 has been thoroughly tested and proven in staging, you can then\nupdate production, too. But if there turns out to be a bug in v0.0.2, no big\ndeal, because it has no effect on the real users of your production\nenvironment. Fix the bug, release a new version, and repeat the entire\nprocess again until you have something stable enough for production.\nDEVELOPING MODULES\nVersioned modules are great when you’re deploying to a shared environment (e.g.,\nstaging or production), but when you’re just testing on your own computer, you’ll want\nto use local file paths. This allows you to iterate faster, because you’ll be able to make a\nchange in the module folders and rerun the plan or apply command in the live\nfolders immediately, rather than having to commit your code, publish a new version, and\nrerun init each time.\nSince the goal of this book is to help you learn and experiment with Terraform as\nquickly as possible, the rest of the code examples will use local file paths for modules.\nConclusion\nBy defining infrastructure as code in modules, you can apply a variety of\nsoftware engineering best practices to your infrastructure. You can validate\neach change to a module through code reviews and automated tests, you can\ncreate semantically versioned releases of each module, and you can safely\ntry out different versions of a module in different environments and roll\nback to previous versions if you hit a problem.\nAll of this can dramatically increase your ability to build infrastructure\nquickly and reliably because developers will be able to reuse entire pieces\n\nof proven, tested, and documented infrastructure. For example, you could\ncreate a canonical module that defines how to deploy a single microservice\n—including how to run a cluster, how to scale the cluster in response to\nload, and how to distribute traffic requests across the cluster—and each\nteam could use this module to manage their own microservices with just a\nfew lines of code.\nTo make such a module work for multiple teams, the Terraform code in that\nmodule must be flexible and configurable. For example, one team might\nwant to use your module to deploy a single Instance of their microservice\nwith no load balancer, whereas another might want a dozen Instances of\ntheir microservice with a load balancer to distribute traffic between those\nInstances. How do you do conditional statements in Terraform? Is there a\nway to do a for-loop? Is there a way to use Terraform to roll out changes to\nthis microservice without downtime? These advanced aspects of Terraform\nsyntax are the topic of Chapter 5.\n1 For the full details on source URLs, see the Terraform website.\n2 See the GitHub documentation for a nice guide on working with SSH keys.' metadata={'original_pages_range': '231-232', 'source': '065_Conclusion', 'file_path': '/mnt/c/Users/Carlos/Documents/Github/jupiter-iaa-azure/Scripts/RAG/../../data/optimized_chunks/Libro-TF/065_Conclusion.pdf', 'num_pages': 2}", "page_content='Chapter 5. Terraform Tips and\nTricks: Loops, If-Statements,\nDeployment, and Gotchas\nTerraform is a declarative language. As discussed in Chapter 1, IaC in a\ndeclarative language tends to provide a more accurate view of what’s\nactually deployed than a procedural language, so it’s easier to reason about\nand makes it easier to keep the codebase small. However, certain types of\ntasks are more difficult in a declarative language.\nFor example, because declarative languages typically don’t have for-loops,\nhow do you repeat a piece of logic—such as creating multiple similar\nresources—without copy and paste? And if the declarative language doesn’t\nsupport if-statements, how can you conditionally configure resources, such\nas creating a Terraform module that can create certain resources for some\nusers of that module but not for others? Finally, how do you express an\ninherently procedural idea, such as a zero-downtime deployment, in a\ndeclarative language?\nFortunately, Terraform provides a few primitives—namely, the meta-\nparameter count, for_each and for expressions, a ternary operator, a\nlifecycle block called create_before_destroy, and a large number\nof functions—that allow you to do certain types of loops, if-statements, and\nzero-downtime deployments. Here are the topics I’ll cover in this chapter:\nLoops\nConditionals\nZero-downtime deployment\nTerraform gotchas' metadata={'original_pages_range': '233', 'source': '066_5._Terraform_Tips_and_Tricks_Loops,_If-Statements,_Deployment,_and_Gotchas', 'file_path': '/mnt/c/Users/Carlos/Documents/Github/jupiter-iaa-azure/Scripts/RAG/../../data/optimized_chunks/Libro-TF/066_5._Terraform_Tips_and_Tricks_Loops,_If-Statements,_Deployment,_and_Gotchas.pdf', 'num_pages': 1}", "page_content='EXAMPLE CODE\nAs a reminder, you can find all of the code examples in the book on GitHub.\nLoops\nTerraform offers several different looping constructs, each intended to be\nused in a slightly different scenario:\ncount parameter, to loop over resources and modules\nfor_each expressions, to loop over resources, inline blocks within a\nresource, and modules\nfor expressions, to loop over lists and maps\nfor string directive, to loop over lists and maps within a string\nLet’s go through these one at a time.\nLoops with the count Parameter\nIn Chapter 2, you created an AWS Identity and Access Management (IAM)\nuser by clicking around the Console. Now that you have this user, you can\ncreate and manage all future IAM users with Terraform. Consider the\nfollowing Terraform code, which should live in live/global/iam/main.tf:\nprovider \"aws\" {\n  region = \"us-east-2\"\n} \n \nresource \"aws_iam_user\" \"example\" {\n  name = \"neo\"\n}\nThis code uses the aws_iam_user resource to create a single new IAM\nuser. What if you want to create three IAM users? In a general-purpose\nprogramming language, you’d probably use a for-loop:' metadata={'original_pages_range': '234', 'source': '067_Loops', 'file_path': '/mnt/c/Users/Carlos/Documents/Github/jupiter-iaa-azure/Scripts/RAG/../../data/optimized_chunks/Libro-TF/067_Loops.pdf', 'num_pages': 1}", "page_content='EXAMPLE CODE\nAs a reminder, you can find all of the code examples in the book on GitHub.\nLoops\nTerraform offers several different looping constructs, each intended to be\nused in a slightly different scenario:\ncount parameter, to loop over resources and modules\nfor_each expressions, to loop over resources, inline blocks within a\nresource, and modules\nfor expressions, to loop over lists and maps\nfor string directive, to loop over lists and maps within a string\nLet’s go through these one at a time.\nLoops with the count Parameter\nIn Chapter 2, you created an AWS Identity and Access Management (IAM)\nuser by clicking around the Console. Now that you have this user, you can\ncreate and manage all future IAM users with Terraform. Consider the\nfollowing Terraform code, which should live in live/global/iam/main.tf:\nprovider \"aws\" {\n  region = \"us-east-2\"\n} \n \nresource \"aws_iam_user\" \"example\" {\n  name = \"neo\"\n}\nThis code uses the aws_iam_user resource to create a single new IAM\nuser. What if you want to create three IAM users? In a general-purpose\nprogramming language, you’d probably use a for-loop:\n\n# This is just pseudo code. It won't actually work in Terraform.\nfor (i = 0; i < 3; i++) { \n  resource \"aws_iam_user\" \"example\" {\n    name = \"neo\" \n  }\n}\nTerraform does not have for-loops or other traditional procedural logic built\ninto the language, so this syntax will not work. However, every Terraform\nresource has a meta-parameter you can use called count. count is\nTerraform’s oldest, simplest, and most limited iteration construct: all it does\nis define how many copies of the resource to create. Here’s how you use\ncount to create three IAM users:\nresource \"aws_iam_user\" \"example\" {\n  count = 3\n  name  = \"neo\"\n}\nOne problem with this code is that all three IAM users would have the same\nname, which would cause an error, since usernames must be unique. If you\nhad access to a standard for-loop, you might use the index in the for-loop,\ni, to give each user a unique name:\n# This is just pseudo code. It won't actually work in Terraform.\nfor (i = 0; i < 3; i++) { \n  resource \"aws_iam_user\" \"example\" {\n    name = \"neo.${i}\" \n  }\n}\nTo accomplish the same thing in Terraform, you can use count.index to\nget the index of each “iteration” in the “loop”:\nresource \"aws_iam_user\" \"example\" {\n  count = 3\n  name  = \"neo.${count.index}\"\n}\n\nIf you run the plan command on the preceding code, you will see that\nTerraform wants to create three IAM users, each with a different name\n(\"neo.0\", \"neo.1\", \"neo.2\"):\nTerraform will perform the following actions: \n \n  # aws_iam_user.example[0] will be created \n  + resource \"aws_iam_user\" \"example\" { \n      + name          = \"neo.0\" \n      (...) \n    } \n \n  # aws_iam_user.example[1] will be created \n  + resource \"aws_iam_user\" \"example\" { \n      + name          = \"neo.1\" \n      (...) \n    } \n \n  # aws_iam_user.example[2] will be created \n  + resource \"aws_iam_user\" \"example\" { \n      + name          = \"neo.2\" \n      (...) \n    } \n \nPlan: 3 to add, 0 to change, 0 to destroy.\nOf course, a username like \"neo.0\" isn’t particularly usable. If you\ncombine count.index with some built-in functions from Terraform, you\ncan customize each “iteration” of the “loop” even more.\nFor example, you could define all of the IAM usernames you want in an\ninput variable in live/global/iam/variables.tf:\nvariable \"user_names\" {\n  description = \"Create IAM users with these names\"\n  type        = list(string)\n  default     = [\"neo\", \"trinity\", \"morpheus\"]\n}\nIf you were using a general-purpose programming language with loops and\narrays, you would configure each IAM user to use a different name by\nlooking up index i in the array var.user_names:\n\n# This is just pseudo code. It won't actually work in Terraform.\nfor (i = 0; i < 3; i++) { \n  resource \"aws_iam_user\" \"example\" {\n    name = vars.user_names[i] \n  }\n}\nIn Terraform, you can accomplish the same thing by using count along\nwith the following:\nArray lookup syntax\nThe syntax for looking up members of an array in Terraform is similar\nto most other programming languages:\nARRAY[<INDEX>]\nFor example, here’s how you would look up the element at index 1 of\nvar.user_names:\nvar.user_names[1]\nThe length function\nTerraform has a built-in function called length that has the following\nsyntax:\nlength(<ARRAY>)\nAs you can probably guess, the length function returns the number of\nitems in the given ARRAY. It also works with strings and maps.\nPutting these together, you get the following:\nresource \"aws_iam_user\" \"example\" {\n  count = length(var.user_names)\n\nname  = var.user_names[count.index]\n}\nNow when you run the plan command, you’ll see that Terraform wants to\ncreate three IAM users, each with a unique, readable name:\nTerraform will perform the following actions: \n \n  # aws_iam_user.example[0] will be created \n  + resource \"aws_iam_user\" \"example\" { \n      + name          = \"neo\" \n      (...) \n    } \n \n  # aws_iam_user.example[1] will be created \n  + resource \"aws_iam_user\" \"example\" { \n      + name          = \"trinity\" \n      (...) \n    } \n \n  # aws_iam_user.example[2] will be created \n  + resource \"aws_iam_user\" \"example\" { \n      + name          = \"morpheus\" \n      (...) \n    } \n \nPlan: 3 to add, 0 to change, 0 to destroy.\nNote that after you’ve used count on a resource, it becomes an array of\nresources rather than just one resource. Because\naws_iam_user.example is now an array of IAM users, instead of\nusing the standard syntax to read an attribute from that resource\n(<PROVIDER>_<TYPE>.<NAME>.<ATTRIBUTE>), you must specify\nwhich IAM user you’re interested in by specifying its index in the array\nusing the same array lookup syntax:\n<PROVIDER>_<TYPE>.<NAME>[INDEX].ATTRIBUTE\nFor example, if you want to provide the Amazon Resource Name (ARN) of\nthe first IAM user in the list as an output variable, you would need to do the\nfollowing:\n\noutput \"first_arn\" {\n  value       = aws_iam_user.example[0].arn\n  description = \"The ARN for the first user\"\n}\nIf you want the ARNs of all of the IAM users, you need to use a splat\nexpression, “*”, instead of the index:\noutput \"all_arns\" {\n  value       = aws_iam_user.example[*].arn\n  description = \"The ARNs for all users\"\n}\nWhen you run the apply command, the first_arn output will contain\njust the ARN for neo, whereas the all_arns output will contain the list\nof all ARNs:\n$ terraform apply \n \n(...) \n \nApply complete! Resources: 3 added, 0 changed, 0 destroyed. \n \nOutputs: \n \nfirst_arn = \"arn:aws:iam::123456789012:user/neo\" \nall_arns = [ \n  \"arn:aws:iam::123456789012:user/neo\", \n  \"arn:aws:iam::123456789012:user/trinity\", \n  \"arn:aws:iam::123456789012:user/morpheus\", \n]\nAs of Terraform 0.13, the count parameter can also be used on modules.\nFor example, imagine you had a module at modules/landing-zone/iam-user\nthat can create a single IAM user:\nresource \"aws_iam_user\" \"example\" {\n  name = var.user_name\n}\nThe username is passed into this module as an input variable:\n\nvariable \"user_name\" {\n  description = \"The user name to use\"\n  type        = string\n}\nAnd the module returns the ARN of the created IAM user as an output\nvariable:\noutput \"user_arn\" {\n  value       = aws_iam_user.example.arn\n  description = \"The ARN of the created IAM user\"\n}\nYou could use this module with a count parameter to create three IAM\nusers as follows:\nmodule \"users\" {\n  source = \"../../../modules/landing-zone/iam-user\" \n \n  count     = length(var.user_names)\n  user_name = var.user_names[count.index]\n}\nThe preceding code uses count to loop over this list of usernames:\nvariable \"user_names\" {\n  description = \"Create IAM users with these names\"\n  type        = list(string)\n  default     = [\"neo\", \"trinity\", \"morpheus\"]\n}\nAnd it outputs the ARNs of the created IAM users as follows:\noutput \"user_arns\" {\n  value       = module.users[*].user_arn\n  description = \"The ARNs of the created IAM users\"\n}\nJust as adding count to a resource turns it into an array of resources,\nadding count to a module turns it into an array of modules.\n\nIf you run apply on this code, you’ll get the following output:\n$ terraform apply \n \n(...) \n \nApply complete! Resources: 3 added, 0 changed, 0 destroyed. \n \nOutputs: \n \nall_arns = [ \n  \"arn:aws:iam::123456789012:user/neo\", \n  \"arn:aws:iam::123456789012:user/trinity\", \n  \"arn:aws:iam::123456789012:user/morpheus\", \n]\nSo, as you can see, count works more or less identically with resources\nand with modules.\nUnfortunately, count has two limitations that significantly reduce its\nusefulness. First, although you can use count to loop over an entire\nresource, you can’t use count within a resource to loop over inline blocks.\nFor example, consider how tags are set in the\naws_autoscaling_group resource:\nresource \"aws_autoscaling_group\" \"example\" {\n  launch_configuration = aws_launch_configuration.example.name\n  vpc_zone_identifier  = data.aws_subnets.default.ids\n  target_group_arns    = [aws_lb_target_group.asg.arn]\n  health_check_type    = \"ELB\" \n \n  min_size = var.min_size\n  max_size = var.max_size \n \n  tag {\n    key                 = \"Name\"\n    value               = var.cluster_name\n    propagate_at_launch = true \n  }\n}\n\nEach tag requires you to create a new inline block with values for key,\nvalue, and propagate_at_launch. The preceding code hardcodes a\nsingle tag, but you might want to allow users to pass in custom tags. You\nmight be tempted to try to use the count parameter to loop over these tags\nand generate dynamic inline tag blocks, but unfortunately, using count\nwithin an inline block is not supported.\nThe second limitation with count is what happens when you try to change\nits value. Consider the list of IAM users you created earlier:\nvariable \"user_names\" {\n  description = \"Create IAM users with these names\"\n  type        = list(string)\n  default     = [\"neo\", \"trinity\", \"morpheus\"]\n}\nImagine that you removed \"trinity\" from this list. What happens when\nyou run terraform plan?\n$ terraform plan \n \n(...) \n \nTerraform will perform the following actions: \n \n  # aws_iam_user.example[1] will be updated in-place \n  ~ resource \"aws_iam_user\" \"example\" { \n        id            = \"trinity\" \n      ~ name          = \"trinity\" -> \"morpheus\" \n    } \n \n  # aws_iam_user.example[2] will be destroyed \n  - resource \"aws_iam_user\" \"example\" { \n      - id            = \"morpheus\" -> null \n      - name          = \"morpheus\" -> null \n    } \n \nPlan: 0 to add, 1 to change, 1 to destroy.\nWait a second, that’s probably not what you were expecting! Instead of just\ndeleting the \"trinity\" IAM user, the plan output is indicating that\n\nTerraform wants to rename the \"trinity\" IAM user to \"morpheus\"\nand delete the original \"morpheus\" user. What’s going on?\nWhen you use the count parameter on a resource, that resource becomes\nan array of resources. Unfortunately, the way Terraform identifies each\nresource within the array is by its position (index) in that array. That is,\nafter running apply the first time with three usernames, Terraform’s\ninternal representation of these IAM users looks something like this:\naws_iam_user.example[0]: neo \naws_iam_user.example[1]: trinity \naws_iam_user.example[2]: morpheus\nWhen you remove an item from the middle of an array, all the items after it\nshift back by one, so after running plan with just two bucket names,\nTerraform’s internal representation will look something like this:\naws_iam_user.example[0]: neo \naws_iam_user.example[1]: morpheus\nNotice how \"morpheus\" has moved from index 2 to index 1. Because it\nsees the index as a resource’s identity, to Terraform, this change roughly\ntranslates to “rename the bucket at index 1 to morpheus and delete the\nbucket at index 2.” In other words, every time you use count to create a\nlist of resources, if you remove an item from the middle of the list,\nTerraform will delete every resource after that item and then re-create those\nresources again from scratch. Ouch. The end result, of course, is exactly\nwhat you requested (i.e., two IAM users named \"morpheus\" and\n\"neo\"), but deleting resources is probably not how you want to get there,\nas you may lose availability (you can’t use the IAM user during the\napply), and, even worse, you may lose data (if the resource you’re\ndeleting is a database, you may lose all the data in it!).\nTo solve these two limitations, Terraform 0.12 introduced for_each\nexpressions.' metadata={'original_pages_range': '234-243', 'source': '068_Loops_with_the_count_Parameter', 'file_path': '/mnt/c/Users/Carlos/Documents/Github/jupiter-iaa-azure/Scripts/RAG/../../data/optimized_chunks/Libro-TF/068_Loops_with_the_count_Parameter.pdf', 'num_pages': 10}", "page_content='Loops with for_each Expressions\nThe for_each expression allows you to loop over lists, sets, and maps to\ncreate (a) multiple copies of an entire resource, (b) multiple copies of an\ninline block within a resource, or (c) multiple copies of a module. Let’s first\nwalk through how to use for_each to create multiple copies of a\nresource.\nThe syntax looks like this:\nresource \"<PROVIDER>_<TYPE>\" \"<NAME>\" {\n  for_each = <COLLECTION> \n \n  [CONFIG ...]\n}\nwhere COLLECTION is a set or map to loop over (lists are not supported\nwhen using for_each on a resource) and CONFIG consists of one or\nmore arguments that are specific to that resource. Within CONFIG, you can\nuse each.key and each.value to access the key and value of the\ncurrent item in COLLECTION.\nFor example, here’s how you can create the same three IAM users using\nfor_each on a resource:\nresource \"aws_iam_user\" \"example\" {\n  for_each = toset(var.user_names)\n  name     = each.value\n}\nNote the use of toset to convert the var.user_names list into a set.\nThis is because for_each supports sets and maps only when used on a\nresource. When for_each loops over this set, it makes each username\navailable in each.value. The username will also be available in\neach.key, though you typically use each.key only with maps of key-\nvalue pairs.\n\nOnce you’ve used for_each on a resource, it becomes a map of\nresources, rather than just one resource (or an array of resources as with\ncount). To see what that means, remove the original all_arns and\nfirst_arn output variables, and add a new all_users output variable:\noutput \"all_users\" {\n  value = aws_iam_user.example\n}\nHere’s what happens when you run terraform apply:\n$ terraform apply \n \n(...) \n \nApply complete! Resources: 3 added, 0 changed, 0 destroyed. \n \nOutputs: \n \nall_users = { \n  \"morpheus\" = { \n    \"arn\" = \"arn:aws:iam::123456789012:user/morpheus\" \n    \"force_destroy\" = false \n    \"id\" = \"morpheus\" \n    \"name\" = \"morpheus\" \n    \"path\" = \"/\" \n    \"tags\" = {} \n  } \n  \"neo\" = { \n    \"arn\" = \"arn:aws:iam::123456789012:user/neo\" \n    \"force_destroy\" = false \n    \"id\" = \"neo\" \n    \"name\" = \"neo\" \n    \"path\" = \"/\" \n    \"tags\" = {} \n  } \n  \"trinity\" = { \n    \"arn\" = \"arn:aws:iam::123456789012:user/trinity\" \n    \"force_destroy\" = false \n    \"id\" = \"trinity\" \n    \"name\" = \"trinity\" \n    \"path\" = \"/\" \n    \"tags\" = {}\n\n} \n}\nYou can see that Terraform created three IAM users and that the\nall_users output variable contains a map where the keys are the keys in\nfor_each (in this case, the usernames) and the values are all the outputs\nfor that resource. If you want to bring back the all_arns output variable,\nyou’d need to do a little extra work to extract those ARNs using the\nvalues built-in function (which returns just the values from a map) and a\nsplat expression:\noutput \"all_arns\" {\n  value = values(aws_iam_user.example)[*].arn\n}\nThis gives you the expected output:\n$ terraform apply \n \n(...) \n \nApply complete! Resources: 0 added, 0 changed, 0 destroyed. \n \nOutputs: \n \nall_arns = [ \n  \"arn:aws:iam::123456789012:user/morpheus\", \n  \"arn:aws:iam::123456789012:user/neo\", \n  \"arn:aws:iam::123456789012:user/trinity\", \n]\nThe fact that you now have a map of resources with for_each rather than\nan array of resources as with count is a big deal, because it allows you to\nremove items from the middle of a collection safely. For example, if you\nagain remove \"trinity\" from the middle of the var.user_names list\nand run terraform plan, here’s what you’ll see:\n$ terraform plan \n \nTerraform will perform the following actions:\n\n# aws_iam_user.example[\"trinity\"] will be destroyed \n  - resource \"aws_iam_user\" \"example\" { \n      - arn           = \"arn:aws:iam::123456789012:user/trinity\" \n-> null \n      - name          = \"trinity\" -> null \n    } \n \nPlan: 0 to add, 0 to change, 1 to destroy.\nThat’s more like it! You’re now deleting solely the exact resource you want,\nwithout shifting all of the other ones around. This is why you should almost\nalways prefer to use for_each instead of count to create multiple\ncopies of a resource.\nfor_each works with modules in a more or less identical fashion. Using\nthe iam-user module from earlier, you can create three IAM users with it\nusing for_each as follows:\nmodule \"users\" {\n  source = \"../../../modules/landing-zone/iam-user\" \n \n  for_each  = toset(var.user_names)\n  user_name = each.value\n}\nAnd you can output the ARNs of those users as follows:\noutput \"user_arns\" {\n  value       = values(module.users)[*].user_arn\n  description = \"The ARNs of the created IAM users\"\n}\nWhen you run apply on this code, you get the expected output:\n$ terraform apply \n \n(...) \n \nApply complete! Resources: 3 added, 0 changed, 0 destroyed. \n \nOutputs:\n\nall_arns = [ \n  \"arn:aws:iam::123456789012:user/morpheus\", \n  \"arn:aws:iam::123456789012:user/neo\", \n  \"arn:aws:iam::123456789012:user/trinity\", \n]\nLet’s now turn our attention to another advantage of for_each: its ability\nto create multiple inline blocks within a resource. For example, you can use\nfor_each to dynamically generate tag inline blocks for the ASG in the\nwebserver-cluster module. First, to allow users to specify custom\ntags, add a new map input variable called custom_tags in\nmodules/services/webserver-cluster/variables.tf:\nvariable \"custom_tags\" {\n  description = \"Custom tags to set on the Instances in the ASG\"\n  type        = map(string)\n  default     = {}\n}\nNext, set some custom tags in the production environment, in\nlive/prod/services/webserver-cluster/main.tf, as follows:\nmodule \"webserver_cluster\" {\n  source = \"../../../../modules/services/webserver-cluster\" \n \n  cluster_name           = \"webservers-prod\"\n  db_remote_state_bucket = \"(YOUR_BUCKET_NAME)\"\n  db_remote_state_key    = \"prod/data-\nstores/mysql/terraform.tfstate\" \n \n  instance_type        = \"m4.large\"\n  min_size             = 2\n  max_size             = 10 \n \n  custom_tags = {\n    Owner     = \"team-foo\"\n    ManagedBy = \"terraform\" \n  }\n}\n\nThe preceding code sets a couple of useful tags: the Owner tag specifies\nwhich team owns this ASG, and the ManagedBy tag specifies that this\ninfrastructure is managed using Terraform (indicating that this infrastructure\nshouldn’t be modified manually).\nNow that you’ve specified your tags, how do you actually set them on the\naws_autoscaling_group resource? What you need is a for-loop over\nvar.custom_tags, similar to the following pseudocode:\nresource \"aws_autoscaling_group\" \"example\" {\n  launch_configuration = aws_launch_configuration.example.name\n  vpc_zone_identifier  = data.aws_subnets.default.ids\n  target_group_arns    = [aws_lb_target_group.asg.arn]\n  health_check_type    = \"ELB\" \n \n  min_size = var.min_size\n  max_size = var.max_size \n \n  tag {\n    key                 = \"Name\"\n    value               = var.cluster_name\n    propagate_at_launch = true \n  } \n \n  # This is just pseudo code. It won't actually work in \nTerraform. \n  for (tag in var.custom_tags) { \n    tag {\n      key                 = tag.key\n      value               = tag.value\n      propagate_at_launch = true \n    } \n  }\n}\nThe preceding pseudocode won’t work, but a for_each expression will.\nThe syntax for using for_each to dynamically generate inline blocks\nlooks like this:\ndynamic \"<VAR_NAME>\" {\n  for_each = <COLLECTION> \n \n  content {\n\n[CONFIG...] \n  }\n}\nwhere VAR_NAME is the name to use for the variable that will store the\nvalue of each “iteration,” COLLECTION is a list or map to iterate over, and\nthe content block is what to generate from each iteration. You can use\n<VAR_NAME>.key and <VAR_NAME>.value within the content\nblock to access the key and value, respectively, of the current item in the\nCOLLECTION. Note that when you’re using for_each with a list, the\nkey will be the index, and the value will be the item in the list at that\nindex, and when using for_each with a map, the key and value will be\none of the key-value pairs in the map.\nPutting this all together, here is how you can dynamically generate tag\nblocks using for_each in the aws_autoscaling_group resource:\nresource \"aws_autoscaling_group\" \"example\" {\n  launch_configuration = aws_launch_configuration.example.name\n  vpc_zone_identifier  = data.aws_subnets.default.ids\n  target_group_arns    = [aws_lb_target_group.asg.arn]\n  health_check_type    = \"ELB\" \n \n  min_size = var.min_size\n  max_size = var.max_size \n \n  tag {\n    key                 = \"Name\"\n    value               = var.cluster_name\n    propagate_at_launch = true \n  } \n \n  dynamic \"tag\" {\n    for_each = var.custom_tags \n \n    content {\n      key                 = tag.key\n      value               = tag.value\n      propagate_at_launch = true \n    } \n  }\n}\n\nIf you run terraform plan now, you should see a plan that looks\nsomething like this:\n$ terraform plan \n \nTerraform will perform the following actions: \n \n  # aws_autoscaling_group.example will be updated in-place \n  ~ resource \"aws_autoscaling_group\" \"example\" { \n        (...) \n \n        tag { \n            key                 = \"Name\" \n            propagate_at_launch = true \n            value               = \"webservers-prod\" \n        } \n      + tag { \n          + key                 = \"Owner\" \n          + propagate_at_launch = true \n          + value               = \"team-foo\" \n        } \n      + tag { \n          + key                 = \"ManagedBy\" \n          + propagate_at_launch = true \n          + value               = \"terraform\" \n        } \n    } \n \nPlan: 0 to add, 1 to change, 0 to destroy.' metadata={'original_pages_range': '244-251', 'source': '069_Loops_with_for_each_Expressions', 'file_path': '/mnt/c/Users/Carlos/Documents/Github/jupiter-iaa-azure/Scripts/RAG/../../data/optimized_chunks/Libro-TF/069_Loops_with_for_each_Expressions.pdf', 'num_pages': 8}", "page_content='ENFORCING TAGGING STANDARDS\nIt’s typically a good idea to come up with a tagging standard for your\nteam and create Terraform modules that enforce this standard as code.\nOne way to do this is to manually ensure that every resource in every\nmodule sets the proper tags, but with many resources, this is tedious\nand error prone. If there are tags that you want to apply to all of your\nAWS resources, a more reliable approach is to add the\ndefault_tags block to the aws provider in every one of your\nmodules:\nprovider \"aws\" {\n  region = \"us-east-2\" \n \n  # Tags to apply to all AWS resources by default \n  default_tags {\n    tags = {\n      Owner     = \"team-foo\"\n      ManagedBy = \"Terraform\" \n    } \n  }\n}\nThe preceding code will ensure that every single AWS resource you\ncreate in this module will include the Owner and ManagedBy tags\n(the only exceptions are resources that don’t support tags and the\naws_autoscaling_group resource, which does support tags but\ndoesn’t work with default_tags, which is why you had to do all\nthat work in the previous section to set tags in the webserver-\ncluster module). default_tags gives you a way to ensure all\nresources have a common baseline of tags while still allowing you to\noverride those tags on a resource-by-resource basis. In Chapter 9, you’ll\nsee how to define and enforce policies as code such as “all resources\nmust have a ManagedBy tag” using tools such as OPA.\nLoops with for Expressions\n\nYou’ve now seen how to use loops to create multiple copies of entire\nresources and inline blocks, but what if you need a loop to set a single\nvariable or parameter?\nImagine that you wrote some Terraform code that took in a list of names:\nvariable \"names\" {\n  description = \"A list of names\"\n  type        = list(string)\n  default     = [\"neo\", \"trinity\", \"morpheus\"]\n}\nHow could you convert all of these names to uppercase? In a general-\npurpose programming language such as Python, you could write the\nfollowing for-loop:\nnames = [\"neo\", \"trinity\", \"morpheus\"] \n \nupper_case_names = []\nfor name in names: \n    upper_case_names.append(name.upper()) \n \nprint upper_case_names \n \n# Prints out: ['NEO', 'TRINITY', 'MORPHEUS']\nPython offers another way to write the exact same code in one line using a\nsyntax known as a list comprehension:\nnames = [\"neo\", \"trinity\", \"morpheus\"]\nupper_case_names = [name.upper() for name in names]\nprint upper_case_names \n \n# Prints out: ['NEO', 'TRINITY', 'MORPHEUS']\nPython also allows you to filter the resulting list by specifying a condition:\nnames = [\"neo\", \"trinity\", \"morpheus\"]\nshort_upper_case_names = [name.upper() for name in names if \nlen(name) < 5]\nprint short_upper_case_names\n\n# Prints out: ['NEO']\nTerraform offers similar functionality in the form of a for expression (not to\nbe confused with the for_each expression you saw in the previous\nsection). The basic syntax of a for expression is as follows:\n[for <ITEM> in <LIST> : <OUTPUT>]\nwhere LIST is a list to loop over, ITEM is the local variable name to assign\nto each item in LIST, and OUTPUT is an expression that transforms ITEM\nin some way. For example, here is the Terraform code to convert the list of\nnames in var.names to uppercase:\noutput \"upper_names\" {\n  value = [for name in var.names : upper(name)]\n}\nIf you run terraform apply on this code, you get the following output:\n$ terraform apply \n \nApply complete! Resources: 0 added, 0 changed, 0 destroyed. \n \nOutputs: \n \nupper_names = [ \n  \"NEO\", \n  \"TRINITY\", \n  \"MORPHEUS\", \n]\nJust as with Python’s list comprehensions, you can filter the resulting list by\nspecifying a condition:\noutput \"short_upper_names\" {\n  value = [for name in var.names : upper(name) if length(name) < \n5]\n}\n\nRunning terraform apply on this code gives you this:\nshort_upper_names = [ \n  \"NEO\", \n]\nTerraform’s for expression also allows you to loop over a map using the\nfollowing syntax:\n[for <KEY>, <VALUE> in <MAP> : <OUTPUT>]\nHere, MAP is a map to loop over, KEY and VALUE are the local variable\nnames to assign to each key-value pair in MAP, and OUTPUT is an\nexpression that transforms KEY and VALUE in some way. Here’s an\nexample:\nvariable \"hero_thousand_faces\" {\n  description = \"map\"\n  type        = map(string)\n  default     = {\n    neo      = \"hero\"\n    trinity  = \"love interest\"\n    morpheus = \"mentor\" \n  }\n} \n \noutput \"bios\" {\n  value = [for name, role in var.hero_thousand_faces : \"${name} \nis the ${role}\"]\n}\nWhen you run terraform apply on this code, you get the following:\nbios = [ \n  \"morpheus is the mentor\", \n  \"neo is the hero\", \n  \"trinity is the love interest\", \n]' metadata={'original_pages_range': '252-255', 'source': '070_Loops_with_for_Expressions', 'file_path': '/mnt/c/Users/Carlos/Documents/Github/jupiter-iaa-azure/Scripts/RAG/../../data/optimized_chunks/Libro-TF/070_Loops_with_for_Expressions.pdf', 'num_pages': 4}", "page_content='You can also use for expressions to output a map rather than a list using\nthe following syntax:\n# Loop over a list and output a map\n{for <ITEM> in <LIST> : <OUTPUT_KEY> => <OUTPUT_VALUE>} \n \n# Loop over a map and output a map\n{for <KEY>, <VALUE> in <MAP> : <OUTPUT_KEY> => <OUTPUT_VALUE>}\nThe only differences are that (a) you wrap the expression in curly braces\nrather than square brackets, and (b) rather than outputting a single value\neach iteration, you output a key and value, separated by an arrow. For\nexample, here is how you can transform a map to make all the keys and\nvalues uppercase:\noutput \"upper_roles\" {\n  value = {for name, role in var.hero_thousand_faces : \nupper(name) => upper(role)}\n}\nHere’s the output from running this code:\nupper_roles = { \n  \"MORPHEUS\" = \"MENTOR\" \n  \"NEO\" = \"HERO\" \n  \"TRINITY\" = \"LOVE INTEREST\" \n}\nLoops with the for String Directive\nEarlier in the book, you learned about string interpolations, which allow\nyou to reference Terraform code within strings:\n\"Hello, ${var.name}\"\nString directives allow you to use control statements (e.g., for-loops and if-\nstatements) within strings using a syntax similar to string interpolations, but\n\ninstead of a dollar sign and curly braces (${…}), you use a percent sign and\ncurly braces (%{…}).\nTerraform supports two types of string directives: for-loops and\nconditionals. In this section, we’ll go over for-loops; we’ll come back to\nconditionals later in the chapter. The for string directive uses the following\nsyntax:\n%{ for <ITEM> in <COLLECTION> }<BODY>%{ endfor }\nwhere COLLECTION is a list or map to loop over, ITEM is the local\nvariable name to assign to each item in COLLECTION, and BODY is what\nto render each iteration (which can reference ITEM). Here’s an example:\nvariable \"names\" {\n  description = \"Names to render\"\n  type        = list(string)\n  default     = [\"neo\", \"trinity\", \"morpheus\"]\n} \n \noutput \"for_directive\" {\n  value = \"%{ for name in var.names }${name}, %{ endfor }\"\n}\nWhen you run terraform apply, you get the following output:\n$ terraform apply \n \n(...) \n \nOutputs: \n \nfor_directive = \"neo, trinity, morpheus, \"\nThere’s also a version of the for string directive syntax that gives you the\nindex in the for-loop:\n%{ for <INDEX>, <ITEM> in <COLLECTION> }<BODY>%{ endfor }' metadata={'original_pages_range': '256-257', 'source': '071_Loops_with_the_for_String_Directive', 'file_path': '/mnt/c/Users/Carlos/Documents/Github/jupiter-iaa-azure/Scripts/RAG/../../data/optimized_chunks/Libro-TF/071_Loops_with_the_for_String_Directive.pdf', 'num_pages': 2}", "page_content='Here’s an example using the index:\noutput \"for_directive_index\" {\n  value = \"%{ for i, name in var.names }(${i}) ${name}, %{ endfor \n}\"\n}\nWhen you run terraform apply, you get the following output:\n$ terraform apply \n \n(...) \n \nOutputs: \n \nfor_directive_index = \"(0) neo, (1) trinity, (2) morpheus, \"\nNote how in both outputs there is an extra trailing comma and space. You\ncan fix this using conditionals—specifically, the if string directive—as\ndescribed in the next section.\nConditionals\nJust as Terraform offers several different ways to do loops, there are also\nseveral different ways to do conditionals, each intended to be used in a\nslightly different scenario:\ncount parameter\nUsed for conditional resources\nfor_each and for expressions\nUsed for conditional resources and inline blocks within a resource\nif string directive\nUsed for conditionals within a string' metadata={'original_pages_range': '258', 'source': '072_Conditionals', 'file_path': '/mnt/c/Users/Carlos/Documents/Github/jupiter-iaa-azure/Scripts/RAG/../../data/optimized_chunks/Libro-TF/072_Conditionals.pdf', 'num_pages': 1}", "page_content='Let’s go through these, one at a time.\nConditionals with the count Parameter\nThe count parameter you saw earlier lets you do a basic loop. If you’re\nclever, you can use the same mechanism to do a basic conditional. Let’s\nbegin by looking at if-statements in the next section and then move on to if-\nelse-statements in the section thereafter.\nIf-statements with the count parameter\nIn Chapter 4, you created a Terraform module that could be used as a\n“blueprint” for deploying web server clusters. The module created an Auto\nScaling Group (ASG), Application Load Balancer (ALB), security groups,\nand a number of other resources. One thing the module did not create was\nthe scheduled action. Because you want to scale the cluster out only in\nproduction, you defined the aws_autoscaling_schedule resources\ndirectly in the production configurations under\nlive/prod/services/webserver-cluster/main.tf. Is there a way you could\ndefine the aws_autoscaling_schedule resources in the\nwebserver-cluster module and conditionally create them for some\nusers of the module and not create them for others?\nLet’s give it a shot. The first step is to add a Boolean input variable in\nmodules/services/webserver-cluster/variables.tf that you can use to specify\nwhether the module should enable auto scaling:\nvariable \"enable_autoscaling\" {\n  description = \"If set to true, enable auto scaling\"\n  type        = bool\n}\nNow, if you had a general-purpose programming language, you could use\nthis input variable in an if-statement:\n# This is just pseudo code. It won't actually work in Terraform.\nif var.enable_autoscaling { \n  resource \"aws_autoscaling_schedule\"' metadata={'original_pages_range': '259', 'source': '073_Conditionals_with_the_count_Parameter', 'file_path': '/mnt/c/Users/Carlos/Documents/Github/jupiter-iaa-azure/Scripts/RAG/../../data/optimized_chunks/Libro-TF/073_Conditionals_with_the_count_Parameter.pdf', 'num_pages': 1}", "page_content='Let’s go through these, one at a time.\nConditionals with the count Parameter\nThe count parameter you saw earlier lets you do a basic loop. If you’re\nclever, you can use the same mechanism to do a basic conditional. Let’s\nbegin by looking at if-statements in the next section and then move on to if-\nelse-statements in the section thereafter.\nIf-statements with the count parameter\nIn Chapter 4, you created a Terraform module that could be used as a\n“blueprint” for deploying web server clusters. The module created an Auto\nScaling Group (ASG), Application Load Balancer (ALB), security groups,\nand a number of other resources. One thing the module did not create was\nthe scheduled action. Because you want to scale the cluster out only in\nproduction, you defined the aws_autoscaling_schedule resources\ndirectly in the production configurations under\nlive/prod/services/webserver-cluster/main.tf. Is there a way you could\ndefine the aws_autoscaling_schedule resources in the\nwebserver-cluster module and conditionally create them for some\nusers of the module and not create them for others?\nLet’s give it a shot. The first step is to add a Boolean input variable in\nmodules/services/webserver-cluster/variables.tf that you can use to specify\nwhether the module should enable auto scaling:\nvariable \"enable_autoscaling\" {\n  description = \"If set to true, enable auto scaling\"\n  type        = bool\n}\nNow, if you had a general-purpose programming language, you could use\nthis input variable in an if-statement:\n# This is just pseudo code. It won't actually work in Terraform.\nif var.enable_autoscaling { \n  resource \"aws_autoscaling_schedule\"\n\n\"scale_out_during_business_hours\" {\n    scheduled_action_name  = \"${var.cluster_name}-scale-out-\nduring-business-hours\"\n    min_size               = 2\n    max_size               = 10\n    desired_capacity       = 10\n    recurrence             = \"0 9 * * *\"\n    autoscaling_group_name = aws_autoscaling_group.example.name \n  } \n \n  resource \"aws_autoscaling_schedule\" \"scale_in_at_night\" {\n    scheduled_action_name  = \"${var.cluster_name}-scale-in-at-\nnight\"\n    min_size               = 2\n    max_size               = 10\n    desired_capacity       = 2\n    recurrence             = \"0 17 * * *\"\n    autoscaling_group_name = aws_autoscaling_group.example.name \n  }\n}\nTerraform doesn’t support if-statements, so this code won’t work. However,\nyou can accomplish the same thing by using the count parameter and\ntaking advantage of two properties:\nIf you set count to 1 on a resource, you get one copy of that resource;\nif you set count to 0, that resource is not created at all.\nTerraform supports conditional expressions of the format\n<CONDITION> ? <TRUE_VAL> : <FALSE_VAL>. This ternary\nsyntax, which may be familiar to you from other programming\nlanguages, will evaluate the Boolean logic in CONDITION, and if the\nresult is true, it will return TRUE_VAL, and if the result is false,\nit’ll return FALSE_VAL.\nPutting these two ideas together, you can update the webserver-\ncluster module as follows:\nresource \"aws_autoscaling_schedule\" \n\"scale_out_during_business_hours\" {\n  count = var.enable_autoscaling ? 1 : 0\n\nscheduled_action_name  = \"${var.cluster_name}-scale-out-during-\nbusiness-hours\"\n  min_size               = 2\n  max_size               = 10\n  desired_capacity       = 10\n  recurrence             = \"0 9 * * *\"\n  autoscaling_group_name = aws_autoscaling_group.example.name\n} \n \nresource \"aws_autoscaling_schedule\" \"scale_in_at_night\" {\n  count = var.enable_autoscaling ? 1 : 0 \n \n  scheduled_action_name  = \"${var.cluster_name}-scale-in-at-\nnight\"\n  min_size               = 2\n  max_size               = 10\n  desired_capacity       = 2\n  recurrence             = \"0 17 * * *\"\n  autoscaling_group_name = aws_autoscaling_group.example.name\n}\nIf var.enable_autoscaling is true, the count parameter for each\nof the aws_autoscaling_schedule resources will be set to 1, so one\nof each will be created. If var.enable_autoscaling is false, the\ncount parameter for each of the aws_autoscaling_schedule\nresources will be set to 0, so neither one will be created. This is exactly the\nconditional logic you want!\nYou can now update the usage of this module in staging (in\nlive/stage/services/webserver-cluster/main.tf) to disable auto scaling by\nsetting enable_autoscaling to false:\nmodule \"webserver_cluster\" {\n  source = \"../../../../modules/services/webserver-cluster\" \n \n  cluster_name           = \"webservers-stage\"\n  db_remote_state_bucket = \"(YOUR_BUCKET_NAME)\"\n  db_remote_state_key    = \"stage/data-\nstores/mysql/terraform.tfstate\" \n \n  instance_type        = \"t2.micro\"\n  min_size             = 2\n  max_size             = 2' metadata={'original_pages_range': '259-261', 'source': '074_If-statements_with_the_count_parameter', 'file_path': '/mnt/c/Users/Carlos/Documents/Github/jupiter-iaa-azure/Scripts/RAG/../../data/optimized_chunks/Libro-TF/074_If-statements_with_the_count_parameter.pdf', 'num_pages': 3}", "page_content='enable_autoscaling   = false\n}\nSimilarly, you can update the usage of this module in production (in\nlive/prod/services/webserver-cluster/main.tf) to enable auto scaling by\nsetting enable_autoscaling to true (make sure to also remove the\ncustom aws_autoscaling_schedule resources that were in the\nproduction environment from Chapter 4):\nmodule \"webserver_cluster\" {\n  source = \"../../../../modules/services/webserver-cluster\" \n \n  cluster_name           = \"webservers-prod\"\n  db_remote_state_bucket = \"(YOUR_BUCKET_NAME)\"\n  db_remote_state_key    = \"prod/data-\nstores/mysql/terraform.tfstate\" \n \n  instance_type        = \"m4.large\"\n  min_size             = 2\n  max_size             = 10\n  enable_autoscaling   = true \n \n  custom_tags = {\n    Owner     = \"team-foo\"\n    ManagedBy = \"terraform\" \n  }\n}\nIf-else-statements with the count parameter\nNow that you know how to do an if-statement, what about an if-else-\nstatement?\nEarlier in this chapter, you created several IAM users with read-only access\nto EC2. Imagine that you wanted to give one of these users, neo, access to\nCloudWatch as well but allow the person applying the Terraform\nconfigurations to decide whether neo is assigned only read access or both\nread and write access. This is a slightly contrived example, but a useful one\nto demonstrate a simple type of if-else-statement.\nHere is an IAM Policy that allows read-only access to CloudWatch:\n\nresource \"aws_iam_policy\" \"cloudwatch_read_only\" {\n  name   = \"cloudwatch-read-only\"\n  policy = data.aws_iam_policy_document.cloudwatch_read_only.json\n} \n \ndata \"aws_iam_policy_document\" \"cloudwatch_read_only\" { \n  statement {\n    effect    = \"Allow\"\n    actions   = [ \n      \"cloudwatch:Describe*\", \n      \"cloudwatch:Get*\", \n      \"cloudwatch:List*\" \n    ]\n    resources = [\"*\"] \n  }\n}\nAnd here is an IAM Policy that allows full (read and write) access to\nCloudWatch:\nresource \"aws_iam_policy\" \"cloudwatch_full_access\" {\n  name   = \"cloudwatch-full-access\"\n  policy = \ndata.aws_iam_policy_document.cloudwatch_full_access.json\n} \n \ndata \"aws_iam_policy_document\" \"cloudwatch_full_access\" { \n  statement {\n    effect    = \"Allow\"\n    actions   = [\"cloudwatch:*\"]\n    resources = [\"*\"] \n  }\n}\nThe goal is to attach one of these IAM Policies to \"neo\", based on the\nvalue of a new input variable called\ngive_neo_cloudwatch_full_access:\nvariable \"give_neo_cloudwatch_full_access\" {\n  description = \"If true, neo gets full access to CloudWatch\"\n  type        = bool\n}\n\nIf you were using a general-purpose programming language, you might\nwrite an if-else-statement that looks like this:\n# This is just pseudo code. It won't actually work in Terraform.\nif var.give_neo_cloudwatch_full_access { \n  resource \"aws_iam_user_policy_attachment\" \n\"neo_cloudwatch_full_access\" {\n    user       = aws_iam_user.example[0].name\n    policy_arn = aws_iam_policy.cloudwatch_full_access.arn \n  }\n} else { \n  resource \"aws_iam_user_policy_attachment\" \n\"neo_cloudwatch_read_only\" {\n    user       = aws_iam_user.example[0].name\n    policy_arn = aws_iam_policy.cloudwatch_read_only.arn \n  }\n}\nTo do this in Terraform, you can use the count parameter and a\nconditional expression on each of the resources:\nresource \"aws_iam_user_policy_attachment\" \n\"neo_cloudwatch_full_access\" {\n  count = var.give_neo_cloudwatch_full_access ? 1 : 0 \n \n  user       = aws_iam_user.example[0].name\n  policy_arn = aws_iam_policy.cloudwatch_full_access.arn\n} \n \nresource \"aws_iam_user_policy_attachment\" \n\"neo_cloudwatch_read_only\" {\n  count = var.give_neo_cloudwatch_full_access ? 0 : 1 \n \n  user       = aws_iam_user.example[0].name\n  policy_arn = aws_iam_policy.cloudwatch_read_only.arn\n}\nThis code contains two aws_iam_user_policy_attachment\nresources. The first one, which attaches the CloudWatch full access\npermissions, has a conditional expression that will evaluate to 1 if\nvar.give_neo_cloudwatch_full_access is true, and 0\notherwise (this is the if-clause). The second one, which attaches the\n\nCloudWatch read-only permissions, has a conditional expression that does\nthe exact opposite, evaluating to 0 if\nvar.give_neo_cloudwatch_full_access is true, and 1\notherwise (this is the else-clause). And there you are—you now know how\nto do if-else-statements!\nNow that you have the ability to create one resource or the other based on\nan if/else condition, what do you do if you need to access an attribute on the\nresource that actually got created? For example, what if you wanted to add\nan output variable called neo_cloudwatch_policy_arn, which\ncontains the ARN of the policy you actually attached?\nThe simplest option is to use ternary syntax:\noutput \"neo_cloudwatch_policy_arn\" {\n  value = ( \n    var.give_neo_cloudwatch_full_access \n    ? \naws_iam_user_policy_attachment.neo_cloudwatch_full_access[0].poli\ncy_arn \n    : \naws_iam_user_policy_attachment.neo_cloudwatch_read_only[0].policy\n_arn \n  )\n}\nThis will work fine for now, but this code is a bit brittle: if you ever change\nthe conditional in the count parameter of the\naws_iam_user_policy_attachment resources—perhaps in the\nfuture, it’ll depend on multiple variables and not solely on\nvar.give_neo_cloudwatch_full_access—there’s a risk that\nyou’ll forget to update the conditional in this output variable, and as a\nresult, you’ll get a very confusing error when trying to access an array\nelement that might not exist.\nA safer approach is to take advantage of the concat and one functions.\nThe concat function takes two or more lists as inputs and combines them\ninto a single list. The one function takes a list as input and if the list has 0\nelements, it returns null; if the list has 1 element, it returns that element;' metadata={'original_pages_range': '262-265', 'source': '075_If-else-statements_with_the_count_parameter', 'file_path': '/mnt/c/Users/Carlos/Documents/Github/jupiter-iaa-azure/Scripts/RAG/../../data/optimized_chunks/Libro-TF/075_If-else-statements_with_the_count_parameter.pdf', 'num_pages': 4}", "page_content='and if the list has more than 1 element, it shows an error. Putting these two\ntogether, and combining them with a splat expression, you get the\nfollowing:\noutput \"neo_cloudwatch_policy_arn\" {\n  value = one(concat( \n    \naws_iam_user_policy_attachment.neo_cloudwatch_full_access[*].poli\ncy_arn, \n    \naws_iam_user_policy_attachment.neo_cloudwatch_read_only[*].policy\n_arn \n  ))\n}\nDepending on the outcome of the if/else conditional, either\nneo_cloudwatch_full_access will be empty and\nneo_cloudwatch_read_only will contain one element or vice versa,\nso once you concatenate them together, you’ll have a list with one element,\nand the one function will return that element. This will continue to work\ncorrectly no matter how you change your if/else conditional.\nUsing count and built-in functions to simulate if-else-statements is a bit of\na hack, but it’s one that works fairly well, and as you can see from the code,\nit allows you to conceal lots of complexity from your users so that they get\nto work with a clean and simple API.\nConditionals with for_each and for Expressions\nNow that you understand how to do conditional logic with resources using\nthe count parameter, you can probably guess that you can use a similar\nstrategy to do conditional logic by using a for_each expression.\nIf you pass a for_each expression an empty collection, the result will be\nzero copies of the resource, inline block, or module where you have the\nfor_each; if you pass it a nonempty collection, it will create one or more\ncopies of the resource, inline block, or module. The only question is, how\ndo you conditionally decide if the collection should be empty or not?\n\nThe answer is to combine the for_each expression with the for\nexpression. For example, recall the way the webserver-cluster\nmodule in modules/services/webserver-cluster/main.tf sets tags:\n  dynamic \"tag\" {\n    for_each = var.custom_tags \n \n    content {\n      key                 = tag.key\n      value               = tag.value\n      propagate_at_launch = true \n    } \n  }\nIf var.custom_tags is empty, the for_each expression will have\nnothing to loop over, so no tags will be set. In other words, you already\nhave some conditional logic here. But you can go even further, by\ncombining the for_each expression with a for expression as follows:\n  dynamic \"tag\" {\n    for_each = { \n      for key, value in var.custom_tags:\n      key => upper(value)\n      if key != \"Name\" \n    } \n \n    content {\n      key                 = tag.key\n      value               = tag.value\n      propagate_at_launch = true \n    } \n  }\nThe nested for expression loops over var.custom_tags, converts\neach value to uppercase (perhaps for consistency), and uses a conditional in\nthe for expression to filter out any key set to Name because the module\nalready sets its own Name tag. By filtering values in the for expression,\nyou can implement arbitrary conditional logic.' metadata={'original_pages_range': '266-267', 'source': '076_Conditionals_with_for_each_and_for_Expressions', 'file_path': '/mnt/c/Users/Carlos/Documents/Github/jupiter-iaa-azure/Scripts/RAG/../../data/optimized_chunks/Libro-TF/076_Conditionals_with_for_each_and_for_Expressions.pdf', 'num_pages': 2}", "page_content='Note that even though you should almost always prefer for_each over\ncount for creating multiple copies of a resource or module, when it comes\nto conditional logic, setting count to 0 or 1 tends to be simpler than setting\nfor_each to an empty or nonempty collection. Therefore, I typically\nrecommend using count to conditionally create resources and modules,\nand using for_each for all other types of loops and conditionals.\nConditionals with the if String Directive\nLet’s now look at the if string directive, which has the following syntax:\n%{ if <CONDITION> }<TRUEVAL>%{ endif }\nwhere CONDITION is any expression that evaluates to a boolean and\nTRUEVAL is the expression to render if CONDITION evaluates to true.\nEarlier in the chapter, you used the for string directive to do loops within a\nstring to output several comma-separated names. The problem was that\nthere was an extra trailing comma and space at the end of the string. You\ncan use the if string directive to fix this issue as follows:\noutput \"for_directive_index_if\" {\n  value = <<EOF\n%{ for i, name in var.names } \n  ${name}%{ if i < length(var.names) - 1 }, %{ endif }\n%{ endfor }\nEOF\n}\nThere are a few changes here from the original version:\nI put the code in a HEREDOC, which is a way to define multiline\nstrings. This allows me to spread the code out across several lines so it\nis more readable.\nI used the if string directive to not output the comma and space for\nthe last item in the list.\n\nWhen you run terraform apply, you get the following output:\n$ terraform apply \n \n(...) \n \nOutputs: \n \nfor_directive_index_if = <<EOT \n \n  neo, \n \n  trinity, \n \n  morpheus \n \n \nEOT\nWhoops. The trailing comma is gone, but we’ve introduced a bunch of\nextra whitespace (spaces and newlines). Every whitespace you put in a\nHEREDOC ends up in the final string. You can fix this by adding strip\nmarkers (~) to your string directives, which will eat up the extra whitespace\nbefore or after the strip marker:\noutput \"for_directive_index_if_strip\" {\n  value = <<EOF\n%{~ for i, name in var.names ~}\n${name}%{ if i < length(var.names) - 1 }, %{ endif }\n%{~ endfor ~}\nEOF\n}\nLet’s give this version a try:\n$ terraform apply \n \n(...) \n \nOutputs: \n \nfor_directive_index_if_strip = \"neo, trinity, morpheus\"' metadata={'original_pages_range': '268-269', 'source': '077_Conditionals_with_the_if_String_Directive', 'file_path': '/mnt/c/Users/Carlos/Documents/Github/jupiter-iaa-azure/Scripts/RAG/../../data/optimized_chunks/Libro-TF/077_Conditionals_with_the_if_String_Directive.pdf', 'num_pages': 2}", "page_content='OK, that’s a nice improvement: no extra whitespace or commas. You can\nmake this output even prettier by adding an else to the string directive,\nwhich uses the following syntax:\n%{ if <CONDITION> }<TRUEVAL>%{ else }<FALSEVAL>%{ endif }\nwhere FALSEVAL is the expression to render if CONDITION evaluates to\nfalse. Here’s an example of how to use the else clause to add a period at\nthe end:\noutput \"for_directive_index_if_else_strip\" {\n  value = <<EOF\n%{~ for i, name in var.names ~}\n${name}%{ if i < length(var.names) - 1 }, %{ else }.%{ endif }\n%{~ endfor ~}\nEOF\n}\nWhen you run terraform apply, you get the following output:\n$ terraform apply \n \n(...) \n \nOutputs: \n \nfor_directive_index_if_else_strip = \"neo, trinity, morpheus.\"\nZero-Downtime Deployment\nNow that your module has a clean and simple API for deploying a web\nserver cluster, an important question to ask is, how do you update that\ncluster? That is, when you make changes to your code, how do you deploy\na new Amazon Machine Image (AMI) across the cluster? And how do you\ndo it without causing downtime for your users?\nThe first step is to expose the AMI as an input variable in\nmodules/services/webserver-cluster/variables.tf. In real-world examples,\n\nthis is all you would need because the actual web server code would be\ndefined in the AMI. However, in the simplified examples in this book, all of\nthe web server code is actually in the User Data script, and the AMI is just a\nvanilla Ubuntu image. Switching to a different version of Ubuntu won’t\nmake for much of a demonstration, so in addition to the new AMI input\nvariable, you can also add an input variable to control the text the User Data\nscript returns from its one-liner HTTP server:\nvariable \"ami\" {\n  description = \"The AMI to run in the cluster\"\n  type        = string\n  default     = \"ami-0fb653ca2d3203ac1\"\n} \n \nvariable \"server_text\" {\n  description = \"The text the web server should return\"\n  type        = string\n  default     = \"Hello, World\"\n}\nNow you need to update the modules/services/webserver-cluster/user-\ndata.sh Bash script to use this server_text variable in the <h1> tag it\nreturns:\n#!/bin/bash \n \ncat > index.xhtml <<EOF\n<h1>${server_text}</h1>\n<p>DB address: ${db_address}</p>\n<p>DB port: ${db_port}</p>\nEOF \n \nnohup busybox httpd -f -p ${server_port} &\nFinally, find the launch configuration in modules/services/webserver-\ncluster/main.tf, update the image_id parameter to use var.ami, and\nupdate the templatefile call in the user_data parameter to pass in\nvar.server_text:\n\nresource \"aws_launch_configuration\" \"example\" {\n  image_id        = var.ami\n  instance_type   = var.instance_type\n  security_groups = [aws_security_group.instance.id] \n \n  user_data       = templatefile(\"${path.module}/user-data.sh\", {\n    server_port = var.server_port\n    db_address  = data.terraform_remote_state.db.outputs.address\n    db_port     = data.terraform_remote_state.db.outputs.port\n    server_text = var.server_text \n  }) \n \n  # Required when using a launch configuration with an auto \nscaling group. \n  lifecycle {\n    create_before_destroy = true \n  }\n}\nNow, in the staging environment, in live/stage/services/webserver-\ncluster/main.tf, you can set the new ami and server_text parameters:\nmodule \"webserver_cluster\" {\n  source = \"../../../../modules/services/webserver-cluster\" \n \n  ami         = \"ami-0fb653ca2d3203ac1\"\n  server_text = \"New server text\" \n \n  cluster_name           = \"webservers-stage\"\n  db_remote_state_bucket = \"(YOUR_BUCKET_NAME)\"\n  db_remote_state_key    = \"stage/data-\nstores/mysql/terraform.tfstate\" \n \n  instance_type      = \"t2.micro\"\n  min_size           = 2\n  max_size           = 2\n  enable_autoscaling = false\n}\nThis code uses the same Ubuntu AMI, but changes the server_text to a\nnew value. If you run the plan command, you should see something like\nthe following:\n\nTerraform will perform the following actions: \n \n  # module.webserver_cluster.aws_autoscaling_group.ex will be \nupdated in-place \n  ~ resource \"aws_autoscaling_group\" \"example\" { \n        id                        = \"webservers-stage-terraform-\n20190516\" \n      ~ launch_configuration      = \"terraform-20190516\" -> \n(known after apply) \n        (...) \n    } \n \n  # module.webserver_cluster.aws_launch_configuration.ex must be \nreplaced \n+/- resource \"aws_launch_configuration\" \"example\" { \n      ~ id                          = \"terraform-20190516\" -> \n(known after apply) \n        image_id                    = \"ami-0fb653ca2d3203ac1\" \n        instance_type               = \"t2.micro\" \n      ~ name                        = \"terraform-20190516\" -> \n(known after apply) \n      ~ user_data                   = \"bd7c0a6\" -> \"4919a13\" # \nforces replacement \n        (...) \n    } \n \nPlan: 1 to add, 1 to change, 1 to destroy.\nAs you can see, Terraform wants to make two changes: first, replace the old\nlaunch configuration with a new one that has the updated user_data; and\nsecond, modify the Auto Scaling Group in place to reference the new\nlaunch configuration. There is a problem here: merely referencing the new\nlaunch configuration will have no effect until the ASG launches new EC2\nInstances. So how do you instruct the ASG to deploy new Instances?\nOne option is to destroy the ASG (e.g., by running terraform\ndestroy) and then re-create it (e.g., by running terraform apply).\nThe problem is that after you delete the old ASG, your users will\nexperience downtime until the new ASG comes up. What you want to do\ninstead is a zero-downtime deployment. The way to accomplish that is to\ncreate the replacement ASG first and then destroy the original one. As it\n\nturns out, the create_before_destroy lifecycle setting you first saw\nin Chapter 2 does exactly this.\nHere’s how you can take advantage of this lifecycle setting to get a zero-\ndowntime deployment:\n1. Configure the name parameter of the ASG to depend directly on the\nname of the launch configuration. Each time the launch configuration\nchanges (which it will when you update the AMI or User Data), its\nname changes, and therefore the ASG’s name will change, which\nforces Terraform to replace the ASG.\n2. Set the create_before_destroy parameter of the ASG to true\nso that each time Terraform tries to replace it, it will create the\nreplacement ASG before destroying the original.\n3. Set the min_elb_capacity parameter of the ASG to the\nmin_size of the cluster so that Terraform will wait for at least that\nmany servers from the new ASG to pass health checks in the ALB\nbefore it will begin destroying the original ASG.\nHere is what the updated aws_autoscaling_group resource should\nlook like in modules/services/webserver-cluster/main.tf:\nresource \"aws_autoscaling_group\" \"example\" {\n  # Explicitly depend on the launch configuration's name so each \ntime it's\n  # replaced, this ASG is also replaced\n  name = \n\"${var.cluster_name}-${aws_launch_configuration.example.name}\" \n \n  launch_configuration = aws_launch_configuration.example.name\n  vpc_zone_identifier  = data.aws_subnets.default.ids\n  target_group_arns    = [aws_lb_target_group.asg.arn]\n  health_check_type    = \"ELB\" \n \n  min_size = var.min_size\n  max_size = var.max_size \n \n  # Wait for at least this many instances to pass health checks \nbefore\n1\n\n# considering the ASG deployment complete\n  min_elb_capacity = var.min_size \n \n  # When replacing this ASG, create the replacement first, and \nonly delete the\n  # original after \n  lifecycle {\n    create_before_destroy = true \n  } \n \n  tag {\n    key                 = \"Name\"\n    value               = var.cluster_name\n    propagate_at_launch = true \n  } \n \n  dynamic \"tag\" {\n    for_each = { \n      for key, value in var.custom_tags:\n      key => upper(value)\n      if key != \"Name\" \n    } \n \n    content {\n      key                 = tag.key\n      value               = tag.value\n      propagate_at_launch = true \n    } \n  }\n}\nIf you rerun the plan command, you’ll now see something that looks like\nthe following:\nTerraform will perform the following actions: \n \n  # module.webserver_cluster.aws_autoscaling_group.example must \nbe replaced \n+/- resource \"aws_autoscaling_group\" \"example\" { \n      ~ id     = \"example-2019\" -> (known after apply) \n      ~ name   = \"example-2019\" -> (known after apply) # forces \nreplacement \n        (...) \n    } \n \n  # module.webserver_cluster.aws_launch_configuration.example\n\nmust be replaced \n+/- resource \"aws_launch_configuration\" \"example\" { \n      ~ id              = \"terraform-2019\" -> (known after apply) \n        image_id        = \"ami-0fb653ca2d3203ac1\" \n        instance_type   = \"t2.micro\" \n      ~ name            = \"terraform-2019\" -> (known after apply) \n      ~ user_data       = \"bd7c0a\" -> \"4919a\" # forces \nreplacement \n        (...) \n    } \n \n    (...) \n \nPlan: 2 to add, 2 to change, 2 to destroy.\nThe key thing to notice is that the aws_autoscaling_group resource\nnow says forces replacement next to its name parameter, which\nmeans that Terraform will replace it with a new ASG running your new\nAMI or User Data. Run the apply command to kick off the deployment,\nand while it runs, consider how the process works.\nYou start with your original ASG running, say, v1 of your code (Figure 5-\n1).\n\n\n\nFigure 5-1. Initially, you have the original ASG running v1 of your code.\nYou make an update to some aspect of the launch configuration, such as\nswitching to an AMI that contains v2 of your code, and run the apply\ncommand. This forces Terraform to begin deploying a new ASG with v2 of\nyour code (Figure 5-2).\n\n\n\nFigure 5-2. Terraform begins deploying the new ASG with v2 of your code.\nAfter a minute or two, the servers in the new ASG have booted, connected\nto the database, registered in the ALB, and started to pass health checks. At\nthis point, both the v1 and v2 versions of your app will be running\nsimultaneously; and which one users see depends on where the ALB\nhappens to route them (Figure 5-3).\n\n\n\nFigure 5-3. The servers in the new ASG boot up, connect to the DB, register in the ALB, and begin\nserving traffic.\nAfter min_elb_capacity servers from the v2 ASG cluster have\nregistered in the ALB, Terraform will begin to undeploy the old ASG, first\nby deregistering the servers in that ASG from the ALB, and then by\nshutting them down (Figure 5-4).\n\n\n\nFigure 5-4. The servers in the old ASG begin to shut down.\nAfter a minute or two, the old ASG will be gone, and you will be left with\njust v2 of your app running in the new ASG (Figure 5-5).\n\n\n\nFigure 5-5. Now, only the new ASG remains, which is running v2 of your code.\nDuring this entire process, there are always servers running and handling\nrequests from the ALB, so there is no downtime. Open the ALB URL in\nyour browser, and you should see something like Figure 5-6.\nFigure 5-6. The new code is now deployed.\nSuccess! The new server text has deployed. As a fun experiment, make\nanother change to the server_text parameter—for example, update it to\nsay “foo bar”—and run the apply command. In a separate terminal tab, if\nyou’re on Linux/Unix/macOS, you can use a Bash one-liner to run curl in\na loop, hitting your ALB once per second and allowing you to see the zero-\ndowntime deployment in action:\n$ while true; do curl http://<load_balancer_url>; sleep 1; done\nFor the first minute or so, you should see the same response: New server\ntext. Then, you’ll begin seeing it alternate between New server text\nand foo bar. This means the new Instances have registered in the ALB\nand passed health checks. After another minute, the New server text\nmessage will disappear, and you’ll see only foo bar, which means the' metadata={'original_pages_range': '270-286', 'source': '078_Zero-Downtime_Deployment', 'file_path': '/mnt/c/Users/Carlos/Documents/Github/jupiter-iaa-azure/Scripts/RAG/../../data/optimized_chunks/Libro-TF/078_Zero-Downtime_Deployment.pdf', 'num_pages': 17}", "page_content='old ASG has been shut down. The output will look something like this (for\nclarity, I’m listing only the contents of the <h1> tags):\nNew server text \nNew server text \nNew server text \nNew server text \nNew server text \nNew server text \nfoo bar \nNew server text \nfoo bar \nNew server text \nfoo bar \nNew server text \nfoo bar \nNew server text \nfoo bar \nNew server text \nfoo bar \nfoo bar \nfoo bar \nfoo bar \nfoo bar \nfoo bar\nAs an added bonus, if something went wrong during the deployment,\nTerraform will automatically roll back. For example, if there were a bug in\nv2 of your app and it failed to boot, the Instances in the new ASG will not\nregister with the ALB. Terraform will wait up to\nwait_for_capacity_timeout (default is 10 minutes) for\nmin_elb_capacity servers of the v2 ASG to register in the ALB, after\nwhich it considers the deployment a failure, deletes the v2 ASG, and exits\nwith an error (meanwhile, v1 of your app continues to run just fine in the\noriginal ASG).\nTerraform Gotchas\nAfter going through all these tips and tricks, it’s worth taking a step back\nand pointing out a few gotchas, including those related to the loop, if-' metadata={'original_pages_range': '287', 'source': '079_Terraform_Gotchas', 'file_path': '/mnt/c/Users/Carlos/Documents/Github/jupiter-iaa-azure/Scripts/RAG/../../data/optimized_chunks/Libro-TF/079_Terraform_Gotchas.pdf', 'num_pages': 1}", "page_content='statement, and deployment techniques, as well as those related to more\ngeneral problems that affect Terraform as a whole:\ncount and for_each have limitations.\nZero-downtime deployment has limitations.\nValid plans can fail.\nRefactoring can be tricky.\ncount and for_each Have Limitations\nIn the examples in this chapter, you made extensive use of the count\nparameter and for_each expressions in loops and if-statements. This\nworks well, but there’s an important limitation that you need to be aware of:\nyou cannot reference any resource outputs in count or for_each.\nImagine that you want to deploy multiple EC2 Instances, and for some\nreason you didn’t want to use an ASG. The code might look like this:\nresource \"aws_instance\" \"example_1\" {\n  count         = 3\n  ami           = \"ami-0fb653ca2d3203ac1\"\n  instance_type = \"t2.micro\"\n}\nBecause count is being set to a hardcoded value, this code will work\nwithout issues, and when you run apply, it will create three EC2\nInstances. Now, what if you want to deploy one EC2 Instance per\nAvailability Zone (AZ) in the current AWS region? You could update your\ncode to fetch the list of AZs using the aws_availability_zones data\nsource and use the count parameter and array lookups to “loop” over each\nAZ and create an EC2 Instance in it:\nresource \"aws_instance\" \"example_2\" {\n  count             = \nlength(data.aws_availability_zones.all.names)\n  availability_zone =\n\ndata.aws_availability_zones.all.names[count.index]\n  ami               = \"ami-0fb653ca2d3203ac1\"\n  instance_type     = \"t2.micro\"\n} \n \ndata \"aws_availability_zones\" \"all\" {}\nAgain, this code works just fine, since count can reference data sources\nwithout problems. However, what happens if the number of instances you\nneed to create depends on the output of some resource? The easiest way to\nexperiment with this is to use the random_integer resource, which, as\nyou can probably guess from the name, returns a random integer:\nresource \"random_integer\" \"num_instances\" {\n  min = 1\n  max = 3\n}\nThis code generates a random integer between 1 and 3. Let’s see what\nhappens if you try to use the result output from this resource in the\ncount parameter of your aws_instance resource:\nresource \"aws_instance\" \"example_3\" {\n  count         = random_integer.num_instances.result\n  ami           = \"ami-0fb653ca2d3203ac1\"\n  instance_type = \"t2.micro\"\n}\nIf you run terraform plan on this code, you’ll get the following error:\nError: Invalid count argument \n \n  on main.tf line 30, in resource \"aws_instance\" \"example_3\": \n  30:   count         = random_integer.num_instances.result \n \nThe \"count\" value depends on resource attributes that cannot be \ndetermined \nuntil apply, so Terraform cannot predict how many instances will \nbe created. \nTo work around this, use the -target argument to first apply only' metadata={'original_pages_range': '288-289', 'source': '080_count_and_for_each_Have_Limitations', 'file_path': '/mnt/c/Users/Carlos/Documents/Github/jupiter-iaa-azure/Scripts/RAG/../../data/optimized_chunks/Libro-TF/080_count_and_for_each_Have_Limitations.pdf', 'num_pages': 2}", "page_content='the \nresources that the count depends on.\nTerraform requires that it can compute count and for_each during the\nplan phase, before any resources are created or modified. This means that\ncount and for_each can reference hardcoded values, variables, data\nsources, and even lists of resources (so long as the length of the list can be\ndetermined during plan), but not computed resource outputs.\nZero-Downtime Deployment Has Limitations\nThere are a couple of gotchas with using create_before_destroy\nwith an ASG to do zero-downtime deployment.\nThe first issue is that it doesn’t work with auto scaling policies. Or, to be\nmore accurate, it resets your ASG size back to its min_size after each\ndeployment, which can be a problem if you had used auto scaling policies\nto increase the number of running servers. For example, the webserver-\ncluster module includes a couple of aws_autoscaling_schedule\nresources that increase the number of servers in the cluster from 2 to 10 at 9\na.m. If you ran a deployment at, say, 11 a.m., the replacement ASG would\nboot up with only 2 servers, rather than 10, and it would stay that way until\n9 a.m. the next day. There are several possible workarounds, such as\ntweaking the recurrence parameter on the\naws_autoscaling_schedule or setting the desired_capacity\nparameter of the ASG to get its value from a custom script that uses the\nAWS API to figure out how many instances were running before\ndeployment.\nHowever, the second, and bigger, issue is that, for important and\ncomplicated tasks like a zero-downtime deployment, you really want to use\nnative, first-class solutions, and not workarounds that require you to\nhaphazardly glue together create_before_destroy,\nmin_elb_capacity, custom scripts, etc. As it turns out, for Auto\nScaling Groups, AWS now offers a native solution called instance refresh.\n\nGo back to your aws_autoscaling_group resource and undo the\nzero-downtime deployment changes:\nSet name back to var.cluster_name, instead of having it depend\non the aws_launch_configuration name.\nRemove the create_before_destroy and\nmin_elb_capacity settings.\nAnd now, update the aws_autoscaling_group resource to instead use\nan instance_refresh block as follows:\nresource \"aws_autoscaling_group\" \"example\" {\n  name                 = var.cluster_name\n  launch_configuration = aws_launch_configuration.example.name\n  vpc_zone_identifier  = data.aws_subnets.default.ids\n  target_group_arns    = [aws_lb_target_group.asg.arn]\n  health_check_type    = \"ELB\" \n \n  min_size = var.min_size\n  max_size = var.max_size \n \n  # Use instance refresh to roll out changes to the ASG \n  instance_refresh {\n    strategy = \"Rolling\" \n    preferences {\n      min_healthy_percentage = 50 \n    } \n  }\n}\nIf you deploy this ASG, and then later change some parameter (e.g., change\nserver_text) and run plan, the diff will be back to just updating the\naws_launch_configuration:\nTerraform will perform the following actions: \n \n  # module.webserver_cluster.aws_autoscaling_group.ex will be \nupdated in-place \n  ~ resource \"aws_autoscaling_group\" \"example\" { \n        id                        = \"webservers-stage-terraform-\n20190516\"\n\n~ launch_configuration      = \"terraform-20190516\" -> \n(known after apply) \n        (...) \n    } \n \n  # module.webserver_cluster.aws_launch_configuration.ex must be \nreplaced \n+/- resource \"aws_launch_configuration\" \"example\" { \n      ~ id                          = \"terraform-20190516\" -> \n(known after apply) \n        image_id                    = \"ami-0fb653ca2d3203ac1\" \n        instance_type               = \"t2.micro\" \n      ~ name                        = \"terraform-20190516\" -> \n(known after apply) \n      ~ user_data                   = \"bd7c0a6\" -> \"4919a13\" # \nforces replacement \n        (...) \n    } \n \nPlan: 1 to add, 1 to change, 1 to destroy.\nIf you run apply, it’ll complete very quickly, and at first, nothing new will\nbe deployed. However, in the background, because you modified the launch\nconfiguration, AWS will kick off the instance refresh process, as shown in\nFigure 5-7.\n\nFigure 5-7. An instance refresh is in progress.\nAWS will initially launch one new instance, wait for it to pass health\nchecks, shut down one of the older instances, and then repeat the process\nwith the second instance, until the instance refresh is completed, as shown\nin Figure 5-8.\n\nFigure 5-8. An instance refresh is completed.\nThis process is entirely managed by AWS, is reasonably configurable,\nhandles errors pretty well, and requires no workarounds. The only\ndrawback is the process can sometimes be slow (taking up to 20 minutes to\nreplace just two servers), but other than that, it’s a much more robust\nsolution to use for most zero-downtime deployments.' metadata={'original_pages_range': '290-294', 'source': '081_Zero-Downtime_Deployment_Has_Limitations', 'file_path': '/mnt/c/Users/Carlos/Documents/Github/jupiter-iaa-azure/Scripts/RAG/../../data/optimized_chunks/Libro-TF/081_Zero-Downtime_Deployment_Has_Limitations.pdf', 'num_pages': 5}", "page_content='In general, you should prefer to use first-class, native deployment options\nlike instance refresh whenever possible. Although such options weren’t\nalways available in the earlier days of Terraform, these days, many\nresources support native deployment options. For example, if you’re using\nAmazon Elastic Container Service (ECS) to deploy Docker containers, the\naws_ecs_service resource natively supports zero-downtime\ndeployments via the deployment_maximum_percent and\ndeployment _minimum_healthy_percent parameters; if you’re\nusing Kubernetes to deploy Docker containers, the\nkubernetes_deployment resource natively supports zero-downtime\ndeployments by setting the strategy parameter to RollingUpdate\nand providing configuration via the rolling_update block. Check the\ndocs for the resources you’re using, and make use of native functionality\nwhen you can!\nValid Plans Can Fail\nSometimes, you run the plan command and it shows you a perfectly valid-\nlooking plan, but when you run apply, you’ll get an error. For example,\ntry to add an aws_iam_user resource with the exact same name you used\nfor the IAM user you created manually in Chapter 2:\nresource \"aws_iam_user\" \"existing_user\" {\n  # Make sure to update this to your own user name!\n  name = \"yevgeniy.brikman\"\n}\nIf you now run the plan command, Terraform will show you a plan that\nlooks reasonable:\nTerraform will perform the following actions: \n \n  # aws_iam_user.existing_user will be created \n  + resource \"aws_iam_user\" \"existing_user\" { \n      + arn           = (known after apply) \n      + force_destroy = false \n      + id            = (known after apply)\n\n+ name          = \"yevgeniy.brikman\" \n      + path          = \"/\" \n      + unique_id     = (known after apply) \n    } \n \nPlan: 1 to add, 0 to change, 0 to destroy.\nIf you run the apply command, you’ll get the following error:\nError: Error creating IAM User yevgeniy.brikman: \nEntityAlreadyExists: \nUser with name yevgeniy.brikman already exists. \n \n  on main.tf line 10, in resource \"aws_iam_user\" \"existing_user\": \n  10: resource \"aws_iam_user\" \"existing_user\" {\nThe problem, of course, is that an IAM user with that name already exists.\nThis can happen not just with IAM users but with almost any resource.\nPerhaps someone created that resource manually or via CLI commands, but\neither way, some identifier is the same, and that leads to a conflict. There\nare many variations on this error, and Terraform newbies are often caught\noff guard by them.\nThe key realization is that terraform plan looks only at resources in\nits Terraform state file. If you create resources out of band—such as by\nmanually clicking around the AWS Console—they will not be in\nTerraform’s state file, and, therefore, Terraform will not take them into\naccount when you run the plan command. As a result, a valid-looking plan\nwill still fail.\nThere are two main lessons to take away from this:\nAfter you start using Terraform, you should only use Terraform.\nWhen a part of your infrastructure is managed by Terraform, you should\nnever manually make changes to it. Otherwise, you not only set yourself\nup for weird Terraform errors, but you also void many of the benefits of\nusing infrastructure as code in the first place, given that the code will no\nlonger be an accurate representation of your infrastructure.\n\nIf you have existing infrastructure, use the import command.\nIf you created infrastructure before you started using Terraform, you can\nuse the terraform import command to add that infrastructure to\nTerraform’s state file so that Terraform is aware of and can manage that\ninfrastructure. The import command takes two arguments. The first\nargument is the “address” of the resource in your Terraform\nconfiguration files. This makes use of the same syntax as resource\nreferences, such as <PROVIDER>_<TYPE>.<NAME> (e.g.,\naws_iam_user.existing_user). The second argument is a\nresource-specific ID that identifies the resource to import. For example,\nthe ID for an aws_iam_user resource is the name of the user (e.g.,\nyevgeniy.brikman), and the ID for an aws_instance is the EC2\nInstance ID (e.g., i-190e22e5). The documentation at the bottom of the\npage for each resource typically specifies how to import it.\nFor example, here is the import command that you can use to sync the\naws_iam_user you just added in your Terraform configurations with\nthe IAM user you created back in Chapter 2 (obviously, you should\nreplace “yevgeniy.brikman” with your own username in this command):\n$ terraform import aws_iam_user.existing_user yevgeniy.brikman\nTerraform will use the AWS API to find your IAM user and create an\nassociation in its state file between that user and the\naws_iam_user.existing_user resource in your Terraform\nconfigurations. From then on, when you run the plan command,\nTerraform will know that an IAM user already exists and not try to\ncreate it again.\nNote that if you have a lot of existing resources that you want to import\ninto Terraform, writing the Terraform code for them from scratch and\nimporting them one at a time can be painful, so you might want to look\ninto tools such as terraformer and terracognita, which can import both\ncode and state from supported cloud environments automatically.' metadata={'original_pages_range': '295-297', 'source': '082_Valid_Plans_Can_Fail', 'file_path': '/mnt/c/Users/Carlos/Documents/Github/jupiter-iaa-azure/Scripts/RAG/../../data/optimized_chunks/Libro-TF/082_Valid_Plans_Can_Fail.pdf', 'num_pages': 3}", "page_content='Refactoring Can Be Tricky\nA common programming practice is refactoring, in which you restructure\nthe internal details of an existing piece of code without changing its\nexternal behavior. The goal is to improve the readability, maintainability,\nand general hygiene of the code. Refactoring is an essential coding practice\nthat you should do regularly. However, when it comes to Terraform, or any\nIaC tool, you have to be careful about what defines the “external behavior”\nof a piece of code, or you will run into unexpected problems.\nFor example, a common refactoring practice is to rename a variable or a\nfunction to give it a clearer name. Many IDEs even have built-in support for\nrefactoring and can automatically rename the variable or function for you,\nacross the entire codebase. Although such a renaming is something you\nmight do without thinking twice in a general-purpose programming\nlanguage, you need to be very careful about how you do it in Terraform, or\nit could lead to an outage.\nFor example, the webserver-cluster module has an input variable\nnamed cluster_name:\nvariable \"cluster_name\" {\n  description = \"The name to use for all the cluster resources\"\n  type        = string\n}\nPerhaps you start using this module for deploying microservices, and,\ninitially, you set your microservice’s name to foo. Later on, you decide that\nyou want to rename the service to bar. This might seem like a trivial\nchange, but it can actually cause an outage!\nThat’s because the webserver-cluster module uses the\ncluster_name variable in a number of resources, including the name\nparameters of two security groups and the ALB:\nresource \"aws_lb\" \"example\" {\n  name               = var.cluster_name\n  load_balancer_type = \"application\"\n\nsubnets            = data.aws_subnets.default.ids\n  security_groups    = [aws_security_group.alb.id]\n}\nIf you change the name parameter of certain resources, Terraform will\ndelete the old version of the resource and create a new version to replace it.\nIf the resource you are deleting happens to be an ALB, there will be nothing\nto route traffic to your web server cluster until the new ALB boots up.\nSimilarly, if the resource you are deleting happens to be a security group,\nyour servers will reject all network traffic until the new security group is\ncreated.\nAnother refactor that you might be tempted to do is to change a Terraform\nidentifier. For example, consider the aws_security_group resource in\nthe webserver-cluster module:\nresource \"aws_security_group\" \"instance\" {\n  # (...)\n}\nThe identifier for this resource is called instance. Perhaps you were\ndoing a refactor and you thought it would be clearer to change this name to\ncluster_instance:\nresource \"aws_security_group\" \"cluster_instance\" {\n  # (...)\n}\nWhat’s the result? Yup, you guessed it: downtime.\nTerraform associates each resource identifier with an identifier from the\ncloud provider, such as associating an iam_user resource with an AWS\nIAM User ID or an aws_instance resource with an AWS EC2 Instance\nID. If you change the resource identifier, such as changing the\naws_security_group identifier from instance to\ncluster_instance, as far as Terraform knows, you deleted the old\nresource and have added a completely new one. As a result, if you apply\n\nthese changes, Terraform will delete the old security group and create a new\none, and in the time period in between, your servers will reject all network\ntraffic. You may run into similar problems if you change the identifier\nassociated with a module, split one module into multiple modules, or add\ncount or for_each to a resource or module that didn’t have it before.\nThere are four main lessons that you should take away from this discussion:\nAlways use the plan command\nYou can catch all of these gotchas by running the plan command,\ncarefully scanning the output, and noticing that Terraform plans to\ndelete a resource that you probably don’t want deleted.\nCreate before destroy\nIf you do want to replace a resource, think carefully about whether its\nreplacement should be created before you delete the original. If so, you\nmight be able to use create_before_destroy to make that\nhappen. Alternatively, you can also accomplish the same effect through\ntwo manual steps: first, add the new resource to your configurations and\nrun the apply command; second, remove the old resource from your\nconfigurations and run the apply command again.\nRefactoring may require changing state\nIf you want to refactor your code without accidentally causing\ndowntime, you’ll need to update the Terraform state accordingly.\nHowever, you should never update Terraform state files by hand!\nInstead, you have two options: do it manually by running terraform\nstate mv commands, or do it automatically by adding a moved\nblock to your code.\nLet’s first look at the terraform state mv command, which has\nthe following syntax:\nterraform state mv <ORIGINAL_REFERENCE> <NEW_REFERENCE>\n\nwhere ORIGINAL_REFERENCE is the reference expression to the\nresource as it is now and NEW_REFERENCE is the new location you\nwant to move it to. For example, if you’re renaming an\naws_security_group group from instance to\ncluster_instance, you could run the following:\n$ terraform state mv \\ \n  aws_security_group.instance \\ \n  aws_security_group.cluster_instance\nThis instructs Terraform that the state that used to be associated with\naws _security_group.instance should now be associated with\naws_security_group.cluster_instance. If you rename an\nidentifier and run this command, you’ll know you did it right if the\nsubsequent terraform plan shows no changes.\nHaving to remember to run CLI commands manually is error prone,\nespecially if you refactored a module used by dozens of teams in your\ncompany, and each of those teams needs to remember to run\nterraform state mv to avoid downtime. Fortunately, Terraform\n1.1 has added a way to handle this automatically: moved blocks. Any\ntime you refactor your code, you should add a moved block to capture\nhow the state should be updated. For example, to capture that the\naws_security_group resource was renamed from instance to\ncluster_instance, you would add the following moved block:\nmoved {\n  from = aws_security_group.instance\n  to   = aws_security_group.cluster_instance\n}\n\nNow, whenever anyone runs apply on this code, Terraform will\nautomatically detect if it needs to update the state file:\nTerraform will perform the following actions: \n \n  # aws_security_group.instance has moved to \n  # aws_security_group.cluster_instance \n    resource \"aws_security_group\" \"cluster_instance\" { \n        name                   = \"moved-example-security-\ngroup\" \n        tags                   = {} \n        # (8 unchanged attributes hidden) \n    } \n \nPlan: 0 to add, 0 to change, 0 to destroy. \n \nDo you want to perform these actions? \n  Terraform will perform the actions described above. \n  Only 'yes' will be accepted to approve. \n \n  Enter a value:\nIf you enter yes, Terraform will update the state automatically, and as\nthe plan shows no resources to add, change, or destroy, Terraform will\nmake no other changes—which is exactly what you want!\nSome parameters are immutable' metadata={'original_pages_range': '298-302', 'source': '083_Refactoring_Can_Be_Tricky', 'file_path': '/mnt/c/Users/Carlos/Documents/Github/jupiter-iaa-azure/Scripts/RAG/../../data/optimized_chunks/Libro-TF/083_Refactoring_Can_Be_Tricky.pdf', 'num_pages': 5}", "page_content='The parameters of many resources are immutable, so if you change\nthem, Terraform will delete the old resource and create a new one to\nreplace it. The documentation for each resource often specifies what\nhappens if you change a parameter, so get used to checking the\ndocumentation. And, once again, make sure to always use the plan\ncommand and consider whether you should use a\ncreate_before_destroy strategy.\nConclusion\nAlthough Terraform is a declarative language, it includes a large number of\ntools, such as variables and modules, which you saw in Chapter 4, and\ncount, for_each, for, create_before_destroy, and built-in\nfunctions, which you saw in this chapter, that give the language a surprising\namount of flexibility and expressive power. There are many permutations of\nthe if-statement tricks shown in this chapter, so spend some time browsing\nthe functions documentation, and let your inner hacker go wild. OK, maybe\nnot too wild, as someone still needs to maintain your code, but just wild\nenough that you can create clean, beautiful APIs for your modules.\nLet’s now move on to Chapter 6, where I’ll go over how create modules\nthat are not only clean and beautiful but also handle secrets and sensitive\ndata in a safe and secure manner.\n1 Credit for this technique goes to Paul Hinze.' metadata={'original_pages_range': '303', 'source': '084_Conclusion', 'file_path': '/mnt/c/Users/Carlos/Documents/Github/jupiter-iaa-azure/Scripts/RAG/../../data/optimized_chunks/Libro-TF/084_Conclusion.pdf', 'num_pages': 1}", "page_content='Chapter 6. Managing Secrets\nwith Terraform\nAt some point, you and your software will be entrusted with a variety of\nsecrets, such as database passwords, API keys, TLS certificates, SSH keys,\nGPG keys, and so on. This is all sensitive data that, if it were to get into the\nwrong hands, could do a lot of damage to your company and its customers.\nIf you build software, it is your responsibility to keep those secrets secure.\nFor example, consider the following Terraform code for deploying a\ndatabase:\nresource \"aws_db_instance\" \"example\" {\n  identifier_prefix   = \"terraform-up-and-running\"\n  engine              = \"mysql\"\n  allocated_storage   = 10\n  instance_class      = \"db.t2.micro\"\n  skip_final_snapshot = true\n  db_name             = var.db_name \n \n  # How to set these parameters securely?\n  username = \"???\"\n  password = \"???\"\n}\nThis code requires you to set two secrets, the username and password,\nwhich are the credentials for the master user of the database. If the wrong\nperson gets access to them, it could be catastrophic, as these credentials\ngive you superuser access to that database and all the data within it. So,\nhow do you keep these secrets secure?\nThis is part of the broader topic of secrets management, which is the focus\nof this chapter. This chapter will cover:\nSecret management basics' metadata={'original_pages_range': '304', 'source': '085_6._Managing_Secrets_with_Terraform', 'file_path': '/mnt/c/Users/Carlos/Documents/Github/jupiter-iaa-azure/Scripts/RAG/../../data/optimized_chunks/Libro-TF/085_6._Managing_Secrets_with_Terraform.pdf', 'num_pages': 1}", "page_content='Secret management tools\nSecret management tools with Terraform\nSecret Management Basics\nThe first rule of secrets management is:\nDo not store secrets in plain text.\nThe second rule of secrets management is:\nDO NOT STORE SECRETS IN PLAIN TEXT.\nSeriously, don’t do it. For example, do not hardcode your database\ncredentials directly in your Terraform code and check it into version\ncontrol:\nresource \"aws_db_instance\" \"example\" {\n  identifier_prefix   = \"terraform-up-and-running\"\n  engine              = \"mysql\"\n  allocated_storage   = 10\n  instance_class      = \"db.t2.micro\"\n  skip_final_snapshot = true\n  db_name             = var.db_name \n \n  # DO NOT DO THIS!!!\n  username = \"admin\"\n  password = \"password\"\n  # DO NOT DO THIS!!!\n}\nStoring secrets in plain text in version control is a bad idea. Here are just a\nfew of the reasons why:\nAnyone who has access to the version control system has access to that\nsecret.\nIn the preceding example, every single developer at your company who\ncan access that Terraform code will have access to the master\ncredentials for your database.\n\nEvery computer that has access to the version control system keeps a copy\nof that secret.\nEvery single computer that has ever checked out that repo may still\nhave a copy of that secret on its local hard drive. That includes the\ncomputer of every developer on your team, every computer involved in\nCI (e.g., Jenkins, CircleCI, GitLab, etc.), every computer involved in\nversion control (e.g., GitHub, GitLab, BitBucket), every computer\ninvolved in deployment (e.g., all your pre-prod and prod environments),\nevery computer involved in backup (e.g., CrashPlan, Time Machine,\netc.), and so on.\nEvery piece of software you run has access to that secret.\nBecause the secrets are sitting in plain text on so many hard drives,\nevery single piece of software running on any of those computers can\npotentially read that secret.\nThere’s no way to audit or revoke access to that secret.\nWhen secrets are sitting on hundreds of hard drives in plain text, you\nhave no way to know who accessed them (there’s no audit log) and no\neasy way to revoke access.\nIn short, if you store secrets in plain text, you are giving malicious actors\n(e.g., hackers, competitors, disgruntled former employees) countless ways\nto access your company’s most sensitive data—e.g., by compromising the\nversion control system, or by compromising any of the computers you use,\nor by compromising any piece of software on any of those computers—and\nyou’ll have no idea if you were compromised or have any easy way to fix\nthings if you were.\nTherefore, it’s essential that you use a proper secret management tool to\nstore your secrets.' metadata={'original_pages_range': '305-306', 'source': '086_Secret_Management_Basics', 'file_path': '/mnt/c/Users/Carlos/Documents/Github/jupiter-iaa-azure/Scripts/RAG/../../data/optimized_chunks/Libro-TF/086_Secret_Management_Basics.pdf', 'num_pages': 2}", "page_content='Secret Management Tools\nA comprehensive overview of all aspects of secret management is beyond\nthe scope of this book, but to be able to use secret management tools with\nTerraform, it’s worth briefly touching on the following topics:\nThe types of secrets you store\nThe way you store secrets\nThe interface you use to access secrets\nA comparison of secret management tools\nThe Types of Secrets You Store\nThere are three primary types of secrets: personal secrets, customer secrets,\nand infrastructure secrets.\nPersonal secrets\nBelong to an individual. Examples include the usernames and\npasswords for websites you visit, your SSH keys, and your Pretty Good\nPrivacy (PGP) keys.\nCustomer secrets\nBelong to your customers. Note that if you run software for other\nemployees of your company—e.g., you manage your company’s\ninternal Active Directory server—then those other employees are your\ncustomers. Examples include the usernames and passwords that your\ncustomers use to log into your product, personally identifiable info (PII)\nfor your customers, and personal health information (PHI) for your\ncustomers.\nInfrastructure secrets\nBelong to your infrastructure. Examples include database passwords,\nAPI keys, and TLS certificates.' metadata={'original_pages_range': '307', 'source': '087_Secret_Management_Tools_y_1_mas', 'file_path': '/mnt/c/Users/Carlos/Documents/Github/jupiter-iaa-azure/Scripts/RAG/../../data/optimized_chunks/Libro-TF/087_Secret_Management_Tools_y_1_mas.pdf', 'num_pages': 1}", "page_content='Most secret management tools are designed to store exactly one of these\ntypes of secrets, and while you could try to force it to store the other types,\nthat’s rarely a good idea from a security or usability standpoint. For\nexample, the way you store passwords that are infrastructure secrets is\ncompletely different from how you store passwords that are customer\nsecrets: for the former, you’d typically use an encryption algorithm such as\nAES (Advanced Encryption Standard), perhaps with a nonce, as you need\nto be able to decrypt the secrets and get back the original password; on the\nother hand, for the latter, you’d typically use a hashing algorithm (e.g.,\nbcrypt) with a salt, as there should be no way to get back the original\npassword. Using the wrong approach can be catastrophic, so use the right\ntool for the job!\nThe Way You Store Secrets\nThe two most common strategies for storing secrets are to use either a file-\nbased secret store or a centralized secret store.\nFile-based secret stores store secrets in encrypted files, which are typically\nchecked into version control. To encrypt the files, you need an encryption\nkey. This key is itself a secret! This creates a bit of a conundrum: How do\nyou securely store that key? You can’t check the key into version control as\nplain text, as then there’s no point of encrypting anything with it. You could\nencrypt the key with another key, but then all you’ve done is kicked the can\ndown the road, as you still have to figure out how to securely store that\nsecond key.\nThe most common solution to this conundrum is to store the key in a key\nmanagement service (KMS) provided by your cloud provider, such as AWS\nKMS, GCP KMS, or Azure Key Vault. This solves the kick-the-can-down-\nthe-road problem by trusting the cloud provider to securely store the secret\nand manage access to it. Another option is to use PGP keys. Each developer\ncan have their own PGP key, which consists of a public key and a private\nkey. If you encrypt a secret with one or more public keys, only developers\nwith the corresponding private keys will be able to decrypt those secrets.' metadata={'original_pages_range': '308', 'source': '088_The_Way_You_Store_Secrets', 'file_path': '/mnt/c/Users/Carlos/Documents/Github/jupiter-iaa-azure/Scripts/RAG/../../data/optimized_chunks/Libro-TF/088_The_Way_You_Store_Secrets.pdf', 'num_pages': 1}", "page_content='The private keys, in turn, are protected by a password that the developer\neither memorizes or stores in a personal secrets manager.\nCentralized secret stores are typically web services that you talk to over the\nnetwork that encrypt your secrets and store them in a data store such as\nMySQL, PostgreSQL, DynamoDB, etc. To encrypt these secrets, these\ncentralized secret stores need an encryption key. Typically, the encryption\nkey is managed by the service itself, or the service relies on a cloud\nprovider’s KMS.\nThe Interface You Use to Access Secrets\nMost secret management tools can be accessed via an API, CLI, and/or UI.\nJust about all centralized secret stores expose an API that you can consume\nvia network requests: e.g., a REST API you access over HTTP. The API is\nconvenient for when your code needs to programmatically read secrets. For\nexample, when an app is booting up, it can make an API call to your\ncentralized secret store to retrieve a database password. Also, as you’ll see\nlater in this chapter, you can write Terraform code that, under the hood, uses\na centralized secret store’s API to retrieve secrets.\nAll the file-based secret stores work via a command-line interface (CLI).\nMany of the centralized secret stores also provide CLI tools that, under the\nhood, make API calls to the service. CLI tools are a convenient way for\ndevelopers to access secrets (e.g., using a few CLI commands to encrypt a\nfile) and for scripting (e.g., writing a script to encrypt secrets).\nSome of the centralized secret stores also expose a user interface (UI) via\nthe web, desktop, or mobile. This is potentially an even more convenient\nway for everyone on your team to access secrets.\nA Comparison of Secret Management Tools\nTable 6-1 shows a comparison of popular secret management tools, broken\ndown by the three considerations defined in the previous sections.' metadata={'original_pages_range': '309', 'source': '089_The_Interface_You_Use_to_Access_Secrets', 'file_path': '/mnt/c/Users/Carlos/Documents/Github/jupiter-iaa-azure/Scripts/RAG/../../data/optimized_chunks/Libro-TF/089_The_Interface_You_Use_to_Access_Secrets.pdf', 'num_pages': 1}", "page_content='The private keys, in turn, are protected by a password that the developer\neither memorizes or stores in a personal secrets manager.\nCentralized secret stores are typically web services that you talk to over the\nnetwork that encrypt your secrets and store them in a data store such as\nMySQL, PostgreSQL, DynamoDB, etc. To encrypt these secrets, these\ncentralized secret stores need an encryption key. Typically, the encryption\nkey is managed by the service itself, or the service relies on a cloud\nprovider’s KMS.\nThe Interface You Use to Access Secrets\nMost secret management tools can be accessed via an API, CLI, and/or UI.\nJust about all centralized secret stores expose an API that you can consume\nvia network requests: e.g., a REST API you access over HTTP. The API is\nconvenient for when your code needs to programmatically read secrets. For\nexample, when an app is booting up, it can make an API call to your\ncentralized secret store to retrieve a database password. Also, as you’ll see\nlater in this chapter, you can write Terraform code that, under the hood, uses\na centralized secret store’s API to retrieve secrets.\nAll the file-based secret stores work via a command-line interface (CLI).\nMany of the centralized secret stores also provide CLI tools that, under the\nhood, make API calls to the service. CLI tools are a convenient way for\ndevelopers to access secrets (e.g., using a few CLI commands to encrypt a\nfile) and for scripting (e.g., writing a script to encrypt secrets).\nSome of the centralized secret stores also expose a user interface (UI) via\nthe web, desktop, or mobile. This is potentially an even more convenient\nway for everyone on your team to access secrets.\nA Comparison of Secret Management Tools\nTable 6-1 shows a comparison of popular secret management tools, broken\ndown by the three considerations defined in the previous sections.\n\nTable 6-1. A comparison of secret management tools\nTypes of secretsSecret storageSecret interface\nHashiCorp Vault Infrastructure Centralized serviceUI, API, CLI\nAWS Secrets Manager Infrastructure Centralized serviceUI, API, CLI\nGoogle Secrets Manager Infrastructure Centralized serviceUI, API, CLI\nAzure Key Vault Infrastructure Centralized serviceUI, API, CLI\nConfidant Infrastructure Centralized serviceUI, API, CLI\nKeywhiz Infrastructure Centralized serviceAPI, CLI\nsops Infrastructure Files CLI\ngit-secret Infrastructure Files CLI\n1Password Personal Centralized serviceUI, API, CLI\nLastPass Personal Centralized serviceUI, API, CLI\nBitwarden Personal Centralized serviceUI, API, CLI\nKeePass Personal Files UI, CLI\nKeychain (macOS) Personal Files UI, CLI\nCredential Manager (Windows)Personal Files UI, CLI\npass Personal Files CLI\nActive Directory Customer Centralized serviceUI, API, CLI\nAuth0 Customer Centralized serviceUI, API, CLI\nOkta Customer Centralized serviceUI, API, CLI\nOneLogin Customer Centralized serviceUI, API, CLI\nPing Customer Centralized serviceUI, API, CLI\nAWS Cognito Customer Centralized serviceUI, API, CLI\na' metadata={'original_pages_range': '309-310', 'source': '090_A_Comparison_of_Secret_Management_Tools', 'file_path': '/mnt/c/Users/Carlos/Documents/Github/jupiter-iaa-azure/Scripts/RAG/../../data/optimized_chunks/Libro-TF/090_A_Comparison_of_Secret_Management_Tools.pdf', 'num_pages': 2}", "page_content='Types of secretsSecret storageSecret interface\na  Vault supports multiple secret engines, most of which are designed for infrastructure\nsecrets, but a few support customer secrets as well.\nSince this is a book about Terraform, from here on out, I’ll mostly be\nfocusing on secret management tools designed for infrastructure secrets that\nare accessed through an API or the CLI (although I’ll mention personal\nsecret management tools from time to time too, as those often contain the\nsecrets you need to authenticate to the infrastructure secret tools).\nSecret Management Tools with Terraform\nLet’s now turn to how to use these secret management tools with Terraform,\ngoing through each of the three places where your Terraform code is likely\nto brush up against secrets:\nProviders\nResources and data sources\nState files and plan files\nProviders\nTypically, your first exposure to secrets when working with Terraform is\nwhen you have to authenticate to a provider. For example, if you want to\nrun terraform apply on code that uses the AWS Provider, you’ll need\nto first authenticate to AWS, and that typically means using your access\nkeys, which are secrets. How should you store those secrets? And how\nshould you make them available to Terraform?\nThere are many ways to answer these questions. One way you should not\ndo it, even though it occasionally comes up in the Terraform\ndocumentation, is by putting secrets directly into the code, in plain text:' metadata={'original_pages_range': '311', 'source': '091_Secret_Management_Tools_with_Terraform_y_1_mas', 'file_path': '/mnt/c/Users/Carlos/Documents/Github/jupiter-iaa-azure/Scripts/RAG/../../data/optimized_chunks/Libro-TF/091_Secret_Management_Tools_with_Terraform_y_1_mas.pdf', 'num_pages': 1}", "page_content='provider \"aws\" {\n  region = \"us-east-2\" \n \n  # DO NOT DO THIS!!!\n  access_key = \"(ACCESS_KEY)\"\n  secret_key = \"(SECRET_KEY)\"\n  # DO NOT DO THIS!!!\n}\nStoring credentials this way, in plain text, is not secure, as discussed earlier\nin this chapter. Moreover, it’s also not practical, as this would hardcode you\nto using one set of credentials for all users of this module, whereas in most\ncases, you’ll need different credentials on different computers (e.g., when\ndifferent developers or your CI server runs apply) and in different\nenvironments (dev, stage, prod).\nThere are several techniques that are far more secure for storing your\ncredentials and making them accessible to Terraform providers. Let’s take a\nlook at these techniques, grouping them based on the user who is running\nTerraform:\nHuman users\nDevelopers running Terraform on their own computers.\nMachine users\nAutomated systems (e.g., a CI server) running Terraform with no\nhumans present.\nHuman users\nJust about all Terraform providers allow you to specify your credentials in\nsome way other than putting them directly into the code. The most common\noption is to use environment variables. For example, here’s how you use\nenvironment variables to authenticate to AWS:\n$  export AWS_ACCESS_KEY_ID=(YOUR_ACCESS_KEY_ID) \n$  export AWS_SECRET_ACCESS_KEY=(YOUR_SECRET_ACCESS_KEY)\n\nSetting your credentials as environment variables keeps plain-text secrets\nout of your code, ensures that everyone running Terraform has to provide\ntheir own credentials, and ensures that credentials are only ever stored in\nmemory, and not on disk.\nOne important question you may ask is where to store the access key ID\nand secret access key in the first place. They are too long and random to\nmemorize, but if you store them on your computer in plain text, then you’re\nstill putting those secrets at risk. Since this section is focused on human\nusers, the solution is to store your access keys (and other secrets) in a secret\nmanager designed for personal secrets. For example, you could store your\naccess keys in 1Password or LastPass and copy/paste them into the\nexport commands in your terminal.\nIf you’re using these credentials frequently on the CLI, an even more\nconvenient option is to use a secret manager that supports a CLI interface.\nFor example, 1Password offers a CLI tool called op. On Mac and Linux,\nyou can use op to authenticate to 1Password on the CLI as follows:\n$ eval $(op signin my)\nOnce you’ve authenticated, assuming you had used the 1Password app to\nstore your access keys under the name “aws-dev” with fields “id” and\n“secret”, here’s how you can use op to set those access keys as environment\nvariables:\n$ export AWS_ACCESS_KEY_ID=$(op get item 'aws-dev' --fields 'id') \n$ export AWS_SECRET_ACCESS_KEY=$(op get item 'aws-dev' --fields \n'secret')\nWhile tools like 1Password and op are great for general-purpose secrets\nmanagement, for certain providers, there are dedicated CLI tools to make\nthis even easier. For example, for authenticating to AWS, you can use the\nopen source tool aws-vault. You can save your access keys using the\naws-vault add command under a profile named dev as follows:\n1' metadata={'original_pages_range': '312-313', 'source': '092_Human_users', 'file_path': '/mnt/c/Users/Carlos/Documents/Github/jupiter-iaa-azure/Scripts/RAG/../../data/optimized_chunks/Libro-TF/092_Human_users.pdf', 'num_pages': 2}", "page_content='$ aws-vault add dev \nEnter Access Key Id: (YOUR_ACCESS_KEY_ID) \nEnter Secret Key: (YOUR_SECRET_ACCESS_KEY)\nUnder the hood, aws-vault will store these credentials securely in your\noperating system’s native password manager (e.g., Keychain on macOS,\nCredential Manager on Windows). Once you’ve stored these credentials,\nnow you can authenticate to AWS for any CLI command as follows:\n$ aws-vault exec <PROFILE> -- <COMMAND>\nwhere PROFILE is the name of a profile you created earlier via the add\ncommand (e.g., dev) and COMMAND is the command to execute. For\nexample, here’s how you can use the dev credentials you saved earlier to\nrun terraform apply:\n$ aws-vault exec dev -- terraform apply\nThe exec command automatically uses AWS STS to fetch temporary\ncredentials and exposes them as environment variables to the command\nyou’re executing (in this case, terraform apply). This way, not only\nare your permanent credentials stored in a secure manner (in your operating\nsystem’s native password manager), but now, you’re also only exposing\ntemporary credentials to any process you run, so the risk of leaking\ncredentials is minimized. aws-vault also has native support for assuming\nIAM roles, using multifactor authentication (MFA), logging into accounts\non the web console, and more.\nMachine users\nWhereas a human user can rely on a memorized password, what do you do\nin cases where there’s no human present? For example, if you’re setting up\na continuous integration / continuous delivery (CI/CD) pipeline to\nautomatically run Terraform code, how do you securely authenticate that\npipeline? In this case, you are dealing with authentication for a machine\nuser. The question is, how do you get one machine (e.g., your CI server) to' metadata={'original_pages_range': '314', 'source': '093_Machine_users', 'file_path': '/mnt/c/Users/Carlos/Documents/Github/jupiter-iaa-azure/Scripts/RAG/../../data/optimized_chunks/Libro-TF/093_Machine_users.pdf', 'num_pages': 1}", "page_content='authenticate itself to another machine (e.g., AWS API servers) without\nstoring any secrets in plain text?\nThe solution here heavily depends on the type of machines involved: that is,\nthe machine you’re authenticating from and the machine you’re\nauthenticating to. Let’s go through three examples:\nCircleCI as a CI server, with stored secrets\nEC2 Instance running Jenkins as a CI server, with IAM roles\nGitHub Actions as a CI server, with OIDC\nWARNING: SIMPLIFIED EXAMPLES\nThis section contains examples that fully flush out how to handle provider\nauthentication in a CI/CD context, but all other aspects of the CI/CD workflow are\nhighly simplified. You’ll see more complete, end-to-end, production-ready CI/CD\nworkflows in Chapter 9.\nCircleCI as a CI server, with stored secrets\nLet’s imagine that you want to use CircleCI, a popular managed CI/CD\nplatform, to run Terraform code. With CircleCI, you configure your build\nsteps in a .circleci/config.yml file, where you might define a job to run\nterraform apply that looks like this:\nversion: '2.1'\norbs:\n  # Install Terraform using a CircleCi Orb\n  terraform: circleci/terraform@1.1.0\njobs:\n  # Define a job to run 'terraform apply'\n  terraform_apply:\n    executor: terraform/default\n    steps:\n      - checkout         # git clone the code\n      - terraform/init   # Run 'terraform init'\n      - terraform/apply  # Run 'terraform apply'\nworkflows:\n  # Create a workflow to run the 'terraform apply' job defined\n\nabove\n  deploy:\n    jobs:\n      - terraform_apply\n    # Only run this workflow on commits to the main branch\n    filters:\n      branches:\n        only:\n          - main\nWith a tool like CircleCI, the way to authenticate to a provider is to create a\nmachine user in that provider (that is, a user solely used for automation, and\nnot by any human), store the credentials for that machine user in CircleCI in\nwhat’s called a CircleCI Context, and when your build runs, CircleCI will\nexpose the credentials in that Context to your workflows as environment\nvariables. For example, if your Terraform code needs to authenticate to\nAWS, you would create a new IAM user in AWS, give that IAM user the\npermissions it needs to deploy your Terraform changes, and manually copy\nthat IAM user’s access keys into a CircleCI Context, as shown in Figure 6-\n1.\n\nFigure 6-1. A CircleCI Context with AWS credentials.\nFinally, you update the workflows in your .circleci/config.yml file to use\nyour CircleCI Context via the context parameter:\nworkflows:\n  # Create a workflow to run the 'terraform apply' job defined \nabove' metadata={'original_pages_range': '315-317', 'source': '094_CircleCI_as_a_CI_server,_with_stored_secrets', 'file_path': '/mnt/c/Users/Carlos/Documents/Github/jupiter-iaa-azure/Scripts/RAG/../../data/optimized_chunks/Libro-TF/094_CircleCI_as_a_CI_server,_with_stored_secrets.pdf', 'num_pages': 3}", "page_content='deploy:\n    jobs:\n      - terraform_apply\n    # Only run this workflow on commits to the main branch\n    filters:\n      branches:\n        only:\n          - main\n    # Expose secrets in the CircleCI context as environment \nvariables\n    context:\n      - example-context\nWhen your build runs, CircleCI will automatically expose the secrets in that\nContext as environment variables—in this case, as the environment\nvariables AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY—\nand terraform apply will automatically use those environment\nvariables to authenticate to your provider.\nThe biggest drawbacks to this approach are that (a) you have to manually\nmanage credentials, and (b) as a result, you have to use permanent\ncredentials, which once saved in CircleCI, rarely (if ever) change. The next\ntwo examples show alternative approaches.\nEC2 Instance running Jenkins as a CI server, with IAM roles\nIf you’re using an EC2 Instance to run Terraform code—e.g., you’re\nrunning Jenkins on an EC2 Instance as a CI server—the solution I\nrecommend for machine user authentication is to give that EC2 Instance an\nIAM role. An IAM role is similar to an IAM user, in that it’s an entity in\nAWS that can be granted IAM permissions. However, unlike IAM users,\nIAM roles are not associated with any one person and do not have\npermanent credentials (password or access keys). Instead, a role can be\nassumed by other IAM entities: for example, an IAM user might assume a\nrole to temporarily get access to different permissions than they normally\nhave; many AWS services, such as EC2 Instances, can assume IAM roles to\ngrant those services permissions in your AWS account.\nFor example, here’s code you’ve seen many times to deploy an EC2\nInstance:\n\nresource \"aws_instance\" \"example\" {\n  ami           = \"ami-0fb653ca2d3203ac1\"\n  instance_type = \"t2.micro\"\n}\nTo create an IAM role, you must first define an assume role policy, which is\nan IAM Policy that defines who is allowed to assume the IAM role. You\ncould write the IAM Policy in raw JSON, but Terraform has a convenient\naws_iam_policy_document data source that can create the JSON for\nyou. Here’s how you can use an aws_iam_policy_document to\ndefine an assume role policy that allows the EC2 service to assume an IAM\nrole:\ndata \"aws_iam_policy_document\" \"assume_role\" { \n  statement {\n    effect  = \"Allow\"\n    actions = [\"sts:AssumeRole\"] \n \n    principals {\n      type        = \"Service\"\n      identifiers = [\"ec2.amazonaws.com\"] \n    } \n  }\n}\nNow, you can use the aws_iam_role resource to create an IAM role and\npass it the JSON from your aws_iam_policy_document to use as the\nassume role policy:\nresource \"aws_iam_role\" \"instance\" {\n  name_prefix        = var.name\n  assume_role_policy = \ndata.aws_iam_policy_document.assume_role.json\n}\nYou now have an IAM role, but by default, IAM roles don’t give you any\npermissions. So, the next step is to attach one or more IAM policies to the\nIAM role that specify what you can actually do with the role once you’ve\nassumed it. Let’s imagine that you’re using Jenkins to run Terraform code\nthat deploys EC2 Instances. You can use the\n\naws_iam_policy_document data source to define an IAM Policy that\ngives admin permissions over EC2 Instances as follows:\ndata \"aws_iam_policy_document\" \"ec2_admin_permissions\" { \n  statement {\n    effect    = \"Allow\"\n    actions   = [\"ec2:*\"]\n    resources = [\"*\"] \n  }\n}\nAnd you can attach this policy to your IAM role using the\naws_iam_role_policy resource:\nresource \"aws_iam_role_policy\" \"example\" {\n  role   = aws_iam_role.instance.id\n  policy = \ndata.aws_iam_policy_document.ec2_admin_permissions.json\n}\nThe final step is to allow your EC2 Instance to automatically assume that\nIAM role by creating an instance profile:\nresource \"aws_iam_instance_profile\" \"instance\" {\n  role = aws_iam_role.instance.name\n}\nAnd then tell your EC2 Instance to use that instance profile via the\niam_instance_profile parameter:\nresource \"aws_instance\" \"example\" {\n  ami           = \"ami-0fb653ca2d3203ac1\"\n  instance_type = \"t2.micro\" \n \n  # Attach the instance profile\n  iam_instance_profile = aws_iam_instance_profile.instance.name\n}\nUnder the hood, AWS runs an instance metadata endpoint on every EC2\nInstance at http://169.254.169.254. This is an endpoint that can only be' metadata={'original_pages_range': '318-320', 'source': '095_EC2_Instance_running_Jenkins_as_a_CI_server,_with_IAM_roles', 'file_path': '/mnt/c/Users/Carlos/Documents/Github/jupiter-iaa-azure/Scripts/RAG/../../data/optimized_chunks/Libro-TF/095_EC2_Instance_running_Jenkins_as_a_CI_server,_with_IAM_roles.pdf', 'num_pages': 3}", "page_content='reached by processes running on the instance itself, and those processes can\nuse that endpoint to fetch metadata about the instance. For example, if you\nSSH to an EC2 Instance, you can query this endpoint using curl:\n$ ssh ubuntu@<IP_OF_INSTANCE> \nWelcome to Ubuntu 20.04.3 LTS (GNU/Linux 5.11.0-1022-aws x86_64) \n(...) \n \n$ curl http://169.254.169.254/latest/meta-data/ \nami-id \nami-launch-index \nami-manifest-path \nblock-device-mapping/ \nevents/ \nhibernation/ \nhostname \nidentity-credentials/ \n(...)\nIf the instance has an IAM role attached (via an instance profile), that\nmetadata will include AWS credentials that can be used to authenticate to\nAWS and assume that IAM role. Any tool that uses the AWS SDK, such as\nTerraform, knows how to use these instance metadata endpoint credentials\nautomatically, so as soon as you run terraform apply on the EC2\nInstance with this IAM role, your Terraform code will authenticate as this\nIAM role, which will thereby grant your code the EC2 admin permissions it\nneeds to run successfully.\nFor any automated process running in AWS, such as a CI server, IAM roles\nprovide a way to authenticate (a) without having to manage credentials\nmanually, and (b) the credentials AWS provides via the instance metadata\nendpoint are always temporary, and rotated automatically. These are two big\nadvantages over the manually managed, permanent credentials with a tool\nlike CircleCI that runs outside of your AWS account. However, as you’ll\nsee in the next example, in some cases, it’s possible to have these same\nadvantages for external tools, too.\nGitHub Actions as a CI server, with OIDC\n2\n\nGitHub Actions is another popular managed CI/CD platform you might\nwant to use to run Terraform. In the past, GitHub Actions required you to\nmanually copy credentials around, just like CircleCI. However, as of 2021,\nGitHub Actions offers a better alternative: Open ID Connect (OIDC). Using\nOIDC, you can establish a trusted link between the CI system and your\ncloud provider (GitHub Actions supports AWS, Azure, and Google Cloud)\nso that your CI system can authenticate to those providers without having to\nmanage any credentials manually.\nYou define GitHub Actions workflows in YAML files in a\n.github/workflows folder, such as the terraform.yml file shown here:\nname: Terraform Apply\n# Only run this workflow on commits to the main branch\non:\n  push:\n    branches:\n      - 'main'\njobs:\n  TerraformApply:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v2 \n \n      # Run Terraform using HashiCorp's setup-terraform Action\n      - uses: hashicorp/setup-terraform@v1\n          with:\n            terraform_version: 1.1.0\n            terraform_wrapper: false\n        run: |\n          terraform init\n          terraform apply -auto-approve\nIf your Terraform code talks to a provider such as AWS, you need to\nprovide a way for this workflow to authenticate to that provider. To do this\nusing OIDC, the first step is to create an IAM OIDC identity provider in\nyour AWS account, using the\naws_iam_openid_connect_provider resource, and to configure it\nto trust the GitHub Actions thumbprint, fetched via the\ntls_certificate data source:\n3\n\n# Create an IAM OIDC identity provider that trusts GitHub\nresource \"aws_iam_openid_connect_provider\" \"github_actions\" {\n  url             = \"https://token.actions.githubusercontent.com\"\n  client_id_list  = [\"sts.amazonaws.com\"]\n  thumbprint_list = [ \n    data.tls_certificate.github.certificates[0].sha1_fingerprint \n  ]\n} \n \n# Fetch GitHub's OIDC thumbprint\ndata \"tls_certificate\" \"github\" {\n  url = \"https://token.actions.githubusercontent.com\"\n}\nNow, you can create IAM roles exactly as in the previous section—e.g., an\nIAM role with EC2 admin permissions attached—except the assume role\npolicy for those IAM roles will look different:\ndata \"aws_iam_policy_document\" \"assume_role_policy\" { \n  statement {\n    actions = [\"sts:AssumeRoleWithWebIdentity\"]\n    effect  = \"Allow\" \n \n    principals {\n      identifiers = \n[aws_iam_openid_connect_provider.github_actions.arn]\n      type        = \"Federated\" \n    } \n \n    condition {\n      test     = \"StringEquals\"\n      variable = \"token.actions.githubusercontent.com:sub\"\n      # The repos and branches defined in \nvar.allowed_repos_branches\n      # will be able to assume this IAM role\n      values = [ \n        for a in var.allowed_repos_branches : \n        \n\"repo:${a[\"org\"]}/${a[\"repo\"]}:ref:refs/heads/${a[\"branch\"]}\" \n      ] \n    } \n  }\n}\n\nThis policy allows the IAM OIDC identity provider to assume the IAM role\nvia federated authentication. Note the condition block, which ensures\nthat only the specific GitHub repos and branches you specify via the\nallowed_repos_branches input variable can assume this IAM role:\nvariable \"allowed_repos_branches\" {\n  description = \"GitHub repos/branches allowed to assume the IAM \nrole.\"\n  type = list(object({\n    org    = string\n    repo   = string\n    branch = string \n  }))\n  # Example:\n  # allowed_repos_branches = [\n  #   {\n  #     org    = \"brikis98\"\n  #     repo   = \"terraform-up-and-running-code\"\n  #     branch = \"main\"\n  #   }\n  # ]\n}\nThis is important to ensure you don’t accidentally allow all GitHub repos to\nauthenticate to your AWS account! You can now configure your builds in\nGitHub Actions to assume this IAM role. First, at the top of your workflow,\ngive your build the id-token: write permission:\npermissions:\n  id-token: write\nNext, add a build step just before running Terraform to authenticate to AWS\nusing the configure-aws-credentials action:\n      # Authenticate to AWS using OIDC\n      - uses: aws-actions/configure-aws-credentials@v1\n        with:\n          # Specify the IAM role to assume here\n          role-to-assume: arn:aws:iam::123456789012:role/example-\nrole\n          aws-region: us-east-2' metadata={'original_pages_range': '321-324', 'source': '096_GitHub_Actions_as_a_CI_server,_with_OIDC', 'file_path': '/mnt/c/Users/Carlos/Documents/Github/jupiter-iaa-azure/Scripts/RAG/../../data/optimized_chunks/Libro-TF/096_GitHub_Actions_as_a_CI_server,_with_OIDC.pdf', 'num_pages': 4}", "page_content='# Run Terraform using HashiCorp's setup-terraform Action\n      - uses: hashicorp/setup-terraform@v1\n          with:\n            terraform_version: 1.1.0\n            terraform_wrapper: false\n        run: |\n          terraform init\n          terraform apply -auto-approve\nNow, when you run this build in one of the repos and branches in the\nallowed_repos_branches variable, GitHub will be able to assume\nyour IAM role automatically, using temporary credentials, and Terraform\nwill authenticate to AWS using that IAM role, all without having to manage\nany credentials manually.\nResources and Data Sources\nThe next place you’ll run into secrets with your Terraform code is with\nresources and data sources. For example, you saw earlier in the chapter the\nexample of passing database credentials to the aws_db_instance\nresource:\nresource \"aws_db_instance\" \"example\" {\n  identifier_prefix   = \"terraform-up-and-running\"\n  engine              = \"mysql\"\n  allocated_storage   = 10\n  instance_class      = \"db.t2.micro\"\n  skip_final_snapshot = true\n  db_name             = var.db_name \n \n  # DO NOT DO THIS!!!\n  username = \"admin\"\n  password = \"password\"\n  # DO NOT DO THIS!!!\n}\nI’ve said it multiple times in this chapter already, but it’s such an important\npoint that it’s worth repeating again: storing those credentials in the code, as\nplain text, is a bad idea. So, what’s a better way to do it?' metadata={'original_pages_range': '325', 'source': '097_Resources_and_Data_Sources', 'file_path': '/mnt/c/Users/Carlos/Documents/Github/jupiter-iaa-azure/Scripts/RAG/../../data/optimized_chunks/Libro-TF/097_Resources_and_Data_Sources.pdf', 'num_pages': 1}", "page_content='There are three main techniques you can use:\nEnvironment variables\nEncrypted files\nSecret stores\nEnvironment variables\nThis first technique, which you saw back in Chapter 3, as well as earlier in\nthis chapter when talking about providers, keeps plain-text secrets out of\nyour code by taking advantage of Terraform’s native support for reading\nenvironment variables.\nTo use this technique, declare variables for the secrets you wish to pass in:\nvariable \"db_username\" {\n  description = \"The username for the database\"\n  type        = string\n  sensitive   = true\n} \n \nvariable \"db_password\" {\n  description = \"The password for the database\"\n  type        = string\n  sensitive   = true\n}\nJust as in Chapter 3, these variables are marked with sensitive =\ntrue to indicate they contain secrets (so Terraform won’t log the values\nwhen you run plan or apply), and these variables do not have a\ndefault (so as not to store secrets in plain text). Next, pass the variables\nto the Terraform resources that need those secrets:\nresource \"aws_db_instance\" \"example\" {\n  identifier_prefix   = \"terraform-up-and-running\"\n  engine              = \"mysql\"\n  allocated_storage   = 10\n  instance_class      = \"db.t2.micro\"\n  skip_final_snapshot = true\n  db_name             = var.db_name\n\n# Pass the secrets to the resource\n  username = var.db_username\n  password = var.db_password\n}\nNow you can pass in a value for each variable foo by setting the\nenvironment variable TF_VAR_foo:\n$  export TF_VAR_db_username=(DB_USERNAME) \n$  export TF_VAR_db_password=(DB_PASSWORD)\nPassing in secrets via environment variables helps you avoid storing secrets\nin plain text in your code, but it doesn’t answer an important question: How\ndo you store the secrets securely? One nice thing about using environment\nvariables is that they work with almost any type of secrets management\nsolution. For example, one option is to store the secrets in a personal secrets\nmanager (e.g., 1Password) and manually set those secrets as environment\nvariables in your terminal. Another option is to store the secrets in a\ncentralized secret store (e.g., HashiCorp Vault) and write a script that uses\nthat secret store’s API or CLI to read those secrets out and set them as\nenvironment variables.\nUsing environment variables has the following advantages:\nKeep plain-text secrets out of your code and version control system.\nStoring secrets is easy, as you can use just about any other secret\nmanagement solution. That is, if your company already has a way to\nmanage secrets, you can typically find a way to make it work with\nenvironment variables.\nRetrieving secrets is easy, as reading environment variables is\nstraightforward in every language.\nIntegrating with automated tests is easy, as you can easily set the\nenvironment variables to mock values.' metadata={'original_pages_range': '326-327', 'source': '098_Environment_variables', 'file_path': '/mnt/c/Users/Carlos/Documents/Github/jupiter-iaa-azure/Scripts/RAG/../../data/optimized_chunks/Libro-TF/098_Environment_variables.pdf', 'num_pages': 2}", "page_content='Using environment variables doesn’t cost any money, unlike some of\nthe other secret management solutions discussed later.\nUsing environment variables has the following drawbacks:\nNot everything is defined in the Terraform code itself. This makes\nunderstanding and maintaining the code harder. Everyone using your\ncode has to know to take extra steps to either manually set these\nenvironment variables or run a wrapper script.\nStandardizing secret management practices is harder. Since all the\nmanagement of secrets happens outside of Terraform, the code doesn’t\nenforce any security properties, and it’s possible someone is still\nmanaging the secrets in an insecure way (e.g., storing them in plain\ntext).\nSince the secrets are not versioned, packaged, and tested with your\ncode, configuration errors are more likely, such as adding a new secret\nin one environment (e.g., staging) but forgetting to add it in another\nenvironment (e.g., production).\nEncrypted files\nThe second technique relies on encrypting the secrets, storing the ciphertext\nin a file, and checking that file into version control.\nTo encrypt some data, such as some secrets in a file, you need an encryption\nkey. As mentioned earlier in this chapter, this encryption key is itself a\nsecret, so you need a secure way to store it. The typical solution is to either\nuse your cloud provider’s KMS (e.g., AWS KMS, Google KMS, Azure Key\nVault) or to use the PGP keys of one or more developers on your team.\nLet’s look at an example that uses AWS KMS. First, you’ll need to create a\nKMS Customer Managed Key (CMK), which is an encryption key that\nAWS manages for you. To create a CMK, you first have to define a key\npolicy, which is an IAM Policy that defines who can use that CMK. To keep\nthis example simple, let’s create a key policy that gives the current user\nadmin permissions over the CMK. You can fetch the current user’s\n\ninformation—their username, ARN, etc.—using the\naws_caller_identity data source:\nprovider \"aws\" {\n  region = \"us-east-2\"\n} \n \ndata \"aws_caller_identity\" \"self\" {}\nAnd now you can use the aws_caller_identity data source’s outputs\ninside an aws_iam_policy_document data source to create a key\npolicy that gives the current user admin permissions over the CMK:\ndata \"aws_iam_policy_document\" \"cmk_admin_policy\" { \n  statement {\n    effect    = \"Allow\"\n    resources = [\"*\"]\n    actions   = [\"kms:*\"] \n    principals {\n      type        = \"AWS\"\n      identifiers = [data.aws_caller_identity.self.arn] \n    } \n  }\n}\nNext, you can create the CMK using the aws_kms_key resource:\nresource \"aws_kms_key\" \"cmk\" {\n  policy = data.aws_iam_policy_document.cmk_admin_policy.json\n}\nNote that, by default, KMS CMKs are only identified by a long numeric\nidentifier (e.g., b7670b0e-ed67-28e4-9b15-0d61e1485be3), so\nit’s a good practice to also create a human-friendly alias for your CMK\nusing the aws_kms_alias resource:\nresource \"aws_kms_alias\" \"cmk\" {\n  name          = \"alias/kms-cmk-example\"\n  target_key_id = aws_kms_key.cmk.id\n}\n\nThe preceding alias will allow you to refer to your CMK as alias/kms-\ncmk-example when using the AWS API and CLI, rather than a long\nidentifier such as b7670b0e-ed67-28e4-9b15-0d61e1485be3.\nOnce you’ve created the CMK, you can start using it to encrypt and decrypt\ndata. Note that, by design, you’ll never be able to see (and, therefore, to\naccidentally leak) the underlying encryption key. Only AWS has access to\nthat encryption key, but you can make use of it by using the AWS API and\nCLI, as described next.\nFirst, create a file called db-creds.yml with some secrets in it, such as the\ndatabase credentials:\nusername: admin\npassword: password\nNote: do not check this file into version control, as you haven’t encrypted it\nyet! To encrypt this data, you can use the aws kms encrypt command\nand write the resulting ciphertext to a new file. Here’s a small Bash script\n(for Linux/Unix/macOS) called encrypt.sh that performs these steps using\nthe AWS CLI:\nCMK_ID=\"$1\"\nAWS_REGION=\"$2\"\nINPUT_FILE=\"$3\"\nOUTPUT_FILE=\"$4\" \n \necho \"Encrypting contents of $INPUT_FILE using CMK $CMK_ID...\"\nciphertext=$(aws kms encrypt \\ \n  --key-id \"$CMK_ID\" \\ \n  --region \"$AWS_REGION\" \\ \n  --plaintext \"fileb://$INPUT_FILE\" \\ \n  --output text \\ \n  --query CiphertextBlob) \n \necho \"Writing result to $OUTPUT_FILE...\"\necho \"$ciphertext\" > \"$OUTPUT_FILE\" \n \necho \"Done!\"\n\nHere’s how you can use encrypt.sh to encrypt the db-creds.yml file with the\nKMS CMK you created earlier and store the resulting ciphertext in a new\nfile called db-creds.yml.encrypted:\n$ ./encrypt.sh \\ \n  alias/kms-cmk-example \\ \n  us-east-2 \\ \n  db-creds.yml \\ \n  db-creds.yml.encrypted \n \nEncrypting contents of db-creds.yml using CMK alias/kms-cmk-\nexample... \nWriting result to db-creds.yml.encrypted... \nDone!\nYou can now delete db-creds.yml (the plain-text file) and safely check db-\ncreds.yml.encrypted (the encrypted file) into version control. At this point,\nyou have an encrypted file with some secrets inside of it, but how do you\nmake use of that file in your Terraform code?\nThe first step is to decrypt the secrets in this file using the\naws_kms_secrets data source:\ndata \"aws_kms_secrets\" \"creds\" { \n  secret {\n    name    = \"db\"\n    payload = file(\"${path.module}/db-creds.yml.encrypted\") \n  }\n}\nThe preceding code reads db-creds.yml.encrypted from disk using the file\nhelper function and, assuming you have permissions to access the\ncorresponding key in KMS, decrypts the contents. That gives you back the\ncontents of the original db-creds.yml file, so the next step is to parse the\nYAML as follows:\nlocals {\n  db_creds = \nyamldecode(data.aws_kms_secrets.creds.plaintext[\"db\"])\n}\n\nThis code pulls out the database secrets from the aws_kms_secrets\ndata source, parses the YAML, and stores the results in a local variable\ncalled db_creds. Finally, you can read the username and password from\ndb_creds and pass those credentials to the aws_db_instance\nresource:\nresource \"aws_db_instance\" \"example\" {\n  identifier_prefix   = \"terraform-up-and-running\"\n  engine              = \"mysql\"\n  allocated_storage   = 10\n  instance_class      = \"db.t2.micro\"\n  skip_final_snapshot = true\n  db_name             = var.db_name \n \n  # Pass the secrets to the resource\n  username = local.db_creds.username\n  password = local.db_creds.password\n}\nSo now you have a way to store secrets in an encrypted file, which are safe\nto check into version control, and you have a way to read those secrets back\nout of the file in your Terraform code automatically.\nOne thing to note with this approach is that working with encrypted files is\nawkward. To make a change, you have to locally decrypt the file with a\nlong aws kms decrypt command, make some edits, re-encrypt the file\nwith another long aws kms encrypt command, and the whole time, be\nextremely careful to not accidentally check the plain-text data into version\ncontrol or leave it sitting behind forever on your computer. This is a tedious\nand error-prone process.\nOne way to make this less awkward is to use an open source tool called\nsops. When you run sops <FILE>, sops will automatically decrypt FILE\nand open your default text editor with the plain-text contents. When you’re\ndone editing and exit the text editor, sops will automatically encrypt the\ncontents. This way, the encryption and decryption are mostly transparent,\nwith no need to run long aws kms commands and less chance of\naccidentally checking plain-text secrets into version control. As of 2022,\n\nsops can work with files encrypted via AWS KMS, GCP KMS, Azure Key\nVault, or PGP keys. Note that Terraform doesn’t yet have native support for\ndecrypting files that were encrypted by sops, so you’ll either need to use a\nthird-party provider such as carlpett/sops or, if you’re a Terragrunt user, you\ncan use the built-in sops_decrypt_file function.\nUsing encrypted files has the following advantages:\nKeep plain-text secrets out of your code and version control system.\nYour secrets are stored in an encrypted format in version control, so\nthey are versioned, packaged, and tested with the rest of your code.\nThis helps reduce configuration errors, such as adding a new secret in\none environment (e.g., staging) but forgetting to add it in another\nenvironment (e.g., production).\nRetrieving secrets is easy, assuming the encryption format you’re\nusing is natively supported by Terraform or a third-party plugin.\nIt works with a variety of different encryption options: AWS KMS,\nGCP KMS, PGP, etc.\nEverything is defined in the code. There are no extra manual steps or\nwrapper scripts required (although sops integration does require a\nthird-party plugin).\nUsing encrypted files has the following drawbacks:\nStoring secrets is harder. You either have to run lots of commands\n(e.g., aws kms encrypt) or use an external tool such as sops.\nThere’s a learning curve to using these tools correctly and securely.\nIntegrating with automated tests is harder, as you will need to do extra\nwork to make encryption keys and encrypted test data available for\nyour test environments.\nThe secrets are now encrypted, but as they are still stored in version\ncontrol, rotating and revoking secrets is hard. If anyone ever' metadata={'original_pages_range': '328-333', 'source': '099_Encrypted_files', 'file_path': '/mnt/c/Users/Carlos/Documents/Github/jupiter-iaa-azure/Scripts/RAG/../../data/optimized_chunks/Libro-TF/099_Encrypted_files.pdf', 'num_pages': 6}", "page_content='compromises the encryption key, they can go back and decrypt all the\nsecrets that were ever encrypted with it.\nThe ability to audit who accessed secrets is minimal. If you’re using a\ncloud key management service (e.g., AWS KMS), it will likely\nmaintain an audit log of who used an encryption key, but you won’t be\nable to tell what the key was actually used for (i.e., what secrets were\naccessed).\nMost managed key services cost a small amount of money. For\nexample, each key you store in AWS KMS costs $1/month, plus $0.03\nper 10,000 API calls, where each decryption and encryption operation\nrequires one API call. A typical usage pattern, where you have a small\nnumber of keys in KMS and your apps use those keys to decrypt\nsecrets during boot, usually costs $1–$10/month. For larger\ndeployments, where you have dozens of apps and hundreds of secrets,\nthe price is typically in the $10–$50/month range.\nStandardizing secret management practices is harder. Different\ndevelopers or teams may use different ways to store encryption keys or\nmanage encrypted files, and mistakes are relatively common, such as\nnot using encryption correctly or accidentally checking in a plain-text\nfile into version control.\nSecret stores\nThe third technique relies on storing your secrets in a centralized secret\nstore.\nSome of the more popular secret stores are AWS Secrets Manager, Google\nSecret Manager, Azure Key Vault, and HashiCorp Vault. Let’s look at an\nexample using AWS Secrets Manager. The first step is to store your\ndatabase credentials in AWS Secrets Manager, which you can do using the\nAWS Web Console, as shown in Figure 6-2.\n\nFigure 6-2. Store secrets in JSON format in AWS Secrets Manager.\n\nNote that the secrets in Figure 6-2 are in a JSON format, which is the\nrecommended format for storing data in AWS Secrets Manager.\nGo to the next step, and make sure to give the secret a unique name, such as\ndb-creds, as shown in Figure 6-3.\n\nFigure 6-3. Give the secret a unique name in AWS Secrets Manager.\n\nClick Next and Store to save the secret. Now, in your Terraform code, you\ncan use the aws_secretsmanager_secret_version data source to\nread the db-creds secret:\ndata \"aws_secretsmanager_secret_version\" \"creds\" {\n  secret_id = \"db-creds\"\n}\nSince the secret is stored as JSON, you can use the jsondecode function\nto parse the JSON into the local variable db_creds:\nlocals {\n  db_creds = jsondecode( \n    data.aws_secretsmanager_secret_version.creds.secret_string \n  )\n}\nAnd now you can read the database credentials from db_creds and pass\nthem into the aws_db_instance resource:\nresource \"aws_db_instance\" \"example\" {\n  identifier_prefix   = \"terraform-up-and-running\"\n  engine              = \"mysql\"\n  allocated_storage   = 10\n  instance_class      = \"db.t2.micro\"\n  skip_final_snapshot = true\n  db_name             = var.db_name \n \n  # Pass the secrets to the resource\n  username = local.db_creds.username\n  password = local.db_creds.password\n}\nUsing secret stores has the following advantages:\nKeep plain-text secrets out of your code and version control system.\nEverything is defined in the code itself. There are no extra manual\nsteps or wrapper scripts required.\nStoring secrets is easy, as you typically can use a web UI.\n\nSecret stores typically support rotating and revoking secrets, which is\nuseful in case a secret gets compromised. You can even enable rotation\non a scheduled basis (e.g., every 30 days) as a preventative measure.\nSecret stores typically support detailed audit logs that show you\nexactly who accessed what data.\nSecret stores make it easier to standardize all your secret practices, as\nthey enforce specific types of encryption, storage, access patterns, etc.\nUsing secret stores has the following drawbacks:\nSince the secrets are not versioned, packaged, and tested with your\ncode, configuration errors are more likely, such as adding a new secret\nin one environment (e.g., staging) but forgetting to add it in another\nenvironment (e.g., production).\nMost managed secret stores cost money. For example, AWS Secrets\nManager charges $0.40 per month for each secret you store, plus $0.05\nfor every 10,000 API calls you make to store or retrieve data. A typical\nusage pattern, where you have several dozen secrets stored across\nseveral environments and a handful of apps that read those secrets\nduring boot, usually costs around $10–$25/month. With larger\ndeployments, where you have dozens of apps reading hundreds of\nsecrets, the price can go up to hundreds of dollars per month.\nIf you’re using a self-managed secret store such as HashiCorp Vault,\nthen you’re both spending money to run the store (e.g., paying AWS\nfor 3–5 EC2 Instances to run Vault in a highly available mode) and\nspending time and money to have your developers deploy, configure,\nmanage, update, and monitor the store. Developer time is very\nexpensive, so depending on how much time they have to spend on\nsetting up and managing the secret store, this could cost you thousands\nof dollars per month.\nRetrieving secrets is harder, especially in automated environments\n(e.g., an app booting up and trying to read a database password), as' metadata={'original_pages_range': '334-339', 'source': '100_Secret_stores', 'file_path': '/mnt/c/Users/Carlos/Documents/Github/jupiter-iaa-azure/Scripts/RAG/../../data/optimized_chunks/Libro-TF/100_Secret_stores.pdf', 'num_pages': 6}", "page_content='you have to solve how to do secure authentication between multiple\nmachines.\nIntegrating with automated tests is harder, as much of the code you’re\ntesting now depends on a running, external system that either needs to\nbe mocked out or have test data stored in it.\nState Files and Plan Files\nThere are two more places where you’ll come across secrets when using\nTerraform:\nState files\nPlan files\nState files\nHopefully, this chapter has convinced you to not store your secrets in plain\ntext and provided you with some better alternatives. However, something\nthat catches many Terraform users off guard is that, no matter which\ntechnique you use, any secrets you pass into your Terraform resources and\ndata sources will end up in plain text in your Terraform state file!\nFor example, no matter where you read the database credentials from—\nenvironment variables, encrypted files, a centralized secret store—if you\npass those credentials to a resource such as aws_db_instance:\nresource \"aws_db_instance\" \"example\" {\n  identifier_prefix   = \"terraform-up-and-running\"\n  engine              = \"mysql\"\n  allocated_storage   = 10\n  instance_class      = \"db.t2.micro\"\n  skip_final_snapshot = true\n  db_name             = var.db_name \n \n  # Pass the secrets to the resource\n  username = local.db_creds.username\n  password = local.db_creds.password\n}' metadata={'original_pages_range': '340', 'source': '101_State_Files_and_Plan_Files_y_1_mas', 'file_path': '/mnt/c/Users/Carlos/Documents/Github/jupiter-iaa-azure/Scripts/RAG/../../data/optimized_chunks/Libro-TF/101_State_Files_and_Plan_Files_y_1_mas.pdf', 'num_pages': 1}", "page_content='then Terraform will store those credentials in your terraform.tfstate file, in\nplain text. This has been an open issue since 2014, with no clear plans for a\nfirst-class solution. There are some workarounds out there that can scrub\nsecrets from your state files, but these are brittle and likely to break with\neach new Terraform release, so I don’t recommend them.\nFor the time being, no matter which of the techniques discussed you end up\nusing to manage secrets, you must do the following:\nStore Terraform state in a backend that supports encryption\nInstead of storing your state in a local terraform.tfstate file and\nchecking it into version control, you should use one of the backends\nTerraform supports that natively supports encryption, such as S3, GCS,\nand Azure Blob Storage. These backends will encrypt your state files,\nboth in transit (e.g., via TLS) and on disk (e.g., via AES-256).\nStrictly control who can access your Terraform backend\nSince Terraform state files may contain secrets, you’ll want to control\nwho has access to your backend with at least as much care as you\ncontrol access to the secrets themselves. For example, if you’re using S3\nas a backend, you’ll want to configure an IAM Policy that solely grants\naccess to the S3 bucket for production to a small handful of trusted\ndevs, or perhaps solely just the CI server you use to deploy to prod.\nPlan files\nYou’ve seen the terraform plan command many times. One feature\nyou may not have seen yet is that you can store the output of the plan\ncommand (the “diff”) in a file:\n$ terraform plan -out=example.plan\nThe preceding command stores the plan in a file called example.plan. You\ncan then run the apply command on this saved plan file to ensure that\nTerraform applies exactly the changes you saw originally:' metadata={'original_pages_range': '341', 'source': '102_Plan_files', 'file_path': '/mnt/c/Users/Carlos/Documents/Github/jupiter-iaa-azure/Scripts/RAG/../../data/optimized_chunks/Libro-TF/102_Plan_files.pdf', 'num_pages': 1}", "page_content='$ terraform apply example.plan\nThis is a handy feature of Terraform, but an important caveat applies: just as\nwith Terraform state, any secrets you pass into your Terraform resources\nand data sources will end up in plain text in your Terraform plan files! For\nexample, if you ran plan on the aws_db_instance code, and saved a\nplan file, the plan file would contain the database username and password,\nin plain text.\nTherefore, if you’re going to use plan files, you must do the following:\nEncrypt your Terraform plan files\nIf you’re going to save your plan files, you’ll need to find a way to\nencrypt those files, both in transit (e.g., via TLS) and on disk (e.g., via\nAES-256). For example, you could store plan files in an S3 bucket,\nwhich supports both types of encryption.\nStrictly control who can access your plan files\nSince Terraform plan files may contain secrets, you’ll want to control\nwho has access to them with at least as much care as you control access\nto the secrets themselves. For example, if you’re using S3 to store your\nplan files, you’ll want to configure an IAM Policy that solely grants\naccess to the S3 bucket for production to a small handful of trusted\ndevs, or perhaps solely just the CI server you use to deploy to prod.\nConclusion\nHere are your key takeaways from this chapter. First, if you remember\nnothing else from this chapter, please remember this: you should not store\nsecrets in plain text.\nSecond, to pass secrets to providers, human users can use personal secrets\nmanagers and set environment variables, and machine users can use stored\ncredentials, IAM roles, or OIDC. See Table 6-2 for the trade-offs between\nthe machine user options.\n\nTable 6-2. A comparison of methods for machine users (e.g., a CI server) to\npass secrets to Terraform providers\nStored\ncredentials IAM roles OIDC\nExample CircleCI Jenkins on an EC2\nInstance\nGitHub Actions\nAvoid manually managing\ncredentials\nx ✓ ✓ \nAvoid using permanent\ncredentials\nx ✓ ✓ \nWorks inside of cloud providerx ✓ x\nWorks outside of cloud\nprovider\n✓ x ✓ \nWidely supported as of 2022✓ ✓ x\nThird, to pass secrets to resources and data sources, use environment\nvariables, encrypted files, or centralized secret stores. See Table 6-3 for the\ntrade-offs between these different options.\n\nTable 6-3. A comparison of methods for passing secrets to Terraform\nresources and data sources\nEnvironment\nvariables Encrypted files\nCentralized\nsecret stores\nKeeps plain-text secrets out of\ncode\n✓ ✓ ✓ \nAll secrets management defined\nas code\nx ✓ ✓ \nAudit log for access to encryption\nkeys\nx ✓ ✓ \nAudit log for access to individual\nsecrets\nx x ✓ \nRotating or revoking secrets is\neasy\nx x ✓ \nStandardizing secrets\nmanagement is easy\nx x ✓ \nSecrets are versioned with the\ncode\nx ✓ x\nStoring secrets is easy ✓ x ✓ \nRetrieving secrets is easy ✓ ✓ x\nIntegrating with automated\ntesting is easy\n✓ x x\nCost 0 $ $$$\nAnd finally, fourth, no matter how you pass secrets to resources and data\nstores, remember that Terraform will store those secrets in your state files\nand plan files, in plain text, so make sure to always encrypt those files (in\ntransit and at rest) and to strictly control access to them.\nNow that you understand how to manage secrets when working with\nTerraform, including how to securely pass secrets to Terraform providers,\n\nlet’s move on to Chapter 7, where you’ll learn how to use Terraform in\ncases where you have multiple providers (e.g., multiple regions, multiple\naccounts, multiple clouds).\n1 Note that in most Linux/Unix/macOS shells, every command you type is stored on disk in\nsome sort of history file (e.g., ~/.bash_history). That’s why the export commands shown\nhere have a leading space: if you start your command with a space, most shells will skip\nwriting that command to the history file. Note that you might need to set the HISTCONTROL\nenvironment variable to “ignoreboth” to enable this if your shell doesn’t enable it by default.\n2 By default, the instance metadata endpoint is open to all OS users running on your EC2\nInstances. I recommend locking this endpoint down so that only specific OS users can access\nit: e.g., if you’re running an app on the EC2 Instance as user app, you could use iptables or\nnftables to only allow app to access the instance metadata endpoint. That way, if an\nattacker finds some vulnerability and is able to execute code on your instance, they will only\nbe able to access the IAM role permissions if they are able to authenticate as user app (rather\nthan as any user). Better still, if you only need the IAM role permissions during boot (e.g., to\nread a database password), you could disable the instance metadata endpoint entirely after\nboot, so an attacker who gets access later can’t use the endpoint at all.\n3 At the time this book was written, OIDC support between GitHub Actions and AWS was\nfairly new and the details subject to change. Make sure to check the latest GitHub OIDC\ndocumentation for the latest updates.' metadata={'original_pages_range': '342-345', 'source': '103_Conclusion', 'file_path': '/mnt/c/Users/Carlos/Documents/Github/jupiter-iaa-azure/Scripts/RAG/../../data/optimized_chunks/Libro-TF/103_Conclusion.pdf', 'num_pages': 4}", "page_content='Chapter 7. Working with\nMultiple Providers\nSo far, almost every single example in this book has included just a single\nprovider block:\nprovider \"aws\" {\n  region = \"us-east-2\"\n}\nThis provider block configures your code to deploy to a single AWS\nregion in a single AWS account. This raises a few questions:\nWhat if you need to deploy to multiple AWS regions?\nWhat if you need to deploy to multiple AWS accounts?\nWhat if you need to deploy to other clouds, such as Azure or GCP?\nTo answer these questions, this chapter takes a deeper look at Terraform\nproviders:\nWorking with one provider\nWorking with multiple copies of the same provider\nWorking with multiple different providers\nWorking with One Provider\nSo far, you’ve been using providers somewhat “magically.” That works\nwell enough for simple examples with one basic provider, but if you want to\nwork with multiple regions, accounts, clouds, etc., you’ll need to go deeper.' metadata={'original_pages_range': '346', 'source': '104_7._Working_with_Multiple_Providers_y_1_mas', 'file_path': '/mnt/c/Users/Carlos/Documents/Github/jupiter-iaa-azure/Scripts/RAG/../../data/optimized_chunks/Libro-TF/104_7._Working_with_Multiple_Providers_y_1_mas.pdf', 'num_pages': 1}", "page_content='Let’s start by taking a closer look at a single provider to better understand\nhow it works:\nWhat is a provider?\nHow do you install providers?\nHow do you use providers?\nWhat Is a Provider?\nWhen I first introduced providers in Chapter 2, I described them as the\nplatforms Terraform works with: e.g., AWS, Azure, Google Cloud,\nDigitalOcean, etc. So how does Terraform interact with these platforms?\nUnder the hood, Terraform consists of two parts:\nCore\nThis is the terraform binary, and it provides all the basic\nfunctionality in Terraform that is used by all platforms, such as a\ncommand-line interface (i.e., plan, apply, etc.), a parser and\ninterpreter for Terraform code (HCL), the ability to build a dependency\ngraph from resources and data sources, logic to read and write state\nfiles, and so on. Under the hood, the code is written in Go and lives in\nan open source GitHub repo owned and maintained by HashiCorp.\nProviders\nTerraform providers are plugins for the Terraform core. Each plugin is\nwritten in Go to implement a specific interface, and the Terraform core\nknows how to install and execute the plugin. Each of these plugins is\ndesigned to work with some platform in the outside world, such as\nAWS, Azure, or Google Cloud. The Terraform core communicates with\nplugins via remote procedure calls (RPCs), and those plugins, in turn,\ncommunicate with their corresponding platforms via the network (e.g.,\nvia HTTP calls), as shown in Figure 7-1.\n\nThe code for each plugin typically lives in its own repo. For example,\nall the AWS functionality you’ve been using in the book so far comes\nfrom a plugin called the Terraform AWS Provider (or just AWS\nProvider for short) that lives in its own repo. Although HashiCorp\ncreated most of the initial providers, and still helps to maintain many of\nthem, these days, much of the work for each provider is done by the\ncompany that owns the underlying platform: e.g., AWS employees work\non the AWS Provider, Microsoft employees work on the Azure\nprovider, Google employees work on the Google Cloud provider, and so\non.\nFigure 7-1. The interaction between the Terraform core, providers, and the outside world.' metadata={'original_pages_range': '347-348', 'source': '105_What_Is_a_Provider', 'file_path': '/mnt/c/Users/Carlos/Documents/Github/jupiter-iaa-azure/Scripts/RAG/../../data/optimized_chunks/Libro-TF/105_What_Is_a_Provider.pdf', 'num_pages': 2}", "page_content='Each provider claims a specific prefix and exposes one or more\nresources and data sources whose names include that prefix: e.g., all the\nresources and data sources from the AWS Provider use the aws_ prefix\n(e.g., aws_instance, aws_autoscaling_group, aws_ami),\nall the resources and data sources from the Azure provider use the\nazurerm_ prefix (e.g., azurerm_virtual_machine,\nazurerm_virtual_machine_scale_set, azurerm_image),\nand so on.\nHow Do You Install Providers?\nFor official Terraform providers, such as the ones for AWS, Azure, and\nGoogle Cloud, it’s enough to just add a provider block to your code:\nprovider \"aws\" {\n  region = \"us-east-2\"\n}\nAs soon as you run terraform init, Terraform automatically\ndownloads the code for the provider:\n$ terraform init \n \nInitializing provider plugins... \n- Finding hashicorp/aws versions matching \"4.19.0\"... \n- Installing hashicorp/aws v4.19.0... \n- Installed hashicorp/aws v4.19.0 (signed by HashiCorp)\nThis is a bit magical, isn’t it? How does Terraform know what provider you\nwant? Or which version you want? Or where to download it from?\nAlthough it’s OK to rely on this sort of magic for learning and\nexperimenting, when writing production code, you’ll probably want a bit\nmore control over how Terraform installs providers. Do this by adding a\nrequired_providers block, which has the following syntax:\nterraform { \n  required_providers {\n1\n\n<LOCAL_NAME> = {\n      source  = \"<URL>\"\n      version = \"<VERSION>\" \n    } \n  }\n}\nwhere:\nLOCAL_NAME\nThis is the local name to use for the provider in this module. You must\ngive each provider a unique name, and you use that name in the\nprovider block configuration. In almost all cases, you’ll use the\npreferred local name of that provider: e.g., for the AWS Provider, the\npreferred local name is aws, which is why you write the provider block\nas provider \"aws\" { … }. However, in rare cases, you may end\nup with two providers that have the same preferred local name—e.g.,\ntwo providers that both deal with HTTP requests and have a preferred\nlocal name of http—so you can use this local name to disambiguate\nbetween them.\nURL\nThis is the URL from where Terraform should download the provider,\nin the format [<HOSTNAME>/]<NAMESPACE>/<TYPE>, where\nHOSTNAME is the hostname of a Terraform Registry that distributes the\nprovider, NAMESPACE is the organizational namespace (typically, a\ncompany name), and TYPE is the name of the platform this provider\nmanages (typically, TYPE is the preferred local name). For example, the\nfull URL for the AWS Provider, which is hosted in the public Terraform\nRegistry, is registry.terraform.io/hashicorp/aws.\nHowever, note that HOSTNAME is optional, and if you omit it,\nTerraform will by default download the provider from the public\nTerraform Registry, so the shorter and more common way to specify the\nexact same AWS Provider URL is hashicorp/aws. You typically\nonly include HOSTNAME for custom providers that you’re downloading\n\nfrom private Terraform Registries (e.g., a private Registry you’re\nrunning in Terraform Cloud or Terraform Enterprise).\nVERSION\nThis is a version constraint. For example, you could set it to a specific\nversion, such as 4.19.0, or to a version range, such as > 4.0, <\n4.3. You’ll learn more about how to handle versioning in Chapter 8.\nFor example, to install version 4.x of the AWS Provider, you can use the\nfollowing code:\nterraform { \n  required_providers {\n    aws = {\n      source  = \"hashicorp/aws\"\n      version = \"~> 4.0\" \n    } \n  }\n}\nSo now you can finally understand the magical provider installation\nbehavior you saw earlier. If you add a new provider block named foo\nto your code, and you don’t specify a required_providers block,\nwhen you run terraform init, Terraform will automatically do the\nfollowing:\nTry to download provider foo with the assumption that the\nHOSTNAME is the public Terraform Registry and that the NAMESPACE\nis hashicorp, so the download URL is\nregistry.terraform.io/hashicorp/foo.\nIf that’s a valid URL, install the latest version of the foo provider\navailable at that URL.\nIf you want to install any provider not in the hashicorp namespace (e.g.,\nif you want to use providers from Datadog, Cloudflare, or Confluent, or a\ncustom provider you built yourself), or you want to control the version of' metadata={'original_pages_range': '349-351', 'source': '106_How_Do_You_Install_Providers', 'file_path': '/mnt/c/Users/Carlos/Documents/Github/jupiter-iaa-azure/Scripts/RAG/../../data/optimized_chunks/Libro-TF/106_How_Do_You_Install_Providers.pdf', 'num_pages': 3}", "page_content='the provider you use, you will need to include a required_providers\nblock.\nALWAYS INCLUDE REQUIRED_PROVIDERS\nAs you’ll learn in Chapter 8, it’s important to control the version of the provider you\nuse, so I recommend always including a required_providers block in your code.\nHow Do You Use Providers?\nWith this new knowledge about providers, let’s revisit how to use them. The\nfirst step is to add a required_providers block to your code to\nspecify which provider you want to use:\nterraform { \n  required_providers {\n    aws = {\n      source  = \"hashicorp/aws\"\n      version = \"~> 4.0\" \n    } \n  }\n}\nNext, you add a provider block to configure that provider:\nprovider \"aws\" {\n  region = \"us-east-2\"\n}\nSo far, you’ve only been configuring the region to use in the AWS\nProvider, but there are many other settings you can configure. Always\ncheck your provider’s documentation for the details: typically, this\ndocumentation lives in the same Registry you use to download the provider\n(the one in the source URL). For example, the documentation for the\nAWS Provider is in the public Terraform Registry. This documentation will\ntypically explain how to configure the provider to work with different users,\nroles, regions, accounts, and so on.' metadata={'original_pages_range': '352', 'source': '107_How_Do_You_Use_Providers', 'file_path': '/mnt/c/Users/Carlos/Documents/Github/jupiter-iaa-azure/Scripts/RAG/../../data/optimized_chunks/Libro-TF/107_How_Do_You_Use_Providers.pdf', 'num_pages': 1}", "page_content='Once you’ve configured a provider, all the resources and data sources from\nthat provider (all the ones with the same prefix) that you put into your code\nwill automatically use that configuration. So, for example, when you set the\nregion in the aws provider to us-east-2, all the aws_ resources to your\ncode will automatically deploy into us-east-2.\nBut what if you want some of those resources to deploy into us-east-2\nand some into a different region, such as us-west-1? Or what if you\nwant to deploy some resources to a completely different AWS account? To\ndo that, you’ll have to learn how to configure multiple copies of the same\nprovider, as discussed in the next section.\nWorking with Multiple Copies of the Same\nProvider\nTo understand how to work with multiple copies of the same provider, let’s\nlook at a few of the common cases where this comes up:\nWorking with multiple AWS regions\nWorking with multiple AWS accounts\nCreating modules that can work with multiple providers\nWorking with Multiple AWS Regions\nMost cloud providers allow you to deploy into datacenters (“regions”) all\nover the world, but when you configure a Terraform provider, you typically\nconfigure it to deploy into just one of those regions. For example, so far\nyou’ve been deploying into just a single AWS region, us-east-2:\nprovider \"aws\" {\n  region = \"us-east-2\"\n}' metadata={'original_pages_range': '353', 'source': '108_Working_with_Multiple_Copies_of_the_Same_Provider', 'file_path': '/mnt/c/Users/Carlos/Documents/Github/jupiter-iaa-azure/Scripts/RAG/../../data/optimized_chunks/Libro-TF/108_Working_with_Multiple_Copies_of_the_Same_Provider.pdf', 'num_pages': 1}", "page_content='Once you’ve configured a provider, all the resources and data sources from\nthat provider (all the ones with the same prefix) that you put into your code\nwill automatically use that configuration. So, for example, when you set the\nregion in the aws provider to us-east-2, all the aws_ resources to your\ncode will automatically deploy into us-east-2.\nBut what if you want some of those resources to deploy into us-east-2\nand some into a different region, such as us-west-1? Or what if you\nwant to deploy some resources to a completely different AWS account? To\ndo that, you’ll have to learn how to configure multiple copies of the same\nprovider, as discussed in the next section.\nWorking with Multiple Copies of the Same\nProvider\nTo understand how to work with multiple copies of the same provider, let’s\nlook at a few of the common cases where this comes up:\nWorking with multiple AWS regions\nWorking with multiple AWS accounts\nCreating modules that can work with multiple providers\nWorking with Multiple AWS Regions\nMost cloud providers allow you to deploy into datacenters (“regions”) all\nover the world, but when you configure a Terraform provider, you typically\nconfigure it to deploy into just one of those regions. For example, so far\nyou’ve been deploying into just a single AWS region, us-east-2:\nprovider \"aws\" {\n  region = \"us-east-2\"\n}\n\nWhat if you wanted to deploy into multiple regions? For example, how\ncould you deploy some resources into us-east-2 and other resources\ninto us-west-1? You might be tempted to solve this by defining two\nprovider configurations, one for each region:\nprovider \"aws\" {\n  region = \"us-east-2\"\n} \n \nprovider \"aws\" {\n  region = \"us-west-1\"\n}\nBut now there’s a new problem: How do you specify which of these\nprovider configurations each of your resources, data sources, and\nmodules should use? Let’s look at data sources first. Imagine you had two\ncopies of the aws_region data source, which returns the current AWS\nregion:\ndata \"aws_region\" \"region_1\" {\n} \n \ndata \"aws_region\" \"region_2\" {\n}\nHow do you get the region_1 data source to use the us-east-2\nprovider and the region_2 data source to use the us-west-1 provider?\nThe solution is to add an alias to each provider:\nprovider \"aws\" {\n  region = \"us-east-2\"\n  alias  = \"region_1\"\n} \n \nprovider \"aws\" {\n  region = \"us-west-1\"\n  alias  = \"region_2\"\n}\n\nAn alias is a custom name for the provider, which you can explicitly\npass to individual resources, data sources, and modules to get them to use\nthe configuration in that particular provider. To tell those aws_region\ndata sources to use a specific provider, you set the provider parameter as\nfollows:\ndata \"aws_region\" \"region_1\" {\n  provider = aws.region_1\n} \n \ndata \"aws_region\" \"region_2\" {\n  provider = aws.region_2\n}\nAdd some output variables so you can check that this is working:\noutput \"region_1\" {\n  value       = data.aws_region.region_1.name\n  description = \"The name of the first region\"\n} \n \noutput \"region_2\" {\n  value       = data.aws_region.region_2.name\n  description = \"The name of the second region\"\n}\nAnd run apply:\n$ terraform apply \n \n(...) \n \nOutputs: \n \nregion_1 = \"us-east-2\" \nregion_2 = \"us-west-1\"\nAnd there you go: each of the aws_region data sources is now using a\ndifferent provider and, therefore, running against a different AWS region.\nThe same technique of setting the provider parameter works with\n\nresources too. For example, here’s how you can deploy two EC2 Instances\nin different regions:\nresource \"aws_instance\" \"region_1\" {\n  provider = aws.region_1 \n \n  # Note different AMI IDs!!\n  ami           = \"ami-0fb653ca2d3203ac1\"\n  instance_type = \"t2.micro\"\n} \n \nresource \"aws_instance\" \"region_2\" {\n  provider = aws.region_2 \n \n  # Note different AMI IDs!!\n  ami           = \"ami-01f87c43e618bf8f0\"\n  instance_type = \"t2.micro\"\n}\nNotice how each aws_instance resource sets the provider parameter\nto ensure it deploys into the proper region. Also, note that the ami\nparameter has to be different on the two aws_instance resources: that’s\nbecause AMI IDs are unique to each AWS region, so the ID for Ubuntu\n20.04 in us-east-2 is different than for Ubuntu 20.04 in us-west-1.\nHaving to look up and manage these AMI IDs manually is tedious and error\nprone. Fortunately, there’s a better alternative: use the aws_ami data\nsource that, given a set of filters, can find AMI IDs for you automatically.\nHere’s how you can use this data source twice, once in each region, to look\nup Ubuntu 20.04 AMI IDs:\ndata \"aws_ami\" \"ubuntu_region_1\" {\n  provider = aws.region_1 \n \n  most_recent = true\n  owners      = [\"099720109477\"] # Canonical \n \n  filter {\n    name   = \"name\"\n    values = [\"ubuntu/images/hvm-ssd/ubuntu-focal-20.04-amd64-\nserver-*\"] \n  }\n\n} \n \ndata \"aws_ami\" \"ubuntu_region_2\" {\n  provider = aws.region_2 \n \n  most_recent = true\n  owners      = [\"099720109477\"] # Canonical \n \n  filter {\n    name   = \"name\"\n    values = [\"ubuntu/images/hvm-ssd/ubuntu-focal-20.04-amd64-\nserver-*\"] \n  }\n}\nNotice how each data source sets the provider parameter to ensure it’s\nlooking up the AMI ID in the proper region. Go back to the\naws_instance code and update the ami parameter to use the output of\nthese data sources instead of the hardcoded values:\nresource \"aws_instance\" \"region_1\" {\n  provider = aws.region_1 \n \n  ami           = data.aws_ami.ubuntu_region_1.id\n  instance_type = \"t2.micro\"\n} \n \nresource \"aws_instance\" \"region_2\" {\n  provider = aws.region_2 \n \n  ami           = data.aws_ami.ubuntu_region_2.id\n  instance_type = \"t2.micro\"\n}\nMuch better. Now, no matter what region you deploy into, you’ll\nautomatically get the proper AMI ID for Ubuntu. To check that these EC2\nInstances are really deploying into different regions, add output variables\nthat show you which availability zone (each of which is in one region) each\ninstance was actually deployed into:\noutput \"instance_region_1_az\" {\n  value       = aws_instance.region_1.availability_zone\n\ndescription = \"The AZ where the instance in the first region \ndeployed\"\n} \n \noutput \"instance_region_2_az\" {\n  value       = aws_instance.region_2.availability_zone\n  description = \"The AZ where the instance in the second region \ndeployed\"\n}\nAnd now run apply:\n$ terraform apply \n \n(...) \n \nOutputs: \n \ninstance_region_1_az = \"us-east-2a\" \ninstance_region_2_az = \"us-west-1b\"\nOK, so now you know how to deploy data sources and resources into\ndifferent regions. What about modules? For example, in Chapter 3, you\nused Amazon RDS to deploy a single instance of a MySQL database in the\nstaging environment (stage/data-stores/mysql):\nprovider \"aws\" {\n  region = \"us-east-2\"\n} \n \nresource \"aws_db_instance\" \"example\" {\n  identifier_prefix   = \"terraform-up-and-running\"\n  engine              = \"mysql\"\n  allocated_storage   = 10\n  instance_class      = \"db.t2.micro\"\n  skip_final_snapshot = true \n \n  username = var.db_username\n  password = var.db_password\n}\nThis is fine in staging, but in production, a single database is a single point\nof failure. Fortunately, Amazon RDS natively supports replication, where\n\nyour data is automatically copied from a primary database to a secondary\ndatabase—a read-only replica—which is useful for scalability and as a\nstandby in case the primary goes down. You can even run the replica in a\ntotally different AWS region, so if one region goes down (e.g., there’s a\nmajor outage in us-east-2), you can switch to the other region (e.g.,\nus-west-1).\nLet’s turn that MySQL code in the staging environment into a reusable\nmysql module that supports replication. First, copy all the contents of\nstage/data-stores/mysql, which should include main.tf, variables.tf, and\noutputs.tf, into a new modules/data-stores/mysql folder. Next, open\nmodules/data-stores/mysql/variables.tf and expose two new variables:\nvariable \"backup_retention_period\" {\n  description = \"Days to retain backups. Must be > 0 to enable \nreplication.\"\n  type        = number\n  default     = null\n} \n \nvariable \"replicate_source_db\" {\n  description = \"If specified, replicate the RDS database at the \ngiven ARN.\"\n  type        = string\n  default     = null\n}\nAs you’ll see shortly, you’ll set the backup_retention_period\nvariable on the primary database to enable replication, and you’ll set the\nreplicate_source_db variable on the secondary database to turn it\ninto a replica. Open up modules/data-stores/mysql/main.tf, and update the\naws_db_instance resource as follows:\n1. Pass the backup_retention_period and\nreplicate_source_db variables into parameters of the same\nname in the aws_db_instance resource.\n2. If a database instance is a replica, AWS does not allow you to set the\nengine, db_name, username, or password parameters, as those\n\nare all inherited from the primary. So you must add some conditional\nlogic to the aws_db_instance resource to not set those parameters\nwhen the replicate_source_db variable is set.\nHere’s what the resource should look like after the changes:\nresource \"aws_db_instance\" \"example\" {\n  identifier_prefix   = \"terraform-up-and-running\"\n  allocated_storage   = 10\n  instance_class      = \"db.t2.micro\"\n  skip_final_snapshot = true \n \n  # Enable backups\n  backup_retention_period = var.backup_retention_period \n \n  # If specified, this DB will be a replica\n  replicate_source_db = var.replicate_source_db \n \n  # Only set these params if replicate_source_db is not set\n  engine   = var.replicate_source_db == null ? \"mysql\" : null\n  db_name  = var.replicate_source_db == null ? var.db_name : null\n  username = var.replicate_source_db == null ? var.db_username : \nnull\n  password = var.replicate_source_db == null ? var.db_password : \nnull\n}\nNote that for replicas, this implies that the db_name, db_username, and\ndb_password input variables in this module should be optional, so it’s a\ngood idea to go back to modules/data-stores/mysql/variables.tf and set the\ndefault for those variables to null:\nvariable \"db_name\" {\n  description = \"Name for the DB.\"\n  type        = string\n  default     = null\n} \n \nvariable \"db_username\" {\n  description = \"Username for the DB.\"\n  type        = string\n  sensitive   = true\n  default     = null\n\n} \n \nvariable \"db_password\" {\n  description = \"Password for the DB.\"\n  type        = string\n  sensitive   = true\n  default     = null\n}\nTo use the replicate_source_db variable, you’ll need set it to the\nARN of another RDS database, so you should also update modules/data-\nstores/mysql/outputs.tf to add the database ARN as an output variable:\noutput \"arn\" {\n  value       = aws_db_instance.example.arn\n  description = \"The ARN of the database\"\n}\nOne more thing: you should add a required_providers block to this\nmodule to specify that this module expects to use the AWS Provider, and to\nspecify which version of the provider the module expects.\nterraform { \n  required_providers {\n    aws = {\n      source  = \"hashicorp/aws\"\n      version = \"~> 4.0\" \n    } \n  }\n}\nYou’ll see in a moment why this is important when working with multiple\nregions, too!\nOK, you can now use this mysql module to deploy a MySQL primary and\na MySQL replica in the production environment. First, create\nlive/prod/data-stores/mysql/variables.tf to expose input variables for the\ndatabase username and password (so you can pass these secrets in as\nenvironment variables, as discussed in Chapter 6):\n\nvariable \"db_username\" {\n  description = \"The username for the database\"\n  type        = string\n  sensitive   = true\n} \n \nvariable \"db_password\" {\n  description = \"The password for the database\"\n  type        = string\n  sensitive   = true\n}\nNext, create live/prod/data-stores/mysql/main.tf, and use the mysql\nmodule to configure the primary as follows:\nmodule \"mysql_primary\" {\n  source = \"../../../../modules/data-stores/mysql\" \n \n  db_name     = \"prod_db\"\n  db_username = var.db_username\n  db_password = var.db_password \n \n  # Must be enabled to support replication\n  backup_retention_period = 1\n}\nNow, add a second usage of the mysql module to create a replica:\nmodule \"mysql_replica\" {\n  source = \"../../../../modules/data-stores/mysql\" \n \n  # Make this a replica of the primary\n  replicate_source_db = module.mysql_primary.arn\n}\nNice and short! All you’re doing is passing the ARN of the primary\ndatabase into the replicate_source_db parameter, which should spin\nup an RDS database as a replica.\nThere’s just one problem: How do you tell the code to deploy the primary\nand replica into different regions? To do so, create two provider blocks,\neach with its own alias:\n\nprovider \"aws\" {\n  region = \"us-east-2\"\n  alias  = \"primary\"\n} \n \nprovider \"aws\" {\n  region = \"us-west-1\"\n  alias  = \"replica\"\n}\nTo tell a module which providers to use, you set the providers\nparameter. Here’s how you configure the MySQL primary to use the\nprimary provider (the one in us-east-2):\nmodule \"mysql_primary\" {\n  source = \"../../../../modules/data-stores/mysql\" \n \n  providers = {\n    aws = aws.primary \n  } \n \n  db_name     = \"prod_db\"\n  db_username = var.db_username\n  db_password = var.db_password \n \n  # Must be enabled to support replication\n  backup_retention_period = 1\n}\nAnd here is how you configure the MySQL replica to use the replica\nprovider (the one in us-west-1):\nmodule \"mysql_replica\" {\n  source = \"../../../../modules/data-stores/mysql\" \n \n  providers = {\n    aws = aws.replica \n  } \n \n  # Make this a replica of the primary\n  replicate_source_db = module.mysql_primary.arn\n}\n\nNotice that with modules, the providers (plural) parameter is a map,\nwhereas with resources and data sources, the provider (singular)\nparameter is a single value. That’s because each resource and data source\ndeploys into exactly one provider, but a module may contain multiple data\nsources and resources and use multiple providers (you’ll see an example of\nmultiple providers in a module later). In the providers map you pass to\na module, the key must match the local name of the provider in the\nrequired_providers map within the module (in this case, both are set\nto aws). This is yet another reason defining required_providers\nexplicitly is a good idea in just about every module.\nAlright, the last step is to create live/prod/data-stores/mysql/outputs.tf with\nthe following output variables:\noutput \"primary_address\" {\n  value       = module.mysql_primary.address\n  description = \"Connect to the primary database at this \nendpoint\"\n} \n \noutput \"primary_port\" {\n  value       = module.mysql_primary.port\n  description = \"The port the primary database is listening on\"\n} \n \noutput \"primary_arn\" {\n  value       = module.mysql_primary.arn\n  description = \"The ARN of the primary database\"\n} \n \noutput \"replica_address\" {\n  value       = module.mysql_replica.address\n  description = \"Connect to the replica database at this \nendpoint\"\n} \n \noutput \"replica_port\" {\n  value       = module.mysql_replica.port\n  description = \"The port the replica database is listening on\"\n} \n \noutput \"replica_arn\" {\n  value       = module.mysql_replica.arn\n\ndescription = \"The ARN of the replica database\"\n}\nAnd now you’re finally ready to deploy! Note that running apply to spin\nup a primary and replica can take a long time, some 20–30 minutes, so be\npatient:\n$ terraform apply \n \n(...) \n \nApply complete! Resources: 2 added, 0 changed, 0 destroyed. \n \nOutputs: \nprimary_address = \"terraform-up-and-running.cmyd6qwb.us-east-\n2.rds.amazonaws.com\" \nprimary_arn     = \"arn:aws:rds:us-east-\n2:111111111111:db:terraform-up-and-running\" \nprimary_port    = 3306 \nreplica_address = \"terraform-up-and-running.drctpdoe.us-west-\n1.rds.amazonaws.com\" \nreplica_arn     = \"arn:aws:rds:us-west-\n1:111111111111:db:terraform-up-and-running\" \nreplica_port    = 3306\nAnd there you have it, cross-region replication! You can log into the RDS\nConsole to confirm replication is working. As shown in Figure 7-2, you\nshould see a primary in us-east-2 and a replica in us-west-1.\n\n\n\nFigure 7-2. The RDS console shows a primary database in us-east-2 and a replica in us-\nwest-1.\nAs an exercise for the reader, I leave it up to you to update the staging\nenvironment (stage/data-stores/mysql) to use your mysql module\n(modules/data-stores/mysql) as well, but to configure it without replication,\nas you don’t usually need that level of availability in pre-production\nenvironments.\nAs you can see in these examples, by using multiple providers with aliases,\ndeploying resources across multiple regions with Terraform is pretty easy.\nHowever, I want to give two warnings before moving on:\nWarning 1: Multiregion is hard\nTo run infrastructure in multiple regions around the world, especially in\n“active-active” mode, where more than one region is actively\nresponding to user requests at the same time (as opposed to one region\nbeing a standby), there are many hard problems to solve, such as\ndealing with latency between regions, deciding between one writer\n(which means you have lower availability and higher latency) or\nmultiple writers (which means you have either eventual consistency or\nsharding), figuring out how to generate unique IDs (the standard auto\nincrement ID in most databases no longer suffices), working to meet\nlocal data regulations, and so on. These challenges are all beyond the\nscope of the book, but I figured I’d at least mention them to make it\nclear that multiregion deployments in the real world are not just a matter\nof tossing a few provider aliases into your Terraform code!\nWarning 2: Use aliases sparingly\nAlthough it’s easy to use aliases with Terraform, I would caution against\nusing them too often, especially when setting up multiregion\ninfrastructure. One of the main reasons to set up multiregion\ninfrastructure is so you can be resilient to the outage of one region: e.g.,\nif us-east-2 goes down, your infrastructure in us-west-1 can\nkeep running. But if you use a single Terraform module that uses aliases\n\nto deploy into both regions, then when one of those regions is down, the\nmodule will not be able to connect to that region, and any attempt to run\nplan or apply will fail. So right when you need to roll out changes—\nwhen there’s a major outage—your Terraform code will stop working.\nMore generally, as discussed in Chapter 3, you should keep\nenvironments completely isolated: so instead of managing multiple\nregions in one module with aliases, you manage each region in separate\nmodules. That way, you minimize the blast radius, both from your own\nmistakes (e.g., if you accidentally break something in one region, it’s\nless likely to affect the other) and from problems in the world itself\n(e.g., an outage in one region is less likely to affect the other).\nSo when does it make sense to use aliases? Typically, aliases are a good fit\nwhen the infrastructure you’re deploying across several aliased providers is\ntruly coupled and you want to always deploy it together. For example, if\nyou wanted to use Amazon CloudFront as a CDN (Content Distribution\nNetwork), and to provision a TLS certificate for it using AWS Certification\nManager (ACM), then AWS requires the certificate to be created in the us-\neast-1 region, no matter what other regions you happen to be using for\nCloudFront itself. In that case, your code may have two provider blocks,\none for the primary region you want to use for CloudFront and one with an\nalias hardcoded specifically to us-east-1 for configuring the TLS\ncertificate. Another use case for aliases is if you’re deploying resources\ndesigned for use across many regions: for example, AWS recommends\ndeploying GuardDuty, an automated threat detection service, in every single\nregion you’re using in your AWS account. In this case, it may make sense\nto have a module with a provider block and custom alias for each\nAWS region.\nBeyond a few corner cases like this, using aliases to handle multiple regions\nis relatively rare. A more common use case for aliases is when you have\nmultiple providers that need to authenticate in different ways, such as each\none authenticating to a different AWS account.' metadata={'original_pages_range': '353-368', 'source': '109_Working_with_Multiple_AWS_Regions', 'file_path': '/mnt/c/Users/Carlos/Documents/Github/jupiter-iaa-azure/Scripts/RAG/../../data/optimized_chunks/Libro-TF/109_Working_with_Multiple_AWS_Regions.pdf', 'num_pages': 16}", "page_content='Working with Multiple AWS Accounts\nSo far, throughout this book, you’ve likely been using a single AWS\naccount for all of your infrastructure. For production code, it’s more\ncommon to use multiple AWS accounts: e.g., you put your staging\nenvironment in a stage account, your production environment in a prod\naccount, and so on. This concept applies to other clouds too, such as Azure\nand Google Cloud. Note that I’ll be using the term account in this book,\neven though some clouds use slightly different terminology for the same\nconcept (e.g., Google Cloud calls them projects instead of accounts).\nThe main reasons for using multiple accounts are as follows:\nIsolation (aka compartmentalization)\nYou use separate accounts to isolate different environments from each\nother and to limit the “blast radius” when things go wrong. For\nexample, putting your staging and production environments in separate\naccounts ensures that if an attacker manages to break into staging, they\nstill have no access whatsoever to production. Likewise, this isolation\nensures that a developer making changes in staging is less likely to\naccidentally break something in production.\nAuthentication and authorization\nIf everything is in one account, it’s tricky to grant access to some things\n(e.g., the staging environment) but not accidentally grant access to other\nthings (e.g., the production environment). Using multiple accounts\nmakes it easier to have fine-grained control, as any permissions you\ngrant in one account have no effect on any other account.\nThe authentication requirements of multiple accounts also help reduce\nthe chance of mistakes. With everything in a single account, it’s too\neasy to make the mistake where you think you’re making a change in,\nsay, your staging environment, but you’re actually making the change in\nproduction (which can be a disaster if the change you’re making is, for\nexample, to drop all database tables). With multiple accounts, this is less\nlikely, as authenticating to each account requires a separate set of steps.\n\nNote that having multiple accounts does not imply that developers have\nmultiple separate user profiles (e.g., a separate IAM user in each AWS\naccount). In fact, that would be an antipattern, as that would require\nmanaging multiple sets of credentials, permissions, etc. Instead, you can\nconfigure just about all the major clouds so that each developer has\nexactly one user profile, which they can use to authenticate to any\naccount they have access to. The cross-account authentication\nmechanism varies depending on the cloud you’re using: e.g., in AWS,\nyou can authenticate across AWS accounts by assuming IAM roles, as\nyou’ll see shortly.\nAuditing and reporting\nA properly configured account structure will allow you to maintain an\naudit trail of all the changes happening in all your environments, check\nif you’re adhering to compliance requirements, and detect anomalies.\nMoreover, you’ll be able to have consolidated billing, with all the\ncharges for all of your accounts in one place, including cost breakdowns\nby account, service, tag, etc. This is especially useful in large\norganizations, as it allows finance to track and budget spending by team\nsimply by looking at which account the charges are coming from.\nLet’s go through a multi-account example with AWS. First, you’ll want to\ncreate a new AWS account to use for testing. Since you already have one\nAWS account, to create new child accounts, you can use AWS\nOrganizations, which ensures that the billing from all the child accounts\nrolls up into the parent account (sometimes called the root account) and\ngives you a dashboard you can use to manage all the child accounts.\nHead over to the AWS Organizations Console, and click the “Add an AWS\naccount” button, as shown in Figure 7-3.\n\n\n\nFigure 7-3. Use AWS Organizations to create a new AWS account.\nOn the next page, fill in the following info, as shown in Figure 7-4:\nAWS account name\nThe name to use for the account. For example, if this account was going\nto be used for your staging environment, you might name it “staging.”\nEmail address of the account’s owner\nThe email address to use for the root user of the AWS account. Note\nthat every AWS account must use a different email address for the root\nuser, so you can’t reuse the email address you used to create your first\n(root) AWS account (see “How to Get Multiple Aliases from One Email\nAddress” for a workaround). So what about the root user’s password?\nBy default, AWS does not configure a password for the root user of a\nnew child account (you’ll see shortly an alternative way to authenticate\nto the child account). If you ever do want to log in as this root user, after\nyou create the child account, you’ll need to go through the password\nreset flow with the email address you’re specifying here.\nIAM role name\nWhen AWS Organizations creates a child AWS account, it\nautomatically creates an IAM role within that child AWS account that\nhas admin permissions and can be assumed from the parent account.\nThis is convenient, as it allows you to authenticate to the child AWS\naccount without having to create any IAM users or IAM roles yourself.\nI recommend leaving this IAM role name at the default value of\nOrganizationAccountAccessRole.\n\nFigure 7-4. Fill in the details for the new AWS account.\n\nHOW TO GET MULTIPLE ALIASES FROM ONE EMAIL\nADDRESS\nIf you use Gmail, you can get multiple email aliases out of a single\naddress by taking advantage of the fact that Gmail ignores everything\nafter a plus sign in an email address. For example, if your Gmail\naddress is example@gmail.com, you can send email to\nexample+foo@gmail.com and example+any-text-you-\nwant@gmail.com, and all of those emails will go to\nexample@gmail.com. This also works if your company uses Gmail via\nGoogle Workspace, even with a custom domain: e.g.,\nexample+dev@company.com and example+stage@company.com will\nall go to example@company.com.\nThis is useful if you’re creating a dozen child AWS accounts, as instead\nof having to create a dozen totally separate email addresses, you could\nuse example+dev@company.com for your dev account,\nexample+stage@company.com for your stage account, and so on; AWS\nwill see each of those email addresses as a different, unique address, but\nunder the hood, all the emails will go to the same account.\nClick the Create AWS Account button, wait a few minutes for AWS to\ncreate the account, and then jot down the 12-digit ID of the AWS account\nthat gets created. For the rest of this chapter, let’s assume the following:\nParent AWS account ID: 111111111111\nChild AWS account ID: 222222222222\nYou can authenticate to your new child account from the AWS Console by\nclicking your username and selecting “Switch role”, as shown in Figure 7-5.\n\nFigure 7-5. Select the “Switch role” button.\nNext, enter the details for the IAM role you want to assume, as shown in\nFigure 7-6:\nAccount\nThe 12-digit ID of the AWS account to switch to. You’ll want to enter\nthe ID of your new child account.\nRole\n\nThe name of the IAM role to assume in that AWS account. Enter the\nname you used for the IAM role when creating the new child account,\nwhich is OrganizationAccountAccessRole by default.\nDisplay name\nAWS will create a shortcut in the nav to allow you to switch to this\naccount in the future with a single click. This is the name to show in this\nshortcut. It only affects your IAM user in this browser.\nFigure 7-6. Enter the details for the role to switch to.\n\nClick Switch Role and, voilà, AWS should log you into the web console of\nthe new AWS account!\nLet’s now write an example Terraform module in examples/multi-account-\nroot that can authenticate to multiple AWS accounts. Just as with the\nmultiregion AWS example, you will need to add two provider blocks in\nmain.tf, each with a different alias. First, the provider block for the\nparent AWS account:\nprovider \"aws\" {\n  region = \"us-east-2\"\n  alias  = \"parent\"\n}\nNext, the provider block for the child AWS account:\nprovider \"aws\" {\n  region = \"us-east-2\"\n  alias  = \"child\"\n}\nTo be able to authenticate to the child AWS account, you’ll assume an IAM\nrole. In the web console, you did this by clicking the Switch Role button; in\nyour Terraform code, you do this by adding an assume_role block to the\nchild provider block:\nprovider \"aws\" {\n  region = \"us-east-2\"\n  alias  = \"child\" \n \n  assume_role {\n    role_arn = \"arn:aws:iam::<ACCOUNT_ID>:role/<ROLE_NAME>\" \n  }\n}\nIn the role_arn parameter, you’ll need to replace ACCOUNT_ID with\nyour child account ID and ROLE_NAME with the name of the IAM role in\nthat account, just as you did when switching roles in the web console.\n\nHere’s what it looks like with the account ID 222222222222 and role\nname OrganizationAccountAccessRole plugged in:\nprovider \"aws\" {\n  region = \"us-east-2\"\n  alias  = \"child\" \n \n  assume_role {\n    role_arn = \n\"arn:aws:iam::222222222222:role/OrganizationAccountAccessRole\" \n  }\n}\nNow, to check this is actually working, add two\naws_caller_identity data sources, and configure each one to use a\ndifferent provider:\ndata \"aws_caller_identity\" \"parent\" {\n  provider = aws.parent\n} \n \ndata \"aws_caller_identity\" \"child\" {\n  provider = aws.child\n}\nFinally, add output variables in outputs.tf to print out the account IDs:\noutput \"parent_account_id\" {\n  value       = data.aws_caller_identity.parent.account_id\n  description = \"The ID of the parent AWS account\"\n} \n \noutput \"child_account_id\" {\n  value       = data.aws_caller_identity.child.account_id\n  description = \"The ID of the child AWS account\"\n}\nRun apply, and you should see the different IDs for each account:\n$ terraform apply \n \n(...)\n\nApply complete! Resources: 0 added, 0 changed, 0 destroyed. \n \nOutputs: \n \nparent_account_id = \"111111111111\" \nchild_account_id = \"222222222222\"\nAnd there you have it: by using provider aliases and assume_role\nblocks, you now know how to write Terraform code that can operate across\nmultiple AWS accounts.\nAs with the multiregion section, a few warnings:\nWarning 1: Cross-account IAM roles are double opt-in\nIn order for an IAM role to allow access from one AWS account to\nanother—e.g., to allow an IAM role in account 222222222222 to be\nassumed from account 111111111111—you need to grant\npermissions in both AWS accounts:\nFirst, in the AWS account where the IAM role lives (e.g., the child\naccount 222222222222), you must configure its assume role\npolicy to trust the other AWS account (e.g., the parent account\n111111111111). This happened magically for you with the\nOrganizationAccountAccessRole IAM role because AWS\nOrganizations automatically configures the assume role policy of\nthis IAM role to trust the parent account. However, for any custom\nIAM roles you create, you need to remember to explicitly grant the\nsts:AssumeRole permission yourself.\nSecond, in the AWS account from which you assume the role (e.g.,\nthe parent account 111111111111), you must also grant your user\npermissions to assume that IAM role. Again, this happened for you\nmagically because, in Chapter 2, you gave your IAM user\nAdministratorAccess, which gives you permissions to do just\nabout everything in the parent AWS account, including assuming\nIAM roles. In most real-world use cases, your user won’t be' metadata={'original_pages_range': '369-379', 'source': '110_Working_with_Multiple_AWS_Accounts', 'file_path': '/mnt/c/Users/Carlos/Documents/Github/jupiter-iaa-azure/Scripts/RAG/../../data/optimized_chunks/Libro-TF/110_Working_with_Multiple_AWS_Accounts.pdf', 'num_pages': 11}", "page_content='(shouldn’t be!) an admin, so you’ll need to explicitly grant your user\nsts:AssumeRole permissions on the IAM role(s) you want to be\nable to assume.\nWarning 2: Use aliases sparingly\nI said this in the multiregion example, but it bears repeating: although\nit’s easy to use aliases with Terraform, I would caution against using\nthem too often, including with multi-account code. Typically, you use\nmultiple accounts to create separation between them, so if something\ngoes wrong in one account, it doesn’t affect the other. Modules that\ndeploy across multiple accounts go against this principle. Only do it\nwhen you intentionally want to have resources in multiple accounts\ncoupled and deployed together.\nCreating Modules That Can Work with Multiple Providers\nWhen working with Terraform modules, you typically work with two types\nof modules:\nReusable modules\nThese are low-level modules that are not meant to be deployed directly\nbut instead are to be combined with other modules, resources, and data\nsources.\nRoot modules\nThese are high-level modules that combine multiple reusable modules\ninto a single unit that is meant to be deployed directly by running\napply (in fact, the definition of a root module is it’s the one on which\nyou run apply).\nThe multiprovider examples you’ve seen so far have put all the provider\nblocks into the root module. What do you do if you want to create a\nreusable module that works with multiple providers? For example, what if\nyou wanted to turn the multi-account code from the previous section into a\n\nreusable module? As a first step, you might put all that code, unchanged,\ninto the modules/multi-account folder. Then, you could create a new\nexample to test it with in the examples/multi-account-module folder, with a\nmain.tf that looks like this:\nmodule \"multi_account_example\" {\n  source = \"../../modules/multi-account\"\n}\nIf you run apply on this code, it’ll work, but there is a problem: all of the\nprovider configuration is now hidden in the module itself (in\nmodules/multi-account). Defining provider blocks within reusable\nmodules is an antipattern for several reasons:\nConfiguration problems\nIf you have provider blocks defined in your reusable module, then\nthat module controls all the configuration for that provider. For\nexample, the IAM role ARN and regions to use are currently hardcoded\nin the modules/multi-account module. You could, of course, expose\ninput variables to allow users to set the regions and IAM role ARNs, but\nthat’s only the tip of the iceberg. If you browse the AWS Provider\ndocumentation, you’ll find that there are roughly 50 different\nconfiguration options you can pass into it! Many of these parameters are\ngoing to be important for users of your module, as they control how to\nauthenticate to AWS, what region to use, what account (or IAM role) to\nuse, what endpoints to use when talking to AWS, what tags to apply or\nignore, and much more. Having to expose 50 extra variables in a\nmodule will make that module very cumbersome to maintain and use.\nDuplication problems\nEven if you expose those 50 settings in your module, or whatever subset\nyou believe is important, you’re creating code duplication for users of\nyour module. That’s because it’s common to combine multiple modules\ntogether, and if you have to pass in some subset of 50 settings into each\nof those modules in order to get them to all authenticate correctly,\n\nyou’re going to have to copy and paste a lot of parameters, which is\ntedious and error prone.\nPerformance problems\nEvery time you include a provider block in your code, Terraform\nspins up a new process to run that provider, and communicates with that\nprocess via RPC. If you have a handful of provider blocks, this\nworks just fine, but as you scale up, it may cause performance\nproblems. Here’s a real-world example: a few years ago, I created\nreusable modules for CloudTrail, AWS Config, GuardDuty, IAM\nAccess Analyzer, and Macie. Each of these AWS services is supposed\nto be deployed into every region in your AWS account, and as AWS had\n~25 regions, I included 25 provider blocks in each of these modules.\nI then created a single root module to deploy all of these as a “baseline”\nin my AWS accounts: if you do the math, that’s 5 modules with 25\nprovider blocks each, or 125 provider blocks total. When I ran\napply, Terraform would fire up 125 processes, each making hundreds\nof API and RPC calls. With thousands of concurrent network requests,\nmy CPU would start thrashing, and a single plan could take 20\nminutes. Worse yet, this would sometimes overload the network stack,\nleading to intermittent failures in API calls, and apply would fail with\nsporadic errors.\nTherefore, as a best practice, you should not define any provider blocks\nin your reusable modules and instead allow your users to create the\nprovider blocks they need solely in their root modules. But then, how do\nyou build a module that can work with multiple providers? If the module\nhas no provider blocks in it, how do you define provider aliases that you\ncan reference in your resources and data sources?\nThe solution is to use configuration aliases. These are very similar to the\nprovider aliases you’ve seen already, except they aren’t defined in a\nprovider block. Instead, you define them in a required_providers\nblock.\n\nOpen up modules/multi-account/main.tf, remove the nested provider\nblocks, and replace them with a required_providers block with\nconfiguration aliases as follows:\nterraform { \n  required_providers {\n    aws = {\n      source                = \"hashicorp/aws\"\n      version               = \"~> 4.0\"\n      configuration_aliases = [aws.parent, aws.child] \n    } \n  }\n}\nJust as with normal provider aliases, you can pass configuration aliases into\nresources and data sources using the provider parameter:\ndata \"aws_caller_identity\" \"parent\" {\n  provider = aws.parent\n} \n \ndata \"aws_caller_identity\" \"child\" {\n  provider = aws.child\n}\nThe key difference from normal provider aliases is that configuration\naliases don’t create any providers themselves; instead, they force users of\nyour module to explicitly pass in a provider for each of your configuration\naliases using a providers map.\nOpen up examples/multi-account-module/main.tf, and define the\nprovider blocks as before:\nprovider \"aws\" {\n  region = \"us-east-2\"\n  alias  = \"parent\"\n} \n \nprovider \"aws\" {\n  region = \"us-east-2\"\n  alias  = \"child\"' metadata={'original_pages_range': '380-383', 'source': '111_Creating_Modules_That_Can_Work_with_Multiple_Providers', 'file_path': '/mnt/c/Users/Carlos/Documents/Github/jupiter-iaa-azure/Scripts/RAG/../../data/optimized_chunks/Libro-TF/111_Creating_Modules_That_Can_Work_with_Multiple_Providers.pdf', 'num_pages': 4}", "page_content='assume_role {\n    role_arn = \n\"arn:aws:iam::222222222222:role/OrganizationAccountAccessRole\" \n  }\n}\nAnd now you can pass them into the modules/multi-account module as\nfollows:\nmodule \"multi_account_example\" {\n  source = \"../../modules/multi-account\" \n \n  providers = {\n    aws.parent = aws.parent\n    aws.child  = aws.child \n  }\n}\nThe keys in the providers map must match the names of the\nconfiguration aliases within the module; if any of the names from\nconfiguration aliases are missing in the providers map, Terraform will\nshow an error. This way, when you’re building a reusable module, you can\ndefine what providers that module needs, and Terraform will ensure users\npass those providers in; and when you’re building a root module, you can\ndefine your provider blocks just once and pass around references to\nthem to the reusable modules you depend on.\nWorking with Multiple Different Providers\nYou’ve now seen how to work with multiple providers when all of them are\nthe same type of provider: e.g., multiple copies of the aws provider. This\nsection talks about how to work with multiple different providers.\nReaders of the first two editions of this book often asked for examples of\nusing multiple clouds together (multicloud), but I couldn’t find much useful\nto share. In part, this is because using multiple clouds is usually a bad\npractice, but even if you’re forced to do it (most large companies are2\n\nmulticloud, whether they want to be or not), it’s rare to manage multiple\nclouds in a single module for the same reason it’s rare to manage multiple\nregions or accounts in a single module. If you’re using multiple clouds,\nyou’re far better off managing each one in a separate module.\nMoreover, translating every single AWS example in the book into the\nequivalent solutions for other clouds (Azure and Google Cloud) is\nimpractical: the book would end up way too long, and while you would\nlearn more about each cloud, you wouldn’t learn any new Terraform\nconcepts along the way, which is the real goal of the book. If you do want\nto see examples of what the Terraform code for similar infrastructure looks\nlike across different clouds, have a look at the examples folder in the\nTerratest repo. As you’ll see in Chapter 9, Terratest provides a set of tools\nfor writing automated tests for different types of infrastructure code and\ndifferent types of clouds, so in the examples folder you’ll find Terraform\ncode for similar infrastructure in AWS, Google Cloud, and Azure, including\nindividual servers, groups of servers, databases, and more. You’ll also find\nautomated tests for all those examples in the test folder.\nIn this book, instead of an unrealistic multicloud example, I decided to\ninstead show you how to use multiple providers together in a slightly more\nrealistic scenario (one that was also requested by many readers of the first\ntwo editions): namely, how to use the AWS Provider with the Kubernetes\nprovider to deploy Dockerized apps. Kubernetes is, in many ways, a cloud\nof its own—it can run applications, networks, data stores, load balancers,\nsecret stores, and much more—so, in a sense, this is both a multiprovider\nand multicloud example. And because Kubernetes is a cloud, that means\nthere is a lot to learn, so I’m going to have to build up to it one step at a\ntime, starting with mini crash courses on Docker and Kubernetes, before\nfinally moving on to the full multiprovider example that uses both AWS and\nKubernetes:\nA crash course on Docker\nA crash course on Kubernetes' metadata={'original_pages_range': '384-385', 'source': '112_Working_with_Multiple_Different_Providers', 'file_path': '/mnt/c/Users/Carlos/Documents/Github/jupiter-iaa-azure/Scripts/RAG/../../data/optimized_chunks/Libro-TF/112_Working_with_Multiple_Different_Providers.pdf', 'num_pages': 2}", "page_content='Deploying Docker containers in AWS using Elastic Kubernetes\nService (EKS)\nA Crash Course on Docker\nAs you may remember from Chapter 1, Docker images are like self-\ncontained “snapshots” of the operating system (OS), the software, the files,\nand all other relevant details. Let’s now see Docker in action.\nFirst, if you don’t have Docker installed already, follow the instructions on\nthe Docker website to install Docker Desktop for your operating system.\nOnce it’s installed, you should have the docker command available on\nyour command line. You can use the docker run command to run\nDocker images locally:\n$ docker run <IMAGE> [COMMAND]\nwhere IMAGE is the Docker image to run and COMMAND is an optional\ncommand to execute. For example, here’s how you can run a Bash shell in\nan Ubuntu 20.04 Docker image (note that the following command includes\nthe -it flag so you get an interactive shell where you can type):\n$ docker run -it ubuntu:20.04 bash \n \nUnable to find image 'ubuntu:20.04' locally \n20.04: Pulling from library/ubuntu \nDigest: \nsha256:669e010b58baf5beb2836b253c1fd5768333f0d1dbcb834f7c07a4dc93\nf474be \nStatus: Downloaded newer image for ubuntu:20.04 \n \nroot@d96ad3779966:/#\nAnd voilà, you’re now in Ubuntu! If you’ve never used Docker before, this\ncan seem fairly magical. Try running some commands. For example, you\ncan look at the contents of /etc/os-release to verify you really are in\nUbuntu:\n\nroot@d96ad3779966:/# cat /etc/os-release \nNAME=\"Ubuntu\" \nVERSION=\"20.04.3 LTS (Focal Fossa)\" \nID=ubuntu \nID_LIKE=debian \nPRETTY_NAME=\"Ubuntu 20.04.3 LTS\" \nVERSION_ID=\"20.04\" \nVERSION_CODENAME=focal\nHow did this happen? Well, first, Docker searches your local filesystem for\nthe ubuntu:20.04 image. If you don’t have that image downloaded\nalready, Docker downloads it automatically from Docker Hub, which is a\nDocker Registry that contains shared Docker images. The ubuntu:20.04\nimage happens to be a public Docker image—an official one maintained by\nthe Docker team—so you’re able to download it without any authentication.\nHowever, it’s also possible to create private Docker images that only certain\nauthenticated users can use.\nOnce the image is downloaded, Docker runs the image, executing the bash\ncommand, which starts an interactive Bash prompt, where you can type. Try\nrunning the ls command to see the list of files:\nroot@d96ad3779966:/# ls -al \ntotal 56 \ndrwxr-xr-x   1 root root 4096 Feb 22 14:22 . \ndrwxr-xr-x   1 root root 4096 Feb 22 14:22 .. \nlrwxrwxrwx   1 root root    7 Jan 13 16:59 bin -> usr/bin \ndrwxr-xr-x   2 root root 4096 Apr 15  2020 boot \ndrwxr-xr-x   5 root root  360 Feb 22 14:22 dev \ndrwxr-xr-x   1 root root 4096 Feb 22 14:22 etc \ndrwxr-xr-x   2 root root 4096 Apr 15  2020 home \nlrwxrwxrwx   1 root root    7 Jan 13 16:59 lib -> usr/lib \ndrwxr-xr-x   2 root root 4096 Jan 13 16:59 media \n(...)\nYou might notice that’s not your filesystem. That’s because Docker images\nrun in containers that are isolated at the userspace level: when you’re in a\ncontainer, you can only see the filesystem, memory, networking, etc., in that\ncontainer. Any data in other containers, or on the underlying host operating\nsystem, is not accessible to you, and any data in your container is not\n\nvisible to those other containers or the underlying host operating system.\nThis is one of the things that makes Docker useful for running applications:\nthe image format is self-contained, so Docker images run the same way no\nmatter where you run them, and no matter what else is running there.\nTo see this in action, write some text to a test.txt file as follows:\nroot@d96ad3779966:/# echo \"Hello, World!\" > test.txt\nNext, exit the container by hitting Ctrl-D on Windows and Linux or Cmd-D\non macOS, and you should be back in your original command prompt on\nyour underlying host OS. If you try to look for the test.txt file you just\nwrote, you’ll see that it doesn’t exist: the container’s filesystem is totally\nisolated from your host OS.\nNow, try running the same Docker image again:\n$ docker run -it ubuntu:20.04 bash \nroot@3e0081565a5d:/#\nNotice that this time, since the ubuntu:20.04 image is already\ndownloaded, the container starts almost instantly. This is another reason\nDocker is useful for running applications: unlike virtual machines,\ncontainers are lightweight, boot up quickly, and incur little CPU or memory\noverhead.\nYou may also notice that the second time you fired up the container, the\ncommand prompt looked different. That’s because you’re now in a totally\nnew container; any data you wrote in the previous one is no longer\naccessible to you. Run ls -al and you’ll see that the test.txt file does not\nexist. Containers are isolated not only from the host OS but also from each\nother.\nHit Ctrl-D or Cmd-D again to exit the container, and back on your host OS,\nrun the docker ps -a command:\n$ docker ps -a \nCONTAINER ID   IMAGE            COMMAND    CREATED\n\nSTATUS \n3e0081565a5d   ubuntu:20.04     \"bash\"     5 min ago    Exited \n(0) 16 sec ago \nd96ad3779966   ubuntu:20.04     \"bash\"     14 min ago   Exited \n(0) 5 min ago\nThis will show you all the containers on your system, including the stopped\nones (the ones you exited). You can start a stopped container again by using\nthe docker start <ID> command, setting ID to an ID from the\nCONTAINER ID column of the docker ps output. For example, here is\nhow you can start the first container up again (and attach an interactive\nprompt to it via the -ia flags):\n$ docker start -ia d96ad3779966 \nroot@d96ad3779966:/#\nYou can confirm this is really the first container by outputting the contents\nof test.txt:\nroot@d96ad3779966:/# cat test.txt \nHello, World!\nLet’s now see how a container can be used to run a web app. Hit Ctrl-D or\nCmd-D again to exit the container, and back on your host OS, run a new\ncontainer:\n$ docker run training/webapp \n * Running on http://0.0.0.0:5000/ (Press CTRL+C to quit)\nThe training/webapp image contains a simple Python “Hello, World” web\napp for testing. When you run the image, it fires up the web app, listening\non port 5000 by default. However, if you open a new terminal on your host\noperating system and try to access the web app, it won’t work:\n$ curl localhost:5000 \ncurl: (7) Failed to connect to localhost port 5000: Connection \nrefused' metadata={'original_pages_range': '386-389', 'source': '113_A_Crash_Course_on_Docker', 'file_path': '/mnt/c/Users/Carlos/Documents/Github/jupiter-iaa-azure/Scripts/RAG/../../data/optimized_chunks/Libro-TF/113_A_Crash_Course_on_Docker.pdf', 'num_pages': 4}", "page_content='What’s the problem? Actually, it’s not a problem but a feature! Docker\ncontainers are isolated from the host operating system and other containers,\nnot only at the filesystem level but also in terms of networking. So while\nthe container really is listening on port 5000, that is only on a port inside\nthe container, which isn’t accessible on the host OS. If you want to expose a\nport from the container on the host OS, you have to do it via the -p flag.\nFirst, hit Ctrl-C to shut down the training/webapp container: note that\nit’s C this time, not D, and it’s Ctrl regardless of OS, as you’re shutting\ndown a process, rather than exiting an interactive prompt. Now rerun the\ncontainer but this time with the -p flag as follows:\n$ docker run -p 5000:5000 training/webapp \n * Running on http://0.0.0.0:5000/ (Press CTRL+C to quit)\nAdding -p 5000:5000 to the command tells Docker to expose port 5000\ninside the container on port 5000 of the host OS. In another terminal on\nyour host OS, you should now be able to see the web app working:\n$ curl localhost:5000 \nHello world!\nCLEANING UP CONTAINERS\nEvery time you run docker run and exit, you are leaving behind containers, which\ntake up disk space. You may wish to clean them up with the docker rm\n<CONTAINER_ID> command, where CONTAINER_ID is the ID of the container from\nthe docker ps output. Alternatively, you could include the --rm flag in your\ndocker run command to have Docker automatically clean up when you exit the\ncontainer.\nA Crash Course on Kubernetes\nKubernetes is an orchestration tool for Docker, which means it’s a platform\nfor running and managing Docker containers on your servers, including\nscheduling (picking which servers should run a given container workload),\n\nauto healing (automatically redeploying containers that failed), auto scaling\n(scaling the number of containers up and down in response to load), load\nbalancing (distributing traffic across containers), and much more.\nUnder the hood, Kubernetes consists of two main pieces:\nControl plane\nThe control plane is responsible for managing the Kubernetes cluster. It\nis the “brains” of the operation, responsible for storing the state of the\ncluster, monitoring containers, and coordinating actions across the\ncluster. It also runs the API server, which provides an API you can use\nfrom command-line tools (e.g., kubectl), web UIs (e.g., the\nKubernetes Dashboard), and IaC tools (e.g., Terraform) to control\nwhat’s happening in the cluster.\nWorker nodes\nThe worker nodes are the servers used to actually run your containers.\nThe worker nodes are entirely managed by the control plane, which tells\neach worker node what containers it should run.\nKubernetes is open source, and one of its strengths is that you can run it\nanywhere: in any public cloud (e.g., AWS, Azure, Google Cloud), in your\nown datacenter, and even on your own developer workstation. A little later\nin this chapter, I’ll show you how you can run Kubernetes in the cloud (in\nAWS), but for now, let’s start small and run it locally. This is easy to do if\nyou installed a relatively recent version of Docker Desktop, as it has the\nability to fire up a Kubernetes cluster locally with just a few clicks.\nIf you open Docker Desktop’s preferences on your computer, you should\nsee Kubernetes in the nav, as shown in Figure 7-7.\n\nFigure 7-7. Enable Kubernetes on Docker Desktop.\nIf it’s not enabled already, check the Enable Kubernetes checkbox, click\nApply & Restart, and wait a few minutes for that to complete. In the\nmeantime, follow the instructions on the Kubernetes website to install\nkubectl, which is the command-line tool for interacting with Kubernetes.\nTo use kubectl, you must first update its configuration file, which lives\nin $HOME/.kube/config (that is, the .kube folder of your home directory), to\ntell it what Kubernetes cluster to connect to. Conveniently, when you enable\nKubernetes in Docker Desktop, it updates this config file for you, adding a\n\ndocker-desktop entry to it, so all you need to do is tell kubectl to\nuse this configuration as follows:\n$ kubectl config use-context docker-desktop \nSwitched to context \"docker-desktop\".\nNow you can check if your Kubernetes cluster is working with the get\nnodes command:\n$ kubectl get nodes \nNAME             STATUS   ROLES                  AGE   VERSION \ndocker-desktop   Ready    control-plane,master   95m   v1.22.5\nThe get nodes command shows you information about all the nodes in\nyour cluster. Since you’re running Kubernetes locally, your computer is the\nonly node, and it’s running both the control plane and acting as a worker\nnode. You’re now ready to run some Docker containers!\nTo deploy something in Kubernetes, you create Kubernetes objects, which\nare persistent entities you write to the Kubernetes cluster (via the API\nserver) that record your intent: e.g., your intent to have specific Docker\nimages running. The cluster runs a reconciliation loop, which continuously\nchecks the objects you stored in it and works to make the state of the cluster\nmatch your intent.\nThere are many different types of Kubernetes objects available. For the\nexamples in this book, let’s use the following two objects:\nKubernetes Deployment\nA Kubernetes Deployment is a declarative way to manage an\napplication in Kubernetes. You declare what Docker images to run, how\nmany copies of them to run (called replicas), a variety of settings for\nthose images (e.g., CPU, memory, port numbers, environment\nvariables), and the strategy to roll out updates to those images, and the\nKubernetes Deployment will then work to ensure that the requirements\nyou declared are always met. For example, if you specified you wanted\nthree replicas, but one of the worker nodes went down so only two\n\nreplicas are left, the Deployment will automatically spin up a third\nreplica on one of the other worker nodes.\nKubernetes Service\nA Kubernetes Service is a way to expose a web app running in\nKubernetes as a networked service. For example, you can use a\nKubernetes Service to configure a load balancer that exposes a public\nendpoint and distributes traffic from that endpoint across the replicas in\na Kubernetes Deployment.\nThe idiomatic way to interact with Kubernetes is to create YAML files\ndescribing what you want—e.g., one YAML file that defines the Kubernetes\nDeployment and another one that defines the Kubernetes Service—and to\nuse the kubectl apply command to submit those objects to the cluster.\nHowever, using raw YAML has drawbacks, such as a lack of support for\ncode reuse (e.g., variables, modules), abstraction (e.g., loops, if-statements),\nclear standards on how to store and manage the YAML files (e.g., to track\nchanges to the cluster over time), and so on. Therefore, many Kubernetes\nusers turn to alternatives, such as Helm or Terraform. Since this is a book\non Terraform, I’m going to show you how to create a Terraform module\ncalled k8s-app (K8S is an acronym for Kubernetes in the same way that\nI18N is an acronym for internationalization) that deploys an app in\nKubernetes using a Kubernetes Deployment and Kubernetes Service.\nCreate a new module in the modules/services/k8s-app folder. Within that\nfolder, create a variables.tf file that defines the module’s API via the\nfollowing input variables:\nvariable \"name\" {\n  description = \"The name to use for all resources created by \nthis module\"\n  type        = string\n} \n \nvariable \"image\" {\n  description = \"The Docker image to run\"\n  type        = string\n\n} \n \nvariable \"container_port\" {\n  description = \"The port the Docker image listens on\"\n  type        = number\n} \n \nvariable \"replicas\" {\n  description = \"How many replicas to run\"\n  type        = number\n} \n \nvariable \"environment_variables\" {\n  description = \"Environment variables to set for the app\"\n  type        = map(string)\n  default     = {}\n}\nThis should give you just about all the inputs you need for creating the\nKubernetes Deployment and Service. Next, add a main.tf file, and at the\ntop, add the required_providers block to it with the Kubernetes\nprovider:\nterraform {\n  required_version = \">= 1.0.0, < 2.0.0\" \n \n  required_providers {\n    kubernetes = {\n      source  = \"hashicorp/kubernetes\"\n      version = \"~> 2.0\" \n    } \n  }\n}\nHey, a new provider, neat! OK, let’s make use of that provider to create a\nKubernetes Deployment by using the kubernetes_deployment\nresource:\nresource \"kubernetes_deployment\" \"app\" {\n}\n\nThere are quite a few settings to configure within the\nkubernetes_deployment resource, so let’s go through them one at a\ntime. First, you need to configure the metadata block:\nresource \"kubernetes_deployment\" \"app\" { \n  metadata {\n    name = var.name \n  }\n}\nEvery Kubernetes object includes metadata that can be used to identify and\ntarget that object in API calls. In the preceding code, I’m setting the\nDeployment name to the name input variable.\nThe rest of the configuration for the kubernetes_deployment\nresource goes into the spec block:\nresource \"kubernetes_deployment\" \"app\" { \n  metadata {\n    name = var.name \n  } \n \n  spec { \n  }\n}\nThe first item to put into the spec block is to specify the number of\nreplicas to create:\n  spec {\n    replicas = var.replicas \n  }\nNext, define the template block:\n  spec {\n    replicas = var.replicas \n \n    template { \n    } \n  }\n\nIn Kubernetes, instead of deploying one container at a time, you deploy\nPods, which are groups of containers that are meant to be deployed\ntogether. For example, you could have a Pod with one container to run a\nweb app (e.g., the Python app you saw earlier) and another container that\ngathers metrics on the web app and sends them to a central service (e.g.,\nDatadog). The template block is where you define the Pod Template,\nwhich specifies what container(s) to run, the ports to use, environment\nvariables to set, and so on.\nOne important ingredient in the Pod Template will be the labels to apply to\nthe Pod. You’ll need to reuse these labels in several places—e.g., the\nKubernetes Service uses labels to identify the Pods that need to be load\nbalanced—so let’s define those labels in a local variable called\npod_labels:\nlocals {\n  pod_labels = {\n    app = var.name \n  }\n}\nAnd now use pod_labels in the metadata block of the Pod Template:\n  spec {\n    replicas = var.replicas \n \n    template { \n      metadata {\n        labels = local.pod_labels \n      } \n    } \n  }\nNext, add a spec block inside of template:\n  spec {\n    replicas = var.replicas \n \n    template { \n      metadata {\n\nlabels = local.pod_labels \n      } \n \n      spec { \n        container {\n          name  = var.name\n          image = var.image \n \n          port {\n            container_port = var.container_port \n          } \n \n          dynamic \"env\" {\n            for_each = var.environment_variables \n            content {\n              name  = env.key\n              value = env.value \n            } \n          } \n        } \n      } \n    } \n  }\nThere’s a lot here, so let’s go through it one piece at a time:\ncontainer\nInside the spec block, you can define one or more container blocks\nto specify which Docker containers to run in this Pod. To keep this\nexample simple, there’s just one container block in the Pod. The\nrest of these items are all within this container block.\nname\nThe name to use for the container. I’ve set this to the name input\nvariable.\nimage\nThe Docker image to run in the container. I’ve set this to the image\ninput variable.\n\nport\nThe ports to expose in the container. To keep the code simple, I’m\nassuming the container only needs to listen on one port, set to the\ncontainer_port input variable.\nenv\nThe environment variables to expose to the container. I’m using a\ndynamic block with for_each (two concepts you may remember\nfrom Chapter 5) to set this to the variables in the\nenvironment_variables input variable.\nOK, that wraps up the Pod Template. There’s just one thing left to add to\nthe kubernetes_deployment resource—a selector block:\n  spec {\n    replicas = var.replicas \n \n    template { \n      metadata {\n        labels = local.pod_labels \n      } \n \n      spec { \n        container {\n          name  = var.name\n          image = var.image \n \n          port {\n            container_port = var.container_port \n          } \n \n          dynamic \"env\" {\n            for_each = var.environment_variables \n            content {\n              name  = env.key\n              value = env.value \n            } \n          } \n        } \n      }\n\n} \n \n    selector {\n      match_labels = local.pod_labels \n    } \n  }\nThe selector block tells the Kubernetes Deployment what to target. By\nsetting it to pod_labels, you are telling it to manage deployments for the\nPod Template you just defined. Why doesn’t the Deployment just assume\nthat the Pod Template defined within that Deployment is the one you want\nto target? Well, Kubernetes tries to be an extremely flexible and decoupled\nsystem: e.g., it’s possible to define a Deployment for Pods that are defined\nseparately, so you always need to specify a selector to tell the\nDeployment what to target.\nThat wraps up the kubernetes_deployment resource. The next step is\nto use the kubernetes_service resource to create a Kubernetes\nService:\nresource \"kubernetes_service\" \"app\" { \n  metadata {\n    name = var.name \n  } \n \n  spec {\n    type = \"LoadBalancer\" \n    port {\n      port        = 80\n      target_port = var.container_port\n      protocol    = \"TCP\" \n    }\n    selector = local.pod_labels \n  }\n}\nLet’s go through these parameters:\nmetadata\nJust as with the Deployment object, the Service object uses metadata to\nidentify and target that object in API calls. In the preceding code, I’ve\n\nset the Service name to the name input variable.\ntype\nI’ve configured this Service as type LoadBalancer, which,\ndepending on how your Kubernetes cluster is configured, will deploy a\ndifferent type of load balancer: e.g., in AWS, with EKS, you might get\nan Elastic Load Balancer, whereas in Google Cloud, with GKE, you\nmight get a Cloud Load Balancer.\nport\nI’m configuring the load balancer to route traffic on port 80 (the default\nport for HTTP) to the port the container is listening on.\nselector\nJust as with the Deployment object, the Service object uses a selector to\nspecify what that Service should be targeting. By setting the selector to\npod_labels, the Service and the Deployment will both operate on\nthe same Pods.\nThe final step is to expose the Service endpoint (the load balancer\nhostname) as an output variable in outputs.tf:\nlocals {\n  status = kubernetes_service.app.status\n} \n \noutput \"service_endpoint\" {\n  value = try( \n    \"http://${local.status[0][\"load_balancer\"][0][\"ingress\"][0]\n[\"hostname\"]}\", \n    \"(error parsing hostname from status)\" \n  )\n  description = \"The K8S Service endpoint\"\n}\n\nThis convoluted code needs a bit of explanation. The\nkubernetes_service resource has an output attribute called status\nthat returns the latest status of the Service. I’ve stored this attribute in a\nlocal variable called status. For a Service of type LoadBalancer,\nstatus will contain a complicated object that looks something like this:\n[ \n  {\n    load_balancer = [ \n      {\n        ingress = [ \n          {\n            hostname = \"<HOSTNAME>\" \n          } \n        ] \n      } \n    ] \n  }\n]\nBuried within this deeply nested object is the hostname for the load\nbalancer that you want. This is why the service_endpoint output\nvariable needs to use a complicated sequence of array lookups (e.g., [0])\nand map lookups (e.g., [\"load_balancer\"]) to extract the hostname.\nBut what happens if the status attribute returned by the\nkubernetes_service resource happens to look a little different? In\nthat case, any of those array and map lookups could fail, leading to a\nconfusing error.\nTo handle this error gracefully, I’ve wrapped the entire expression in a\nfunction called try. The try function has the following syntax:\ntry(ARG1, ARG2, ..., ARGN)\nThis function evaluates all the arguments you pass to it and returns the first\nargument that doesn’t produce any errors. Therefore, the\nservice_endpoint output variable will either end up with a hostname\nin it (the first argument) or, if reading the hostname caused an error, the\n\nvariable will instead say “error parsing hostname from status” (the second\nargument).\nOK, that wraps up the k8s-app module. To use it, add a new example in\nexamples/kubernetes-local, and create a main.tf file in it with the following\ncontents:\nmodule \"simple_webapp\" {\n  source = \"../../modules/services/k8s-app\" \n \n  name           = \"simple-webapp\"\n  image          = \"training/webapp\"\n  replicas       = 2\n  container_port = 5000\n}\nThis configures the module to deploy the training/webapp Docker\nimage you ran earlier, with two replicas listening on port 5000, and to name\nall the Kubernetes objects (based on their metadata) “simple-webapp”.\nTo have this module deploy into your local Kubernetes cluster, add the\nfollowing provider block:\nprovider \"kubernetes\" {\n  config_path    = \"~/.kube/config\"\n  config_context = \"docker-desktop\"\n}\nThis code tells the Kubernetes provider to authenticate to your local\nKubernetes cluster by using the docker-desktop context from your\nkubectl config. Run terraform apply to see how it works:\n$ terraform apply \n \n(...) \n \nApply complete! Resources: 2 added, 0 changed, 0 destroyed. \n \nOutputs: \n \nservice_endpoint = \"http://localhost\"\n\nGive the app a few seconds to boot and then try out that\nservice_endpoint:\n$ curl http://localhost \nHello world!\nSuccess!\nThat said, this looks nearly identical to the output of the docker run\ncommand, so was all that extra work worth it? Well, let’s look under the\nhood to see what’s going on. You can use kubectl to explore your cluster.\nFirst, run the get deployments command:\n$ kubectl get deployments \nNAME            READY   UP-TO-DATE   AVAILABLE   AGE \nsimple-webapp   2/2     2            2           3m21s\nYou can see your Kubernetes Deployment, named simple-webapp, as\nthat was the name in the metadata block. This Deployment is reporting\nthat 2/2 Pods (the two replicas) are ready. To see those Pods, run the get\npods command:\n$ kubectl get pods \nNAME                            READY   STATUS    RESTARTS   AGE \nsimple-webapp-d45b496fd-7d447   1/1     Running   0          \n2m36s \nsimple-webapp-d45b496fd-vl6j7   1/1     Running   0          \n2m36s\nSo that’s one difference from docker run already: there are multiple\ncontainers running here, not just one. Moreover, those containers are being\nactively monitored and managed. For example, if one crashed, a\nreplacement will be deployed automatically. You can see this in action by\nrunning the docker ps command:\n$ docker ps \nCONTAINER ID   IMAGE             COMMAND           CREATED         \nSTATUS\n\nb60f5147954a   training/webapp   \"python app.py\"   3 seconds ago   \nUp 2 seconds \nc350ec648185   training/webapp   \"python app.py\"   12 minutes ago  \nUp 12 minutes\nGrab the CONTAINER ID of one of those containers, and use the docker\nkill command to shut it down:\n$ docker kill b60f5147954a\nIf you run docker ps again very quickly, you’ll see just one container\nleft running:\n$ docker ps \nCONTAINER ID   IMAGE             COMMAND           CREATED         \nSTATUS \nc350ec648185   training/webapp   \"python app.py\"   12 minutes ago  \nUp 12 minutes\nBut just a few seconds later, the Kubernetes Deployment will have detected\nthat there is only one replica instead of the requested two, and it’ll launch a\nreplacement container automatically:\n$ docker ps \nCONTAINER ID   IMAGE             COMMAND           CREATED         \nSTATUS \n56a216b8a829   training/webapp   \"python app.py\"   1 second ago    \nUp 5 seconds \nc350ec648185   training/webapp   \"python app.py\"   12 minutes ago  \nUp 12 minutes\nSo Kubernetes is ensuring that you always have the expected number of\nreplicas running. Moreover, it is also running a load balancer to distribute\ntraffic across those replicas, which you can see by running the kubectl\nget services command:\n$ kubectl get services \nNAME            TYPE           CLUSTER-IP     EXTERNAL-IP   \nPORT(S)        AGE \nkubernetes      ClusterIP      10.96.0.1      <none>\n\n443/TCP        4h26m \nsimple-webapp   LoadBalancer   10.110.25.79   localhost     \n80:30234/TCP   4m58s\nThe first service in the list is Kubernetes itself, which you can ignore. The\nsecond is the Service you created, also with the name simple-webapp\n(based on the metadata block). This service runs a load balancer for your\napp: you can see the IP it’s accessible at (localhost) and the port it’s\nlistening on (80).\nKubernetes Deployments also provide automatic rollout of updates. A fun\ntrick with the training/webapp Docker image is that if you set the\nenvironment variable PROVIDER to some value, it’ll use that value instead\nof the word world in the text “Hello, world!” Update examples/kubernetes-\nlocal/main.tf to set this environment variable as follows:\nmodule \"simple_webapp\" {\n  source = \"../../modules/services/k8s-app\" \n \n  name           = \"simple-webapp\"\n  image          = \"training/webapp\"\n  replicas       = 2\n  container_port = 5000 \n \n  environment_variables = {\n    PROVIDER = \"Terraform\" \n  }\n}\nRun apply one more time:\n$ terraform apply \n \n(...) \n \nApply complete! Resources: 0 added, 1 changed, 0 destroyed. \n \nOutputs: \n \nservice_endpoint = \"http://localhost\"' metadata={'original_pages_range': '390-406', 'source': '114_A_Crash_Course_on_Kubernetes', 'file_path': '/mnt/c/Users/Carlos/Documents/Github/jupiter-iaa-azure/Scripts/RAG/../../data/optimized_chunks/Libro-TF/114_A_Crash_Course_on_Kubernetes.pdf', 'num_pages': 17}", "page_content='After a few seconds, try the endpoint again:\n$ curl http://localhost \nHello Terraform!\nAnd there you go, the Deployment has rolled out your change\nautomatically: under the hood, Deployments do a rolling deployment by\ndefault, similar to what you saw with Auto Scaling Groups (note that you\ncan change deployment settings by adding a strategy block to the\nkubernetes_deployment resource).\nDeploying Docker Containers in AWS Using Elastic\nKubernetes Service\nKubernetes has one more trick up its sleeve: it’s fairly portable. That is, you\ncan reuse both the Docker images and the Kubernetes configurations in a\ntotally different cluster and get similar results. To see this in action, let’s\nnow deploy a Kubernetes cluster in AWS.\nSetting up and managing a secure, highly available, scalable Kubernetes\ncluster in the cloud from scratch is complicated. Fortunately, most cloud\nproviders offer managed Kubernetes services, where they run the control\nplane and worker nodes for you: e.g., Elastic Kubernetes Service (EKS) in\nAWS, Azure Kubernetes Service (AKS) in Azure, and Google Kubernetes\nEngine (GKE) in Google Cloud. I’m going to show you how to deploy a\nvery basic EKS cluster in AWS.\nCreate a new module in modules/services/eks-cluster, and define the API\nfor the module in a variables.tf file with the following input variables:\nvariable \"name\" {\n  description = \"The name to use for the EKS cluster\"\n  type        = string\n} \n \nvariable \"min_size\" {\n  description = \"Minimum number of nodes to have in the EKS \ncluster\"\n  type        = number\n\n} \n \nvariable \"max_size\" {\n  description = \"Maximum number of nodes to have in the EKS \ncluster\"\n  type        = number\n} \n \nvariable \"desired_size\" {\n  description = \"Desired number of nodes to have in the EKS \ncluster\"\n  type        = number\n} \n \nvariable \"instance_types\" {\n  description = \"The types of EC2 instances to run in the node \ngroup\"\n  type        = list(string)\n}\nThis code exposes input variables to set the EKS cluster’s name, size, and\nthe types of instances to use for the worker nodes. Next, in main.tf, create\nan IAM role for the control plane:\n# Create an IAM role for the control plane\nresource \"aws_iam_role\" \"cluster\" {\n  name               = \"${var.name}-cluster-role\"\n  assume_role_policy = \ndata.aws_iam_policy_document.cluster_assume_role.json\n} \n \n# Allow EKS to assume the IAM role\ndata \"aws_iam_policy_document\" \"cluster_assume_role\" { \n  statement {\n    effect  = \"Allow\"\n    actions = [\"sts:AssumeRole\"] \n    principals {\n      type        = \"Service\"\n      identifiers = [\"eks.amazonaws.com\"] \n    } \n  }\n} \n \n# Attach the permissions the IAM role needs\nresource \"aws_iam_role_policy_attachment\" \n\"AmazonEKSClusterPolicy\" {\n\npolicy_arn = \"arn:aws:iam::aws:policy/AmazonEKSClusterPolicy\"\n  role       = aws_iam_role.cluster.name\n}\nThis IAM role can be assumed by the EKS service, and it has a Managed\nIAM Policy attached that gives the control plane the permissions it needs.\nNow, add the aws_vpc and aws_subnets data sources to fetch\ninformation about the Default VPC and its subnets:\n# Since this code is only for learning, use the Default VPC and \nsubnets.\n# For real-world use cases, you should use a custom VPC and \nprivate subnets. \n \ndata \"aws_vpc\" \"default\" {\n  default = true\n} \n \ndata \"aws_subnets\" \"default\" { \n  filter {\n    name   = \"vpc-id\"\n    values = [data.aws_vpc.default.id] \n  }\n}\nNow you can create the control plane for the EKS cluster by using the\naws_eks_cluster resource:\nresource \"aws_eks_cluster\" \"cluster\" {\n  name     = var.name\n  role_arn = aws_iam_role.cluster.arn\n  version  = \"1.21\" \n \n  vpc_config {\n    subnet_ids = data.aws_subnets.default.ids \n  } \n \n  # Ensure that IAM Role permissions are created before and \ndeleted after\n  # the EKS Cluster. Otherwise, EKS will not be able to properly \ndelete\n  # EKS managed EC2 infrastructure such as Security Groups.\n  depends_on = [ \n    aws_iam_role_policy_attachment.AmazonEKSClusterPolicy\n\n]\n}\nThe preceding code configures the control plane to use the IAM role you\njust created, and to deploy into the Default VPC and subnets.\nNext up are the worker nodes. EKS supports several different types of\nworker nodes: self-managed EC2 Instances (e.g., in an ASG that you\ncreate), AWS-managed EC2 Instances (known as a managed node group),\nand Fargate (serverless). The simplest option to use for the examples in\nthis chapter will be the managed node groups.\nTo deploy a managed node group, you first need to create another IAM role:\n# Create an IAM role for the node group\nresource \"aws_iam_role\" \"node_group\" {\n  name               = \"${var.name}-node-group\"\n  assume_role_policy = \ndata.aws_iam_policy_document.node_assume_role.json\n} \n \n# Allow EC2 instances to assume the IAM role\ndata \"aws_iam_policy_document\" \"node_assume_role\" { \n  statement {\n    effect  = \"Allow\"\n    actions = [\"sts:AssumeRole\"] \n    principals {\n      type        = \"Service\"\n      identifiers = [\"ec2.amazonaws.com\"] \n    } \n  }\n} \n \n# Attach the permissions the node group needs\nresource \"aws_iam_role_policy_attachment\" \n\"AmazonEKSWorkerNodePolicy\" {\n  policy_arn = \n\"arn:aws:iam::aws:policy/AmazonEKSWorkerNodePolicy\"\n  role       = aws_iam_role.node_group.name\n} \n \nresource \"aws_iam_role_policy_attachment\" \n\"AmazonEC2ContainerRegistryReadOnly\" {\n  policy_arn = \n\"arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryReadOnly\"\n3\n\nrole       = aws_iam_role.node_group.name\n} \n \nresource \"aws_iam_role_policy_attachment\" \"AmazonEKS_CNI_Policy\" \n{\n  policy_arn = \"arn:aws:iam::aws:policy/AmazonEKS_CNI_Policy\"\n  role       = aws_iam_role.node_group.name\n}\nThis IAM role can be assumed by the EC2 service (which makes sense, as\nmanaged node groups use EC2 Instances under the hood), and it has several\nManaged IAM Policies attached that give the managed node group the\npermissions it needs. Now you can use the aws_eks_node_group\nresource to create the managed node group itself:\nresource \"aws_eks_node_group\" \"nodes\" {\n  cluster_name    = aws_eks_cluster.cluster.name\n  node_group_name = var.name\n  node_role_arn   = aws_iam_role.node_group.arn\n  subnet_ids      = data.aws_subnets.default.ids\n  instance_types  = var.instance_types \n \n  scaling_config {\n    min_size     = var.min_size\n    max_size     = var.max_size\n    desired_size = var.desired_size \n  } \n \n  # Ensure that IAM Role permissions are created before and \ndeleted after\n  # the EKS Node Group. Otherwise, EKS will not be able to \nproperly\n  # delete EC2 Instances and Elastic Network Interfaces.\n  depends_on = [ \n    aws_iam_role_policy_attachment.AmazonEKSWorkerNodePolicy, \n    \naws_iam_role_policy_attachment.AmazonEC2ContainerRegistryReadOnly\n, \n    aws_iam_role_policy_attachment.AmazonEKS_CNI_Policy, \n  ]\n}\nThis code configures the managed node group to use the control plane and\nIAM role you just created, to deploy into the Default VPC, and to use the\n\nname, size, and instance type parameters passed in as input variables.\nIn outputs.tf, add the following output variables:\noutput \"cluster_name\" {\n  value       = aws_eks_cluster.cluster.name\n  description = \"Name of the EKS cluster\"\n} \n \noutput \"cluster_arn\" {\n  value       = aws_eks_cluster.cluster.arn\n  description = \"ARN of the EKS cluster\"\n} \n \noutput \"cluster_endpoint\" {\n  value       = aws_eks_cluster.cluster.endpoint\n  description = \"Endpoint of the EKS cluster\"\n} \n \noutput \"cluster_certificate_authority\" {\n  value       = aws_eks_cluster.cluster.certificate_authority\n  description = \"Certificate authority of the EKS cluster\"\n}\nOK, the eks-cluster module is now ready to roll. Let’s use it and the\nk8s-app module from earlier to deploy an EKS cluster and to deploy the\ntraining/webapp Docker image into that cluster. Create\nexamples/kubernetes-eks/main.tf, and configure the eks-cluster\nmodule as follows:\nprovider \"aws\" {\n  region = \"us-east-2\"\n} \n \nmodule \"eks_cluster\" {\n  source = \"../../modules/services/eks-cluster\" \n \n  name         = \"example-eks-cluster\"\n  min_size     = 1\n  max_size     = 2\n  desired_size = 1 \n \n  # Due to the way EKS works with ENIs, t3.small is the smallest\n  # instance type that can be used for worker nodes. If you try\n\n# something smaller like t2.micro, which only has 4 ENIs,\n  # they'll all be used up by system services (e.g., kube-proxy)\n  # and you won't be able to deploy your own Pods.\n  instance_types = [\"t3.small\"]\n}\nNext, configure the k8s-app module as follows:\nprovider \"kubernetes\" {\n  host = module.eks_cluster.cluster_endpoint\n  cluster_ca_certificate = base64decode( \n    module.eks_cluster.cluster_certificate_authority[0].data \n  )\n  token = data.aws_eks_cluster_auth.cluster.token\n} \n \ndata \"aws_eks_cluster_auth\" \"cluster\" {\n  name = module.eks_cluster.cluster_name\n} \n \nmodule \"simple_webapp\" {\n  source = \"../../modules/services/k8s-app\" \n \n  name           = \"simple-webapp\"\n  image          = \"training/webapp\"\n  replicas       = 2\n  container_port = 5000 \n \n  environment_variables = {\n    PROVIDER = \"Terraform\" \n  } \n \n  # Only deploy the app after the cluster has been deployed\n  depends_on = [module.eks_cluster]\n}\nThe preceding code configures the Kubernetes provider to authenticate to\nthe EKS cluster, rather than your local Kubernetes cluster (from Docker\nDesktop). It then uses the k8s-app module to deploy the\ntraining/webapp Docker image exactly the same way as you did when\ndeploying it to Docker Desktop; the only difference is the addition of the\ndepends_on parameter to ensure that Terraform only tries to deploy the\nDocker image after the EKS cluster has been deployed.\n\nNext, pass through the service endpoint as an output variable:\noutput \"service_endpoint\" {\n  value       = module.simple_webapp.service_endpoint\n  description = \"The K8S Service endpoint\"\n}\nOK, now you’re ready to deploy! Run terraform apply as usual (note\nthat EKS clusters can take 10–20 minutes to deploy, so be patient):\n$ terraform apply \n \n(...) \n \nApply complete! Resources: 10 added, 0 changed, 0 destroyed. \n \nOutputs: \n \nservice_endpoint = \"http://774696355.us-east-2.elb.amazonaws.com\"\nWait a little while for the web app to spin up and pass health checks, and\nthen test out the service_endpoint:\n$ curl http://774696355.us-east-2.elb.amazonaws.com \nHello Terraform!\nAnd there you have it! The same Docker image and Kubernetes code is now\nrunning in an EKS cluster in AWS, just the way it ran on your local\ncomputer. All the same features work here too. For example, try updating\nenvironment_variables to a different PROVIDER value, such as\n“Readers”:\nmodule \"simple_webapp\" {\n  source = \"../../modules/services/k8s-app\" \n \n  name           = \"simple-webapp\"\n  image          = \"training/webapp\"\n  replicas       = 2\n  container_port = 5000 \n \n  environment_variables = {\n\nPROVIDER = \"Readers\" \n  } \n \n  # Only deploy the app after the cluster has been deployed\n  depends_on = [module.eks_cluster]\n}\nRerun apply, and just a few seconds later, the Kubernetes Deployment\nwill have deployed the changes:\n$ curl http://774696355.us-east-2.elb.amazonaws.com \nHello Readers!\nThis is one of the advantages of using Docker: changes can be deployed\nvery quickly.\nYou can use kubectl again to see what’s happening in your cluster. To\nauthenticate kubectl to the EKS cluster, you can use the aws eks\nupdate-kubeconfig command to automatically update your\n$HOME/.kube/config file:\n$ aws eks update-kubeconfig --region <REGION> --name \n<EKS_CLUSTER_NAME>\nwhere REGION is the AWS region and EKS_CLUSTER_NAME is the name\nof your EKS cluster. In the Terraform module, you deployed to the us-\neast-2 region and named the cluster kubernetes-example, so the\ncommand will look like this:\n$ aws eks update-kubeconfig --region us-east-2 --name kubernetes-\nexample\nNow, just as before, you can use the get nodes command to inspect the\nworker nodes in your cluster, but this time, add the -o wide flag to get a\nbit more info:\n$ kubectl get nodes \nNAME                             STATUS   AGE   EXTERNAL-IP    \nOS-IMAGE\n\nxxx.us-east-2.compute.internal   Ready    22m   3.134.78.187   \nAmazon Linux 2\nThe preceding snippet is highly truncated to fit into the book, but in the real\noutput, you should be able to see the one worker node, its internal and\nexternal IP, version information, OS information, and much more.\nYou can use the get deployments command to inspect your\nDeployments:\n$ kubectl get deployments \nNAME            READY   UP-TO-DATE   AVAILABLE   AGE \nsimple-webapp   2/2     2            2           19m\nNext, run get pods to see the Pods:\n$ kubectl get pods \nNAME            READY   UP-TO-DATE   AVAILABLE   AGE \nsimple-webapp   2/2     2            2           19m\nAnd finally, run get services to see the Services:\n$ kubectl get services \nNAME            TYPE           EXTERNAL-IP                         \nPORT(S) \nkubernetes      ClusterIP      <none>                              \n443/TCP \nsimple-webapp   LoadBalancer   774696355.us-east-\n2.elb.amazonaws.com    80/TCP\nYou should be able to see your load balancer and the URL you used to test\nit.\nSo there you have it: two different providers, both working in the same\ncloud, helping you to deploy containerized workloads.\nThat said, just as in previous sections, I want to leave you with a few\nwarnings:\nWarning 1: These Kubernetes examples are very simplified!\n\nKubernetes is complicated, and it’s rapidly evolving and changing;\ntrying to explain all the details can easily fill a book all by itself. Since\nthis is a book about Terraform, and not Kubernetes, my goal with the\nKubernetes examples in this chapter was to keep them as simple and\nminimal as possible. Therefore, while I hope the code examples you’ve\nseen have been useful from a learning and experimentation perspective,\nif you are going to use Kubernetes for real-world, production use cases,\nyou’ll need to change many aspects of this code, such as configuring a\nnumber of additional services and settings in the eks-cluster\nmodule (e.g., ingress controllers, secret envelope encryption, security\ngroups, OIDC authentication, Role-Based Access Control (RBAC)\nmapping, VPC CNI, kube-proxy, CoreDNS), exposing many other\nsettings in the k8s-app module (e.g., secrets management, volumes,\nliveness probes, readiness probes, labels, annotations, multiple ports,\nmultiple containers), and using a custom VPC with private subnets for\nyour EKS cluster instead of the Default VPC and public subnets.\nWarning 2: Use multiple providers sparingly\nAlthough you certainly can use multiple providers in a single module, I\ndon’t recommend doing it too often, for similar reasons to why I don’t\nrecommend using provider aliases too often: in most cases, you want\neach provider to be isolated in its own module so that you can manage it\nseparately and limit the blast radius from mistakes or attackers.\nMoreover, Terraform doesn’t have great support for dependency\nordering between providers. For example, in the Kubernetes example,\nyou had a single module that deployed both the EKS cluster, using the\nAWS Provider, and a Kubernetes app into that cluster, using the\nKubernetes provider. As it turns out, the Kubernetes provider\ndocumentation explicitly recommends against this pattern:\n4' metadata={'original_pages_range': '407-417', 'source': '115_Deploying_Docker_Containers_in_AWS_Using_Elastic_Kubernetes_Service', 'file_path': '/mnt/c/Users/Carlos/Documents/Github/jupiter-iaa-azure/Scripts/RAG/../../data/optimized_chunks/Libro-TF/115_Deploying_Docker_Containers_in_AWS_Using_Elastic_Kubernetes_Service.pdf', 'num_pages': 11}", "page_content='When using interpolation to pass credentials to the Kubernetes\nprovider from other resources, these resources SHOULD NOT be\ncreated in the same Terraform module where Kubernetes provider\nresources are also used. This will lead to intermittent and\nunpredictable errors which are hard to debug and diagnose. The root\nissue lies with the order in which Terraform itself evaluates the\nprovider blocks vs. actual resources.\nThe example code in this book is able to work around these issues by\ndepending on the aws_eks_cluster_auth data source, but that’s a\nbit of a hack. Therefore, in production code, I always recommend\ndeploying the EKS cluster in one module and deploying Kubernetes\napps in separate modules, after the cluster has been deployed.\nConclusion\nAt this point, you hopefully understand how to work with multiple\nproviders in Terraform code, and you can answer the three questions from\nthe beginning of this chapter:\nWhat if you need to deploy to multiple AWS regions?\nUse multiple provider blocks, each configured with a different\nregion and alias parameter.\nWhat if you need to deploy to multiple AWS accounts?\nUse multiple provider blocks, each configured with a different\nassume_role block and an alias parameter.\nWhat if you need to deploy to other clouds, such as Azure or GCP or\nKubernetes?\nUse multiple provider blocks, each configured for its respective\ncloud.\n\nHowever, you’ve also seen that using multiple providers in one module is\ntypically an antipattern. So the real answer to these questions, especially in\nreal-world, production use cases, is to use each provider in a separate\nmodule to keep different regions, accounts, and clouds isolated from one\nanother, and to limit your blast radius.\nLet’s now move on to Chapter 8, where I’ll go over several other patterns\nfor how to build Terraform modules for real-world, production use cases—\nthe kind of modules you could bet your company on.\n1 In fact, you could even skip the provider block and just add any resource or data source\nfrom an official provider and Terraform will figure out which provider to use based on the\nprefix: for example, if you add the aws_instance resource, Terraform will know to use the\nAWS Provider based on the aws_ prefix.\n2 See “Multi-Cloud is the Worst Practice”.\n3 For a comparison of the different types of EKS worker nodes, see the Gruntwork blog.\n4 Alternatively, you can use off-the-shelf production-grade Kubernetes modules, such as the\nones in the Gruntwork Infrastructure as Code Library.' metadata={'original_pages_range': '418-419', 'source': '116_Conclusion', 'file_path': '/mnt/c/Users/Carlos/Documents/Github/jupiter-iaa-azure/Scripts/RAG/../../data/optimized_chunks/Libro-TF/116_Conclusion.pdf', 'num_pages': 2}", "page_content='Chapter 8. Production-Grade\nTerraform Code\nBuilding production-grade infrastructure is difficult. And stressful. And\ntime consuming. By production-grade infrastructure, I mean the kind of\ninfrastructure you’d bet your company on. You’re betting that your\ninfrastructure won’t fall over if traffic goes up, or lose your data if there’s\nan outage, or allow that data to be compromised when hackers try to break\nin—and if that bet doesn’t work out, your company might go out of\nbusiness. That’s what’s at stake when I refer to production-grade\ninfrastructure in this chapter.\nI’ve had the opportunity to work with hundreds of companies, and based on\nall of these experiences, here’s roughly how long you should expect your\nnext production-grade infrastructure project to take:\nIf you want to deploy a service fully managed by a third party, such as\nrunning MySQL using the AWS Relational Database Service (RDS),\nyou can expect it to take you one to two weeks to get that service ready\nfor production.\nIf you want to run your own stateless distributed app, such as a cluster\nof Node.js apps that don’t store any data locally (e.g., they store all\ntheir data in RDS) running on top of an AWS Auto Scaling Group\n(ASG), that will take roughly twice as long, or about two to four weeks\nto get ready for production.\nIf you want to run your own stateful distributed app, such as an\nElasticsearch cluster that runs on top of an ASG and stores data on\nlocal disks, that will be another order-of-magnitude increase, or about\ntwo to four months to get ready for production.\n\nIf you want to build out your entire architecture, including all of your\napps, data stores, load balancers, monitoring, alerting, security, and so\non, that’s another order-of-magnitude (or two) increase, or about 6 to\n36 months of work, with small companies typically being closer to six\nmonths and larger companies typically taking several years.\nTable 8-1 shows a summary of this data.\nTable 8-1. How long it takes to build production-grade infrastructure from\nscratch\nType of infrastructure Example Time estimate\nManaged service Amazon RDS 1–2 weeks\nSelf-managed distributed system\n(stateless)\nA cluster of Node.js apps in an ASG2–4 weeks\nSelf-managed distributed system\n(stateful)\nElasticsearch cluster 2–4 months\nEntire architecture Apps, data stores, load balancers,\nmonitoring, etc.\n6–36 months\nIf you haven’t gone through the process of building out production-grade\ninfrastructure, you may be surprised by these numbers. I often hear\nreactions like, “How can it possibly take that long?” or “I can deploy a\nserver on <cloud> in a few minutes. Surely it can’t take months to get the\nrest done!” And all too often, from many an overconfident engineer, “I’m\nsure those numbers apply to other people, but I will be able to get this done\nin a few days.”\nAnd yet, anyone who has gone through a major cloud migration or\nassembled a brand-new infrastructure from scratch knows that these\nnumbers, if anything, are optimistic—a best-case scenario, really. If you\ndon’t have people on your team with deep expertise in building production-\ngrade infrastructure, or if your team is being pulled in a dozen different' metadata={'original_pages_range': '420-421', 'source': '117_8._Production-Grade_Terraform_Code', 'file_path': '/mnt/c/Users/Carlos/Documents/Github/jupiter-iaa-azure/Scripts/RAG/../../data/optimized_chunks/Libro-TF/117_8._Production-Grade_Terraform_Code.pdf', 'num_pages': 2}", "page_content='directions and you can’t find the time to focus on it, it might take you\nsignificantly longer.\nIn this chapter, I’ll go over why it takes so long to build production-grade\ninfrastructure, what production grade really means, and what patterns work\nbest for creating reusable, production-grade modules:\nWhy it takes so long to build production-grade infrastructure\nThe production-grade infrastructure checklist\nProduction-grade infrastructure modules\nSmall modules\nComposable modules\nTestable modules\nVersioned modules\nBeyond Terraform modules\nEXAMPLE CODE\nAs a reminder, you can find all of the code examples in the book on GitHub.\nWhy It Takes So Long to Build Production-\nGrade Infrastructure\nTime estimates for software projects are notoriously inaccurate. Time\nestimates for DevOps projects, doubly so. That quick tweak that you\nthought would take five minutes takes up the entire day; the minor new\nfeature that you estimated at a day of work takes two weeks; the app that\nyou thought would be in production in two weeks is still not quite there six\nmonths later. Infrastructure and DevOps projects, perhaps more than any\nother type of software, are the ultimate examples of Hofstadter’s Law:1\n\nHofstadter’s Law: It always takes longer than you expect, even when you\ntake into account Hofstadter’s Law.\nI think there are three major reasons for this. The first reason is that\nDevOps, as an industry, is still in the Stone Age. I don’t mean that as an\ninsult but rather in the sense that the industry is still in its infancy. The\nterms “cloud computing,” “infrastructure as code,” and “DevOps” only\nappeared in the mid- to late-2000s, and tools like Terraform, Docker,\nPacker, and Kubernetes were all initially released in the mid- to late-2010s.\nAll of these tools and techniques are relatively new, and all of them are\nchanging rapidly. This means that they are not particularly mature and few\npeople have deep experience with them, so it’s no surprise that projects take\nlonger than expected.\nThe second reason is that DevOps seems to be particularly susceptible to\nyak shaving. If you haven’t heard of “yak shaving” before, I assure you, this\nis a term that you will grow to love (and hate). The best definition I’ve seen\nof this term comes from a blog post by Seth Godin:\n“I want to wax the car today.”\n“Oops, the hose is still broken from the winter. I’ll need to buy a new one\nat Home Depot.”\n“But Home Depot is on the other side of the Tappan Zee bridge and\ngetting there without my EZPass is miserable because of the tolls.”\n“But, wait! I could borrow my neighbor’s EZPass…”\n“Bob won’t lend me his EZPass until I return the mooshi pillow my son\nborrowed, though.”\n“And we haven’t returned it because some of the stuffing fell out and we\nneed to get some yak hair to restuff it.”\nAnd the next thing you know, you’re at the zoo, shaving a yak, all so you\ncan wax your car.\n2\n\nYak shaving consists of all the tiny, seemingly unrelated tasks you must do\nbefore you can do the task you originally wanted to do. If you develop\nsoftware, and especially if you work in the DevOps industry, you’ve\nprobably seen this sort of thing a thousand times. You go to deploy a fix for\na small typo, only to uncover a bug in your app configuration. You try to\ndeploy a fix for the app configuration, but that’s blocked by a TLS\ncertificate issue. After spending hours on Stack Overflow, you try to roll out\na fix for the TLS issue, but that fails due to a problem with your\ndeployment system. You spend hours digging into that problem and find out\nit’s due to an out-of-date Linux version. The next thing you know, you’re\nupdating the operating system on your entire fleet of servers, all so you can\ndeploy a “quick” one-character typo fix.\nDevOps seems to be especially prone to these sorts of yak-shaving\nincidents. In part, this is a consequence of the immaturity of DevOps\ntechnologies and modern system design, which often involves lots of tight\ncoupling and duplication in the infrastructure. Every change you make in\nthe DevOps world is a bit like trying to pull out one wire from a box of\ntangled wires—it just tends to pull up everything else in the box with it. In\npart, this is because the term “DevOps” covers an astonishingly broad set of\ntopics: everything from build to deployment to security and so on.\nThis brings us to the third reason why DevOps work takes so long. The first\ntwo reasons—DevOps is in the Stone Age and yak shaving—can be\nclassified as accidental complexity. Accidental complexity refers to the\nproblems imposed by the particular tools and processes you’ve chosen, as\nopposed to essential complexity, which refers to the problems inherent in\nwhatever it is that you’re working on. For example, if you’re using C++ to\nwrite stock-trading algorithms, dealing with memory allocation bugs is\naccidental complexity: had you picked a different programming language\nwith automatic memory management, you wouldn’t have this as a problem\nat all. On the other hand, figuring out an algorithm that can beat the market\nis essential complexity: you’d have to solve this problem no matter what\nprogramming language you picked.\n3' metadata={'original_pages_range': '422-424', 'source': '118_Why_It_Takes_So_Long_to_Build_Production-Grade_Infrastructure', 'file_path': '/mnt/c/Users/Carlos/Documents/Github/jupiter-iaa-azure/Scripts/RAG/../../data/optimized_chunks/Libro-TF/118_Why_It_Takes_So_Long_to_Build_Production-Grade_Infrastructure.pdf', 'num_pages': 3}", "page_content='The third reason why DevOps takes so long—the essential complexity of\nthis problem—is that there is a genuinely long checklist of tasks that you\nmust do to prepare infrastructure for production. The problem is that the\nvast majority of developers don’t know about most of the items on the\nchecklist, so when they estimate a project, they forget about a huge number\nof critical and time-consuming details. This checklist is the focus of the\nnext section.\nThe Production-Grade Infrastructure\nChecklist\nHere’s a fun experiment: go around your company and ask, “What are the\nrequirements for going to production?” In most companies, if you ask this\nquestion to five people, you’ll get five different answers. One person will\nmention the need for metrics and alerts; another will talk about capacity\nplanning and high availability; someone else will go on a rant about\nautomated tests and code reviews; yet another person will bring up\nencryption, authentication, and server hardening; and if you’re lucky,\nsomeone might remember to bring up data backups and log aggregation.\nMost companies do not have a clear definition of the requirements for going\nto production, which means each piece of infrastructure is deployed a little\ndifferently and can be missing some critical functionality.\nTo help improve this situation, I’d like to share with you the Production-\nGrade Infrastructure Checklist, as shown in Table 8-2. This list covers most\nof the key items that you need to consider to deploy infrastructure to\nproduction.\n\nTable 8-2. The Production-Grade Infrastructure Checklist\nTask Description Example tools\nInstall Install the software binaries and all dependencies.Bash, Ansible,\nDocker, Packer\nConfigure Configure the software at runtime. Includes port settings,\nTLS certs, service discovery, leaders, followers,\nreplication, etc.\nChef, Ansible,\nKubernetes\nProvision Provision the infrastructure. Includes servers, load\nbalancers, network configuration, firewall settings, IAM\npermissions, etc.\nTerraform,\nCloudFormation\nDeploy Deploy the service on top of the infrastructure. Roll out\nupdates with no downtime. Includes blue-green, rolling,\nand canary deployments.\nASG, Kubernetes,\nECS\nHigh availabilityWithstand outages of individual processes, servers,\nservices, datacenters, and regions.\nMulti-datacenter,\nmulti-region\nScalability Scale up and down in response to load. Scale\nhorizontally (more servers) and/or vertically (bigger\nservers).\nAuto scaling,\nreplication\nPerformance Optimize CPU, memory, disk, network, and GPU usage.\nIncludes query tuning, benchmarking, load testing, and\nprofiling.\nDynatrace,\nValgrind,\nVisualVM\nNetworking Configure static and dynamic IPs, ports, service\ndiscovery, firewalls, DNS, SSH access, and VPN access.\nVPCs, firewalls,\nRoute 53\nSecurity Encryption in transit (TLS) and on disk, authentication,\nauthorization, secrets management, server hardening.\nACM, Let’s\nEncrypt, KMS,\nVault\nMetrics Availability metrics, business metrics, app metrics,\nserver metrics, events, observability, tracing, and\nalerting.\nCloudWatch,\nDatadog\nLogs Rotate logs on disk. Aggregate log data to a central\nlocation.\nElastic Stack,\nSumo Logic\nData backup Make backups of DBs, caches, and other data on a\nscheduled basis. Replicate to separate region/account.\nAWS Backup, RDS\nsnapshots' metadata={'original_pages_range': '425-426', 'source': '119_The_Production-Grade_Infrastructure_Checklist', 'file_path': '/mnt/c/Users/Carlos/Documents/Github/jupiter-iaa-azure/Scripts/RAG/../../data/optimized_chunks/Libro-TF/119_The_Production-Grade_Infrastructure_Checklist.pdf', 'num_pages': 2}", "page_content='Task Description Example tools\nCost optimizationPick proper Instance types, use spot and reserved\nInstances, use auto scaling, and clean up unused\nresources.\nAuto scaling,\nInfracost\nDocumentation Document your code, architecture, and practices. Create\nplaybooks to respond to incidents.\nREADMEs, wikis,\nSlack, IaC\nTests Write automated tests for your infrastructure code. Run\ntests after every commit and nightly.\nTerratest, tflint,\nOPA, InSpec\nMost developers are aware of the first few tasks: install, configure,\nprovision, and deploy. It’s all the ones that come after them that catch\npeople off guard. For example, did you think through the resilience of your\nservice and what happens if a server goes down? Or a load balancer goes\ndown? Or an entire datacenter goes dark? Networking tasks are also\nnotoriously tricky: setting up VPCs, VPNs, service discovery, and SSH\naccess are all essential tasks that can take months and yet are often entirely\nleft out of many project plans and time estimates. Security tasks, such as\nencrypting data in transit using TLS, dealing with authentication, and\nfiguring out how to store secrets, are also often forgotten until the last\nminute.\nEvery time you’re working on a new piece of infrastructure, go through this\nchecklist. Not every single piece of infrastructure needs every single item\non the list, but you should consciously and explicitly document which items\nyou’ve implemented, which ones you’ve decided to skip, and why.\nProduction-Grade Infrastructure Modules\nNow that you know the list of tasks that you need to do for each piece of\ninfrastructure, let’s talk about the best practices for building reusable\nmodules to implement these tasks. Here are the topics I’ll cover:\nSmall modules\nComposable modules' metadata={'original_pages_range': '427', 'source': '120_Production-Grade_Infrastructure_Modules', 'file_path': '/mnt/c/Users/Carlos/Documents/Github/jupiter-iaa-azure/Scripts/RAG/../../data/optimized_chunks/Libro-TF/120_Production-Grade_Infrastructure_Modules.pdf', 'num_pages': 1}", "page_content='Testable modules\nVersioned modules\nBeyond Terraform modules\nSmall Modules\nDevelopers who are new to Terraform, and IaC in general, often define all\nof their infrastructure for all of their environments (dev, stage, prod, etc.) in\na single file or single module. As discussed in “State File Isolation”, this is\na bad idea. In fact, I’ll go even further and make the following claim: large\nmodules—modules that contain more than a few hundred lines of code or\nthat deploy more than a few closely related pieces of infrastructure—should\nbe considered harmful.\nHere are just a few of the downsides of large modules:\nLarge modules are slow\nIf all of your infrastructure is defined in one Terraform module, running\nany command will take a long time. I’ve seen modules grow so large\nthat terraform plan takes 20 minutes to run!\nLarge modules are insecure\nIf all your infrastructure is managed in a single large module, to change\nanything, you need permissions to access everything. This means that\nalmost every user must be an admin, which goes against the principle of\nleast privilege.\nLarge modules are risky\nIf all your eggs are in one basket, a mistake anywhere could break\neverything. You might be making a minor change to a frontend app in\nstaging, but due to a typo or running the wrong command, you delete\nthe production database.\nLarge modules are difficult to understand\n\nThe more code you have in one place, the more difficult it is for any one\nperson to understand it all. And when you don’t understand the\ninfrastructure you’re dealing with, you end up making costly mistakes.\nLarge modules are difficult to review\nReviewing a module that consists of several dozen lines of code is easy;\nreviewing a module that consists of several thousand lines of code is\nnearly impossible. Moreover, terraform plan not only takes\nlonger to run, but if the output of the plan command is several\nthousand lines, no one will bother to read it. And that means no one will\nnotice that one little red line that means your database is being deleted.\nLarge modules are difficult to test\nTesting infrastructure code is hard; testing a large amount of\ninfrastructure code is nearly impossible. I’ll come back to this point in\nChapter 9.\nIn short, you should build your code out of small modules that each do one\nthing. This is not a new or controversial insight. You’ve probably heard it\nmany times before, albeit in slightly different contexts, such as this version\nfrom Clean Code:\nThe first rule of functions is that they should be small. The second rule of\nfunctions is that they should be smaller than that.\nImagine you were using a general-purpose programming language such as\nJava or Python or Ruby, and you came across a single function that was\n20,000 lines long—you would immediately know this is a code smell. The\nbetter approach is to refactor this code into a number of small, standalone\nfunctions that each do one thing. You should use the same strategy with\nTerraform.\nImagine that you came across the architecture shown in Figure 8-1.\n4\n\n\n\nFigure 8-1. A relatively complicated AWS architecture.\nIf this architecture was defined in a single Terraform module that was\n20,000 lines long, you should immediately think of it as a code smell. The\nbetter approach is to refactor this module into a number of small, standalone\nmodules that each do one thing, as shown in Figure 8-2.\n\n\n\nFigure 8-2. A relatively complicated AWS architecture refactored into many small modules.\nFor example, consider the webserver-cluster module, which you last\nworked on in Chapter 5. This module has become fairly large, as it is\nhandling three somewhat unrelated tasks:\nAuto Scaling Group (ASG)\nThe webserver-cluster module deploys an ASG that can do a\nzero-downtime, rolling deployment.\nApplication Load Balancer (ALB)\nThe webserver-cluster deploys an ALB.\nHello, World app\nThe webserver-cluster module also deploys a simple “Hello,\nWorld” app.\nLet’s refactor the code accordingly into three smaller modules:\nmodules/cluster/asg-rolling-deploy\nA generic, reusable, standalone module for deploying an ASG that can\ndo a zero-downtime, rolling deployment.\nmodules/networking/alb\nA generic, reusable, standalone module for deploying an ALB.\nmodules/services/hello-world-app\nA module specifically for deploying the “Hello, World” app, which uses\nthe asg-rolling-deploy and alb modules under the hood.\nBefore getting started, make sure to run terraform destroy on any\nwebserver-cluster deployments you have from previous chapters.\nAfter you do that, you can start putting together the asg-rolling-\n\ndeploy and alb modules. Create a new folder at modules/cluster/asg-\nrolling-deploy, and move the following resources from\nmodule/services/webserver-cluster/main.tf to modules/cluster/asg-rolling-\ndeploy/main.tf:\naws_launch_configuration\naws_autoscaling_group\naws_autoscaling_schedule (both of them)\naws_security_group (for the Instances but not for the ALB)\naws_security_group_rule (just the one rule for the Instances\nbut not those for the ALB)\naws_cloudwatch_metric_alarm (both of them)\nNext, move the following variables from module/services/webserver-\ncluster/variables.tf to modules/cluster/asg-rolling-deploy/variables.tf:\ncluster_name\nami\ninstance_type\nmin_size\nmax_size\nenable_autoscaling\ncustom_tags\nserver_port\nLet’s now move on to the ALB module. Create a new folder at\nmodules/networking/alb, and move the following resources from\nmodule/services/webserver-cluster/main.tf to\nmodules/networking/alb/main.tf:' metadata={'original_pages_range': '428-434', 'source': '121_Small_Modules', 'file_path': '/mnt/c/Users/Carlos/Documents/Github/jupiter-iaa-azure/Scripts/RAG/../../data/optimized_chunks/Libro-TF/121_Small_Modules.pdf', 'num_pages': 7}", "page_content='aws_lb\naws_lb_listener\naws_security_group (the one for the ALB but not for the\nInstances)\naws_security_group_rule (both of the rules for the ALB but\nnot the one for the Instances)\nCreate modules/networking/alb/variables.tf, and define a single variable\nwithin:\nvariable \"alb_name\" {\n  description = \"The name to use for this ALB\"\n  type        = string\n}\nUse this variable as the name argument of the aws_lb resource:\nresource \"aws_lb\" \"example\" {\n  name               = var.alb_name\n  load_balancer_type = \"application\"\n  subnets            = data.aws_subnets.default.ids\n  security_groups    = [aws_security_group.alb.id]\n}\nAnd the name argument of the aws_security_group resource:\nresource \"aws_security_group\" \"alb\" {\n  name = var.alb_name\n}\nThis is a lot of code to shuffle around, so feel free to use the code examples\nfor this chapter from GitHub.\nComposable Modules\nYou now have two small modules—asg-rolling-deploy and alb—\nthat each do one thing and do it well. How do you make them work\n\ntogether? How do you build modules that are reusable and composable?\nThis question is not unique to Terraform but is something programmers\nhave been thinking about for decades. To quote Doug McIlroy,  the original\ndeveloper of Unix pipes and a number of other Unix tools, including diff,\nsort, join, and tr:\nThis is the Unix philosophy: Write programs that do one thing and do it\nwell. Write programs to work together.\nOne way to do this is through function composition, in which you can take\nthe outputs of one function and pass them as the inputs to another. For\nexample, if you had the following small functions in Ruby:\n# Simple function to do addition\ndef add(x, y) \n  return x + y\nend \n \n# Simple function to do subtraction\ndef sub(x, y) \n  return x - y\nend \n \n# Simple function to do multiplication\ndef multiply(x, y) \n  return x * y\nend\nyou can use function composition to put them together by taking the outputs\nfrom add and sub and passing them as the inputs to multiply:\n# Complex function that composes several simpler functions\ndef do_calculation(x, y) \n  return multiply(add(x, y), sub(x, y))\nend\nOne of the main ways to make functions composable is to minimize side\neffects: that is, where possible, avoid reading state from the outside world\nand instead have it passed in via input parameters, and avoid writing state to\nthe outside world and instead return the result of your computations via\n5\n\noutput parameters. Minimizing side effects is one of the core tenets of\nfunctional programming because it makes the code easier to reason about,\neasier to test, and easier to reuse. The reuse story is particularly compelling,\nsince function composition allows you to gradually build up more\ncomplicated functions by combining simpler functions.\nAlthough you can’t avoid side effects when working with infrastructure\ncode, you can still follow the same basic principles in your Terraform\nmodules: pass everything in through input variables, return everything\nthrough output variables, and build more complicated modules by\ncombining simpler modules.\nOpen up modules/cluster/asg-rolling-deploy/variables.tf, and add four new\ninput variables:\nvariable \"subnet_ids\" {\n  description = \"The subnet IDs to deploy to\"\n  type        = list(string)\n} \n \nvariable \"target_group_arns\" {\n  description = \"The ARNs of ELB target groups in which to \nregister Instances\"\n  type        = list(string)\n  default     = []\n} \n \nvariable \"health_check_type\" {\n  description = \"The type of health check to perform. Must be one \nof: EC2, ELB.\"\n  type        = string\n  default     = \"EC2\"\n} \n \nvariable \"user_data\" {\n  description = \"The User Data script to run in each Instance at \nboot\"\n  type        = string\n  default     = null\n}\n\nThe first variable, subnet_ids, tells the asg-rolling-deploy\nmodule what subnets to deploy into. Whereas the webserver-cluster\nmodule was hardcoded to deploy into the Default VPC and subnets, by\nexposing the subnet_ids variable, you allow this module to be used\nwith any VPC or subnets. The next two variables, target_group_arns\nand health_check_type, configure how the ASG integrates with load\nbalancers. Whereas the webserver-cluster module had a built-in\nALB, the asg-rolling-deploy module is meant to be a generic\nmodule, so exposing the load-balancer settings as input variables allows\nyou to use the ASG with a wide variety of use cases; e.g., no load balancer,\none ALB, multiple NLBs, and so on.\nTake these three new input variables and pass them through to the\naws_autoscaling_group resource in modules/cluster/asg-rolling-\ndeploy/main.tf, replacing the previously hardcoded settings that were\nreferencing resources (e.g., the ALB) and data sources (e.g.,\naws_subnets) that we didn’t copy into the asg-rolling-deploy\nmodule:\nresource \"aws_autoscaling_group\" \"example\" {\n  name                 = var.cluster_name\n  launch_configuration = aws_launch_configuration.example.name\n  vpc_zone_identifier  = var.subnet_ids \n \n  # Configure integrations with a load balancer\n  target_group_arns    = var.target_group_arns\n  health_check_type    = var.health_check_type \n \n  min_size = var.min_size\n  max_size = var.max_size \n \n  # (...)\n}\nThe fourth variable, user_data, is for passing in a User Data script.\nWhereas the webserver-cluster module had a hardcoded User Data\nscript that could only be used to deploy a “Hello, World” app, by taking in a\nUser Data script as an input variable, the asg-rolling-deploy\n\nmodule can be used to deploy any app across an ASG. Pass this\nuser_data variable through to the aws_launch_configuration\nresource:\nresource \"aws_launch_configuration\" \"example\" {\n  image_id        = var.ami\n  instance_type   = var.instance_type\n  security_groups = [aws_security_group.instance.id]\n  user_data       = var.user_data \n \n  # Required when using a launch configuration with an auto \nscaling group. \n  lifecycle {\n    create_before_destroy = true \n  }\n}\nYou’ll also want to add a couple of useful output variables to\nmodules/cluster/asg-rolling-deploy/outputs.tf:\noutput \"asg_name\" {\n  value       = aws_autoscaling_group.example.name\n  description = \"The name of the Auto Scaling Group\"\n} \n \noutput \"instance_security_group_id\" {\n  value       = aws_security_group.instance.id\n  description = \"The ID of the EC2 Instance Security Group\"\n}\nOutputting this data makes the asg-rolling-deploy module even\nmore reusable, since consumers of the module can use these outputs to add\nnew behaviors, such as attaching custom rules to the security group.\nFor similar reasons, you should add several output variables to\nmodules/networking/alb/outputs.tf:\noutput \"alb_dns_name\" {\n  value       = aws_lb.example.dns_name\n  description = \"The domain name of the load balancer\"\n}\n\noutput \"alb_http_listener_arn\" {\n  value       = aws_lb_listener.http.arn\n  description = \"The ARN of the HTTP listener\"\n} \n \noutput \"alb_security_group_id\" {\n  value       = aws_security_group.alb.id\n  description = \"The ALB Security Group ID\"\n}\nYou’ll see how to use these shortly.\nThe last step is to convert the webserver-cluster module into a\nhello-world-app module that can deploy a “Hello, World” app using\nthe asg-rolling-deploy and alb modules. To do this, rename\nmodule/services/webserver-cluster to module/services/hello-world-app.\nAfter all the changes in the previous steps, you should have only the\nfollowing resources and data sources left in module/services/hello-world-\napp/main.tf:\naws_lb_target_group\naws_lb_listener_rule\nterraform_remote_state (for the DB)\naws_vpc\naws_subnets\nAdd the following variable to modules/services/hello-world-\napp/variables.tf:\nvariable \"environment\" {\n  description = \"The name of the environment we're deploying to\"\n  type        = string\n}\nNow, add the asg-rolling-deploy module that you created earlier to\nthe hello-world-app module to deploy an ASG:\n\nmodule \"asg\" {\n  source = \"../../cluster/asg-rolling-deploy\" \n \n  cluster_name  = \"hello-world-${var.environment}\"\n  ami           = var.ami\n  instance_type = var.instance_type \n \n  user_data     = templatefile(\"${path.module}/user-data.sh\", {\n    server_port = var.server_port\n    db_address  = data.terraform_remote_state.db.outputs.address\n    db_port     = data.terraform_remote_state.db.outputs.port\n    server_text = var.server_text \n  }) \n \n  min_size           = var.min_size\n  max_size           = var.max_size\n  enable_autoscaling = var.enable_autoscaling \n \n  subnet_ids        = data.aws_subnets.default.ids\n  target_group_arns = [aws_lb_target_group.asg.arn]\n  health_check_type = \"ELB\" \n \n  custom_tags = var.custom_tags\n}\nAnd add the alb module, also that you created earlier, to the hello-\nworld-app module to deploy an ALB:\nmodule \"alb\" {\n  source = \"../../networking/alb\" \n \n  alb_name   = \"hello-world-${var.environment}\"\n  subnet_ids = data.aws_subnets.default.ids\n}\nNote the use of the input variable environment as a way to enforce a\nnaming convention, so all of your resources will be namespaced based on\nthe environment (e.g., hello-world-stage, hello-world-prod).\nThis code also sets the new subnet_ids, target_group_arns,\nhealth_check_type, and user_data variables you added earlier to\nappropriate values.\n\nNext, you need to configure the ALB target group and listener rule for this\napp. Update the aws_lb_target_group resource in\nmodules/services/hello-world-app/main.tf to use environment in its\nname:\nresource \"aws_lb_target_group\" \"asg\" {\n  name     = \"hello-world-${var.environment}\"\n  port     = var.server_port\n  protocol = \"HTTP\"\n  vpc_id   = data.aws_vpc.default.id \n \n  health_check {\n    path                = \"/\"\n    protocol            = \"HTTP\"\n    matcher             = \"200\"\n    interval            = 15\n    timeout             = 3\n    healthy_threshold   = 2\n    unhealthy_threshold = 2 \n  }\n}\nNow, update the listener_arn parameter of the\naws_lb_listener_rule resource to point at the\nalb_http_listener_arn output of the ALB module:\nresource \"aws_lb_listener_rule\" \"asg\" {\n  listener_arn = module.alb.alb_http_listener_arn\n  priority     = 100 \n \n  condition { \n    path_pattern {\n      values = [\"*\"] \n    } \n  } \n \n  action {\n    type             = \"forward\"\n    target_group_arn = aws_lb_target_group.asg.arn \n  }\n}' metadata={'original_pages_range': '435-442', 'source': '122_Composable_Modules', 'file_path': '/mnt/c/Users/Carlos/Documents/Github/jupiter-iaa-azure/Scripts/RAG/../../data/optimized_chunks/Libro-TF/122_Composable_Modules.pdf', 'num_pages': 8}", "page_content='Finally, pass through the important outputs from the asg-rolling-\ndeploy and alb modules as outputs of the hello-world-app\nmodule:\noutput \"alb_dns_name\" {\n  value       = module.alb.alb_dns_name\n  description = \"The domain name of the load balancer\"\n} \n \noutput \"asg_name\" {\n  value       = module.asg.asg_name\n  description = \"The name of the Auto Scaling Group\"\n} \n \noutput \"instance_security_group_id\" {\n  value       = module.asg.instance_security_group_id\n  description = \"The ID of the EC2 Instance Security Group\"\n}\nThis is function composition at work: you’re building up more complicated\nbehavior (a “Hello, World” app) from simpler parts (ASG and ALB\nmodules).\nTestable Modules\nAt this stage, you’ve written a whole lot of code in the form of three\nmodules: asg-rolling-deploy, alb, and hello-world-app. The\nnext step is to check that your code actually works.\nThe modules you’ve created aren’t root modules meant to be deployed\ndirectly. To deploy them, you need to write some Terraform code to plug in\nthe arguments you want, set up the provider, configure the backend,\nand so on. A great way to do this is to create an examples folder that, as the\nname suggests, shows examples of how to use your modules. Let’s try it\nout.\nCreate examples/asg/main.tf with the following contents:\nprovider \"aws\" {\n  region = \"us-east-2\"\n\n} \n \nmodule \"asg\" {\n  source = \"../../modules/cluster/asg-rolling-deploy\" \n \n  cluster_name  = var.cluster_name\n  ami           = data.aws_ami.ubuntu.id\n  instance_type = \"t2.micro\" \n \n  min_size           = 1\n  max_size           = 1\n  enable_autoscaling = false \n \n  subnet_ids        = data.aws_subnets.default.ids\n} \n \ndata \"aws_vpc\" \"default\" {\n  default = true\n} \n \ndata \"aws_subnets\" \"default\" { \n  filter {\n    name   = \"vpc-id\"\n    values = [data.aws_vpc.default.id] \n  }\n} \n \ndata \"aws_ami\" \"ubuntu\" {\n  most_recent = true\n  owners      = [\"099720109477\"] # Canonical \n \n  filter {\n    name   = \"name\"\n    values = [\"ubuntu/images/hvm-ssd/ubuntu-focal-20.04-amd64-\nserver-*\"] \n  }\n}\nThis bit of code uses the asg-rolling-deploy module to deploy an\nASG of size 1. Try it out by running terraform init and terraform\napply and checking to see that it runs without errors and actually spins up\nan ASG. Now, add in a README.md file with these instructions, and\nsuddenly this tiny little example takes on a whole lot of power. In just\nseveral files and lines of code, you now have the following:\n\nA manual test harness\nYou can use this example code while working on the asg-rolling-\ndeploy module to repeatedly deploy and undeploy it by manually\nrunning terraform apply and terraform destroy to check\nthat it works as you expect.\nAn automated test harness\nAs you will see in Chapter 9, this example code is also how you create\nautomated tests for your modules. I typically recommend that tests go\ninto the test folder.\nExecutable documentation\nIf you commit this example (including README.md) into version\ncontrol, other members of your team can find it, use it to understand\nhow your module works, and take the module for a spin without writing\na line of code. It’s both a way to teach the rest of your team and, if you\nadd automated tests around it, a way to ensure that your teaching\nmaterials always work as expected.\nEvery Terraform module you have in the modules folder should have a\ncorresponding example in the examples folder. And every example in the\nexamples folder should have a corresponding test in the test folder. In fact,\nyou’ll most likely have multiple examples (and therefore multiple tests) for\neach module, with each example showing different configurations and\npermutations of how that module can be used. For example, you might want\nto add other examples for the asg-rolling-deploy module that show\nhow to use it with auto scaling policies, how to hook up load balancers to it,\nhow to set custom tags, and so on.\nPutting this all together, the folder structure for a typical modules repo will\nlook something like this:\nmodules \n └ examples \n └\n\n└ alb \n   └ asg-rolling-deploy \n     └ one-instance \n     └ auto-scaling \n     └ with-load-balancer \n     └ custom-tags \n   └ hello-world-app \n   └ mysql \n └ modules \n   └ alb \n   └ asg-rolling-deploy \n   └ hello-world-app \n   └ mysql \n └ test \n   └ alb \n   └ asg-rolling-deploy \n   └ hello-world-app \n   └ mysql\nAs an exercise for the reader, I leave it up to you to add lots of examples for\nthe alb, asg-rolling-deploy, mysql, and hello-world-app\nmodules.\nA great practice to follow when developing a new module is to write the\nexample code first, before you write even a line of module code. If you\nbegin with the implementation, it’s too easy to become lost in the\nimplementation details, and by the time you resurface and make it back to\nthe API, you end up with a module that is unintuitive and difficult to use.\nOn the other hand, if you begin with the example code, you’re free to think\nthrough the ideal user experience and come up with a clean API for your\nmodule and then work backward to the implementation. Because the\nexample code is the primary way of testing modules anyway, this is a form\nof Test-Driven Development (TDD); I’ll dive more into this topic in\nChapter 9, which is entirely dedicated to testing.\nIn this section, I’ll focus on creating self-validating modules: that is,\nmodules that can check their own behavior to prevent certain types of bugs.\nTerraform has two ways of doing this built in:\nValidations' metadata={'original_pages_range': '443-446', 'source': '123_Testable_Modules', 'file_path': '/mnt/c/Users/Carlos/Documents/Github/jupiter-iaa-azure/Scripts/RAG/../../data/optimized_chunks/Libro-TF/123_Testable_Modules.pdf', 'num_pages': 4}", "page_content='Preconditions and postconditions\nValidations\nAs of Terraform 0.13, you can add validation blocks to any input variable to\nperform checks that go beyond basic type constraints. For example, you can\nadd a validation block to the instance_type variable to ensure not\nonly that the value the user passes in is a string (which is enforced by the\ntype constraint) but that the string has one of two allowed values from the\nAWS Free Tier:\nvariable \"instance_type\" {\n  description = \"The type of EC2 Instances to run (e.g. \nt2.micro)\"\n  type        = string \n \n  validation {\n    condition     = contains([\"t2.micro\", \"t3.micro\"], \nvar.instance_type)\n    error_message = \"Only free tier is allowed: t2.micro | \nt3.micro.\" \n  }\n}\nThe way a validation block works is that the condition parameter\nshould evaluate to true if the value is valid and false otherwise. The\nerror_message parameter allows you to specify the message to show\nthe user if they pass in an invalid value. For example, here’s what happens\nif you try to set instance_type to m4.large, which is not in the AWS\nFree Tier:\n$ terraform apply -var instance_type=\"m4.large\" \n│ Error: Invalid value for variable \n│ \n│   on main.tf line 17: \n│    1: variable \"instance_type\" { \n│     ├──────────────── \n│     │ var.instance_type is \"m4.large\" \n│ \n│ Only free tier is allowed: t2.micro | t3.micro. \n│' metadata={'original_pages_range': '447', 'source': '124_Validations', 'file_path': '/mnt/c/Users/Carlos/Documents/Github/jupiter-iaa-azure/Scripts/RAG/../../data/optimized_chunks/Libro-TF/124_Validations.pdf', 'num_pages': 1}", "page_content='│ \n│ This was checked by the validation rule at main.tf:21,3-13.\nYou can have multiple validation blocks in each variable to check\nmultiple conditions:\nvariable \"min_size\" {\n  description = \"The minimum number of EC2 Instances in the ASG\"\n  type        = number \n \n  validation {\n    condition     = var.min_size > 0\n    error_message = \"ASGs can't be empty or we'll have an \noutage!\" \n  } \n \n  validation {\n    condition     = var.min_size <= 10\n    error_message = \"ASGs must have 10 or fewer instances to keep \ncosts down.\" \n  }\n}\nNote that validation blocks have a major limitation: the condition\nin a validation block can only reference the surrounding input variable.\nIf you try to reference any other input variables, local variables, resources,\nor data sources, you will get an error. So while validation blocks are\nuseful for basic input sanitization, they can’t be used for anything more\ncomplicated: for example, you can’t use them to do checks across multiple\nvariables (such as “exactly one of these two input variables must be set”) or\nany kind of dynamic checks (such as checking that the AMI the user\nrequested uses the x86_64 architecture). To do these sorts of more dynamic\nchecks, you’ll need to use precondition and postcondition\nblocks, as described next.\nPreconditions and postconditions\nAs of Terraform 1.2, you can add precondition and postcondition\nblocks to resources, data sources, and output variables to perform more\ndynamic checks. The precondition blocks are for catching errors\n\nbefore you run apply. For example, you could use a precondition\nblock to do a more robust check that the instance_type the user passes\nin is in the AWS Free Tier. In the previous section, you did this check using\na validation block and a hardcoded list of instance types, but these\nsorts of lists quickly go out of date. You can instead use the\ninstance_type_data data source to always get up-to-date information\nfrom AWS:\ndata \"aws_ec2_instance_type\" \"instance\" {\n  instance_type = var.instance_type\n}\nAnd then you can add a precondition block to the\naws_launch_configuration resource to check that this instance\ntype is eligible for the AWS Free Tier:\nresource \"aws_launch_configuration\" \"example\" {\n  image_id        = var.ami\n  instance_type   = var.instance_type\n  security_groups = [aws_security_group.instance.id]\n  user_data       = var.user_data \n \n  # Required when using a launch configuration with an auto \nscaling group. \n  lifecycle {\n    create_before_destroy = true \n    precondition {\n      condition     = \ndata.aws_ec2_instance_type.instance.free_tier_eligible\n      error_message = \"${var.instance_type} is not part of the \nAWS Free Tier!\" \n    } \n  }\n}\nJust like validation blocks, precondition blocks (and\npostcondition blocks, as you’ll see shortly) include a condition\nthat must evaluate to true or false and an error_message to show\nthe user if the condition evaluates to false. If you now try to run\n\napply with an instance type not in the AWS Free Tier, you’ll see your\nerror message:\n$ terraform apply -var instance_type=\"m4.large\" \n│ Error: Resource precondition failed \n│ \n│   on main.tf line 25, in resource \"aws_launch_configuration\" \n\"example\": \n│   18:    condition = \ndata.aws_ec2_instance_type.instance.free_tier_eligible \n│     ├──────────────── \n│     │ data.aws_ec2_instance_type.instance.free_tier_eligible is \nfalse \n│ \n│ m4.large is not part of the AWS Free Tier!\nThe postcondition blocks are for catching errors after you run apply.\nFor example, you can add a postcondition block to the\naws_autoscaling_group resource to check that the ASG was\ndeployed across more than one Availability Zone (AZ), thereby ensuring\nyou can tolerate the failure of at least one AZ:\nresource \"aws_autoscaling_group\" \"example\" {\n  name                 = var.cluster_name\n  launch_configuration = aws_launch_configuration.example.name\n  vpc_zone_identifier  = var.subnet_ids \n \n  lifecycle { \n    postcondition {\n      condition     = length(self.availability_zones) > 1\n      error_message = \"You must use more than one AZ for high \navailability!\" \n    } \n  } \n \n  # (...)\n}\nNote the use of the self keyword in the condition parameter. Self\nexpressions use the following syntax:\nself.<ATTRIBUTE>' metadata={'original_pages_range': '448-450', 'source': '125_Preconditions_and_postconditions', 'file_path': '/mnt/c/Users/Carlos/Documents/Github/jupiter-iaa-azure/Scripts/RAG/../../data/optimized_chunks/Libro-TF/125_Preconditions_and_postconditions.pdf', 'num_pages': 3}", "page_content='You can use this special syntax solely in postcondition,\nconnection, and provisioner blocks (you’ll see examples of the\nlatter two later in this chapter) to refer to an output ATTRIBUTE of the\nsurrounding resource. If you tried to use the standard\naws_autoscaling_group.example.<ATTRIBUTE> syntax, you’d\nget a circular dependency error, as resources can’t have references to\nthemselves, so the self expression is a workaround added specifically for\nthis sort of use case.\nIf you run apply on this module, Terraform will deploy the module, but\nafter, if it turns out that the subnets the user passed in via the subnet_ids\ninput variable were all in the same AZ, the postcondition block will\nshow an error. This way, you’ll always be warned if your ASG isn’t\nconfigured for high availability.\nWhen to use validations, preconditions, and postconditions\nAs you can see, validation, precondition, and postcondition\nblocks are all similar, so when should you use each one?\nUse validation blocks for basic input sanitization\nUse validation blocks in all of your production-grade modules to\nprevent users from passing invalid variables into your modules. The\ngoal is to catch basic input errors before any changes have been\ndeployed. Although precondition blocks are more powerful, you\nshould still use validation blocks for checking variables whenever\npossible, as validation blocks are defined with the variables they\nvalidate, which leads to a more readable and maintainable API.\nUse precondition blocks for checking basic assumptions\nUse precondition blocks in all of your production-grade modules\nto check assumptions that must be true before any changes have been\ndeployed. This includes any checks on variables you can’t do with\nvalidation blocks (such as checks that reference multiple variables\nor data sources) as well as checks on resources and data sources. The' metadata={'original_pages_range': '451', 'source': '126_When_to_use_validations,_preconditions,_and_postconditions', 'file_path': '/mnt/c/Users/Carlos/Documents/Github/jupiter-iaa-azure/Scripts/RAG/../../data/optimized_chunks/Libro-TF/126_When_to_use_validations,_preconditions,_and_postconditions.pdf', 'num_pages': 1}", "page_content='goal is to catch as many errors as early as you can, before those errors\ncan do any damage.\nUse postcondition blocks for enforcing basic guarantees\nUse postcondition blocks in all of your production-grade modules\nto check guarantees about how your module behaves after changes have\nbeen deployed. The goal is to give users of your module confidence that\nyour module will either do what it says when they run apply or exit\nwith an error. It also gives maintainers of that module a clearer signal of\nwhat behaviors you want this module to enforce, so those aren’t\naccidentally lost during a refactor.\nUse automated testing tools for enforcing more advanced assumptions and\nguarantees\nvalidation, precondition, and postcondition blocks are\nall useful tools, but they can only do basic checks. This is because you\ncan only use data sources, resources, and language constructs built into\nTerraform to do these checks, and those are often not enough for more\nadvanced behavior. For example, if you built a module to deploy a web\nservice, you might want to add a check after deployment that the web\nservice is able to respond to HTTP requests. You could try to do this in\na postcondition block by making HTTP requests to the service\nusing Terraform’s http provider, but most deployments happen\nasynchronously, so you may need to retry the HTTP request multiple\ntimes, and there is no retry mechanism built into that provider.\nMoreover, if you deployed an internal web service, it might not be\naccessible over the public internet, so you’d need to connect to some\ninternal network or VPN first, which is also tricky to do in pure\nTerraform code. Therefore, to do more robust checks, you’ll want to use\nautomated testing tools such as OPA and Terratest, both of which you’ll\nsee in Chapter 9.\nVersioned Modules\n\nThere are two types of versioning you’ll want to think through with\nmodules:\nVersioning of the module’s dependencies\nVersioning of the module itself\nLet’s start with versioning of the module’s dependencies. Your Terraform\ncode has three types of dependencies:\nTerraform core\nThe version of the terraform binary you depend on\nProviders\nThe version of each provider your code depends on, such as the aws\nprovider\nModules\nThe version of each module you depend on that are pulled in via\nmodule blocks\nAs a general rule, you’ll want to practice versioning pinning with all of\nyour dependencies. That means that you pin each of these three types of\ndependencies to a specific, fixed, known version. Deployments should be\npredictable and repeatable: if the code didn’t change, then running apply\nshould always produce the same result, whether you run it today or three\nmonths from now or three years from now. To make that happen, you need\nto avoid pulling in new versions of dependencies accidentally. Instead,\nversion upgrades should always be an explicit, deliberate action that is\nvisible in the code you check into version control.\nLet’s go through how to do version pinning for the three types of Terraform\ndependencies.\nTo pin the version of the first type of dependency, your Terraform core\nversion, you can use the required_version argument in your code. At\n\na bare minimum, you should require a specific major version of Terraform:\nterraform {\n  # Require any 1.x version of Terraform\n  required_version = \">= 1.0.0, < 2.0.0\"\n}\nThis is critical, because each major release of Terraform is backward\nincompatible: e.g., the upgrade from 1.0.0 to 2.0.0 will likely include\nbreaking changes, so you don’t want to do it by accident. The preceding\ncode will allow you to use only 1.x.x versions of Terraform with that\nmodule, so 1.0.0 and 1.2.3 will work, but if you try to use, perhaps\naccidentally, 0.14.3 or 2.0.0, and run terraform apply, you\nimmediately get an error:\n$ terraform apply \n \nError: Unsupported Terraform Core version \n \nThis configuration does not support Terraform version 0.14.3. To \nproceed, \neither choose another supported Terraform version or update the \nroot module's \nversion constraint. Version constraints are normally set for good \nreason, so \nupdating the constraint may lead to other errors or unexpected \nbehavior.\nFor production-grade code, you may want to pin not only the major version\nbut the minor and patch version too:\nterraform {\n  # Require Terraform at exactly version 1.2.3\n  required_version = \"1.2.3\"\n}\nIn the past, before the Terraform 1.0.0 release, this was absolutely required,\nas every release of Terraform potentially included backward-incompatible\nchanges, including to the state file format: e.g., a state file written by\nTerraform version 0.12.1 could not be read by Terraform version 0.12.0.\n\nFortunately, after the 1.0.0 release, this is no longer the case: as per the\nofficially published Terraform v1.0 Compatibility Promises, upgrades\nbetween v1.x releases should require no changes to your code or\nworkflows.\nThat said, you might still not want to upgrade to a new version of Terraform\naccidentally. New versions introduce new features, and if some of your\ncomputers (developer workstations and CI servers) start using those\nfeatures but others are still on the old versions, you’ll run into issues.\nMoreover, new versions of Terraform may have bugs, and you’ll want to\ntest that out in pre-production environments before trying it in production.\nTherefore, while pinning the major version is the bare minimum, I also\nrecommend pinning the minor and patch version and applying Terraform\nupgrades intentionally, carefully, and consistently throughout each\nenvironment.\nNote that, occasionally, you may have to use different versions of Terraform\nwithin a single codebase. For example, perhaps you are testing out\nTerraform 1.2.3 in the stage environment, while the prod environment is\nstill on Terraform 1.0.0. Having to deal with multiple Terraform versions,\nwhether on your own computer or on your CI servers, can be tricky.\nFortunately, the open source tool tfenv, the Terraform version manager,\nmakes this much easier.\nAt its most basic level, you can use tfenv to install and switch between\nmultiple versions of Terraform. For example, you can use the tfenv\ninstall command to install a specific version of Terraform:\n$ tfenv install 1.2.3 \nInstalling Terraform v1.2.3 \nDownloading release tarball from \nhttps://releases.hashicorp.com/terraform/1.2.3/terraform_1.2.3_da\nrwin_amd64.zip \nArchive:  tfenv_download.ZUS3Qn/terraform_1.2.3_darwin_amd64.zip \n  inflating: \n/opt/homebrew/Cellar/tfenv/2.2.2/versions/1.2.3/terraform \nInstallation of terraform v1.2.3 successful.\n\nTFENV ON APPLE SILICON (M1, M2)\nAs of June 2022, tfenv did not install the proper version of Terraform on Apple\nSilicon, such as Macs running M1 or M2 processors (see this open issue for details).\nThe workaround is to set the TFENV_ARCH environment variable to arm64:\n$ export TFENV_ARCH=arm64 $ tfenv install 1.2.3\nYou can list the versions you have installed using the list command:\n$ tfenv list \n  1.2.3 \n  1.1.4 \n  1.1.0 \n* 1.0.0 (set by /opt/homebrew/Cellar/tfenv/2.2.2/version)\nAnd you can select the version of Terraform to use from that list using the\nuse command:\n$ tfenv use 1.2.3 \nSwitching default version to v1.2.3 \nSwitching completed\nThese commands are all handy for working with multiple versions of\nTerraform, but the real power of tfenv is its support for .terraform-\nversion files. tfenv will automatically look for a .terraform-version file in\nthe current folder, as well as all the parent folders, all the way up to the\nproject root—that is, the version control root (e.g., the folder with a .git\nfolder in it)—and if it finds that file, any terraform command you run\nwill automatically use the version defined in that file.\nFor example, if you wanted to try out Terraform 1.2.3 in the stage\nenvironment, while sticking with Terraform 1.0.0 in the prod environment,\nyou could use the following folder structure:\n\nlive \n └ stage \n   └ vpc \n   └ mysql \n   └ frontend-app \n   └ .terraform-version \n └ prod \n   └ vpc \n   └ mysql \n   └ frontend-app \n   └ .terraform-version\nInside of live/stage/.terraform-version, you would have the following:\n1.2.3\nAnd inside of live/prod/.terraform-version, you would have the following:\n1.0.0\nNow, any terraform command you run in stage or any subfolder will\nautomatically use Terraform 1.2.3. You can check this by running the\nterraform version command:\n$ cd stage/vpc \n$ terraform version \nTerraform v1.2.3\nAnd similarly, any terraform command you run in prod will\nautomatically use Terraform 1.0.0:\n$ cd prod/vpc \n$ terraform version \nTerraform v1.0.0\nThis works automatically on any developer workstation and in your CI\nserver so long as everyone has tfenv installed. If you’re a Terragrunt user,\ntgswitch offers similar functionality to automatically pick the Terragrunt\nversion based on a .terragrunt-version file.\n\nLet’s now turn our attention to the second type of dependency in your\nTerraform code: providers. As you saw in Chapter 7, to pin provider\nversions, you can use the required_providers block :\nterraform {\n  required_version = \">= 1.0.0, < 2.0.0\" \n \n  required_providers {\n    aws = {\n      source  = \"hashicorp/aws\"\n      version = \"~> 4.0\" \n    } \n  }\n}\nThis code pins the AWS Provider code to any 4.x version (the ~> 4.0\nsyntax is equivalent to >= 4.0, < 5.0). Again, the bare minimum is to\npin to a specific major version number to avoid accidentally pulling in\nbackward-incompatible changes. With Terraform 0.14.0 and above, you\ndon’t need to pin minor or patch versions for providers, as this happens\nautomatically due to the lock file. The first time you run terraform\ninit, Terraform creates a .terraform.lock.hcl file, which records the\nfollowing information:\nThe exact version of each provider you used\nIf you check the .terraform.lock.hcl file into version control (which you\nshould!), then in the future, if you run terraform init again, on\nthis computer or any other, Terraform will download the exact same\nversion of each provider. That’s why you don’t need to pin the minor\nand patch version number in the required_providers block, as\nthat’s the default behavior anyway. If you want to explicitly upgrade a\nprovider version, you can update the version constraint in the\nrequired_providers block and run terraform init -\nupgrade. Terraform will download new providers that match your\nversion constraints and update the .terraform.lock.hcl file; you should\nreview those updates and commit them to version control.\n\nThe checksums for each provider\nTerraform records the checksum of each provider it downloads, and on\nsubsequent runs of terraform init, it will show an error if the\nchecksum changed. This is a security measure to ensure someone can’t\nswap out provider code with malicious code in the future. If the\nprovider is cryptographically signed (most official HashiCorp providers\nare), Terraform will also validate the signature as an additional check\nthat the code can be trusted.\nLOCK FILES WITH MULTIPLE OPERATINGSYSTEMS\nBy default, Terraform only records checksums for the platform you ran init on: for\nexample, if you ran init on Linux, then Terraform will only record the checksums for\nLinux provider binaries in .terraform.lock.hcl. If you check that file in and, later on, you\nrun init on that code on a Mac, you’ll get an error, as the Mac checksums won’t be in\nthe .terraform.lock.hcl file. If your team works across multiple operating systems, you’ll\nneed to run the terraform providers lock command to record the checksums\nfor every platform you use:\nterraform providers lock \\   -platform=windows_amd64 \\ # 64-bit Windows   -platform=darwin_amd64 \\  # 64-bit macOS   -platform=darwin_arm64 \\  # 64-bit macOS (ARM)   -platform=linux_amd64     # 64-bit Linux\nFinally, let’s now look at the third type of dependencies: modules. As\ndiscussed in “Module Versioning”, I strongly recommend pinning module\nversions by using source URLs (rather than local file paths) with the ref\nparameter set to a Git tag:\n  source = \"git@github.com:foo/modules.git//services/hello-world-\napp?ref=v0.0.5\"\nIf you use these sorts of URLs, Terraform will always download the exact\nsame code for the module every time you run terraform init.\n\nNow that you’ve seen how to version your code’s dependencies, let’s talk\nabout how to version the code itself. As you saw in “Module Versioning”,\nyou can version your code by using Git tags with semantic versioning:\n$ git tag -a \"v0.0.5\" -m \"Create new hello-world-app module\" \n$ git push --follow-tags\nFor example, to deploy version v0.0.5 of your hello-world-app\nmodule in the staging environment, put the following code into\nlive/stage/services/hello-world-app/main.tf:\nprovider \"aws\" {\n  region = \"us-east-2\"\n} \n \nmodule \"hello_world_app\" {\n  # TODO: replace this with your own module URL and version!!\n  source = \"git@github.com:foo/modules.git//services/hello-world-\napp?ref=v0.0.5\" \n \n  server_text            = \"New server text\"\n  environment            = \"stage\"\n  db_remote_state_bucket = \"(YOUR_BUCKET_NAME)\"\n  db_remote_state_key    = \"stage/data-\nstores/mysql/terraform.tfstate\" \n \n  instance_type      = \"t2.micro\"\n  min_size           = 2\n  max_size           = 2\n  enable_autoscaling = false\n  ami                = data.aws_ami.ubuntu.id\n} \n \ndata \"aws_ami\" \"ubuntu\" {\n  most_recent = true\n  owners      = [\"099720109477\"] # Canonical \n \n  filter {\n    name   = \"name\"\n    values = [\"ubuntu/images/hvm-ssd/ubuntu-focal-20.04-amd64-\nserver-*\"] \n  }\n}\n\nNext, pass through the ALB DNS name as an output in\nlive/stage/services/hello-world-app/outputs.tf:\noutput \"alb_dns_name\" {\n  value       = module.hello_world_app.alb_dns_name\n  description = \"The domain name of the load balancer\"\n}\nNow you can deploy your versioned module by running terraform\ninit and terraform apply:\n$ terraform apply \n \n(...) \n \nApply complete! Resources: 13 added, 0 changed, 0 destroyed. \n \nOutputs: \n \nalb_dns_name = \"hello-world-stage-477699288.us-east-\n2.elb.amazonaws.com\"\nIf that works well, you can then deploy the exact same version—and\ntherefore the exact same code—to other environments, including\nproduction. If you ever encounter an issue, versioning also gives you the\noption to roll back by deploying an older version.\nAnother option for releasing modules is to publish them in the Terraform\nRegistry. The Public Terraform Registry includes hundreds of reusable,\ncommunity-maintained, open source modules for AWS, Google Cloud,\nAzure, and many other providers. There are a few requirements to publish a\nmodule to the Public Terraform Registry:\nThe module must live in a public GitHub repo.\nThe repo must be named terraform-<PROVIDER>-<NAME>,\nwhere PROVIDER is the provider the module is targeting (e.g., aws)\nand NAME is the name of the module (e.g., rds).\n6\n\nThe module must follow a specific file structure, including defining\nTerraform code in the root of the repo, providing a README.md, and\nusing the convention of main.tf, variables.tf, and outputs.tf as\nfilenames.\nThe repo must use Git tags with semantic versioning (x.y.z) for\nreleases.\nIf your module meets those requirements, you can share it with the world\nby logging in to the Terraform Registry with your GitHub account and\nusing the web UI to publish the module. Once your modules are in the\nRegistry, your team can use a web UI to discover modules and learn how to\nuse them.\nTerraform even supports a special syntax for consuming modules from the\nTerraform Registry. Instead of long Git URLs with hard-to-spot ref\nparameters, you can use a special shorter registry URL in the source\nargument and specify the version via a separate version argument using\nthe following syntax:\nmodule \"<NAME>\" {\n  source  = \"<OWNER>/<REPO>/<PROVIDER>\"\n  version = \"<VERSION>\" \n \n  # (...)\n}\nwhere NAME is the identifier to use for the module in your Terraform code,\nOWNER is the owner of the GitHub repo (e.g., in\ngithub.com/foo/bar, the owner is foo), REPO is the name of the\nGitHub repo (e.g., in github.com/foo/bar, the repo is bar),\nPROVIDER is the provider you’re targeting (e.g., aws), and VERSION is\nthe version of the module to use. Here’s an example of how to use an open\nsource RDS module from the Terraform Registry:\nmodule \"rds\" {\n  source  = \"terraform-aws-modules/rds/aws\"\n  version = \"4.4.0\"' metadata={'original_pages_range': '452-462', 'source': '127_Versioned_Modules', 'file_path': '/mnt/c/Users/Carlos/Documents/Github/jupiter-iaa-azure/Scripts/RAG/../../data/optimized_chunks/Libro-TF/127_Versioned_Modules.pdf', 'num_pages': 11}", "page_content='# (...)\n}\nIf you are a customer of HashiCorp’s Terraform Cloud or Terraform\nEnterprise, you can have this same experience with a Private Terraform\nRegistry—that is, a registry that lives in your private Git repos and is only\naccessible to your team. This can be a great way to share modules within\nyour company.\nBeyond Terraform Modules\nAlthough this book is all about Terraform, to build out your entire\nproduction-grade infrastructure, you’ll need to use other tools, too, such as\nDocker, Packer, Chef, Puppet, and, of course, the duct tape, glue, and work\nhorse of the DevOps world, the trusty Bash script.\nMost of this code can reside in the modules folder directly alongside your\nTerraform code: e.g., you might have a modules/packer folder that contains\na Packer template and some Bash scripts you use to configure an AMI right\nnext to the modules/asg-rolling-deploy Terraform module you use to deploy\nthat AMI.\nHowever, occasionally, you need to go further and run some non-Terraform\ncode (e.g., a script) directly from a Terraform module. Sometimes, this is to\nintegrate Terraform with another system (e.g., you’ve already used\nTerraform to configure User Data scripts for execution on EC2 Instances);\nother times, it’s to work around a limitation of Terraform, such as a missing\nprovider API, or the inability to implement complicated logic due to\nTerraform’s declarative nature. If you search around, you can find a few\n“escape hatches” within Terraform that make this possible:\nProvisioners\nProvisioners with null_resource\nExternal data source' metadata={'original_pages_range': '463', 'source': '128_Beyond_Terraform_Modules', 'file_path': '/mnt/c/Users/Carlos/Documents/Github/jupiter-iaa-azure/Scripts/RAG/../../data/optimized_chunks/Libro-TF/128_Beyond_Terraform_Modules.pdf', 'num_pages': 1}", "page_content='Let’s go through these one a time.\nProvisioners\nTerraform provisioners are used to execute scripts either on the local\nmachine or a remote machine when you run Terraform, typically to do the\nwork of bootstrapping, configuration management, or cleanup. There are\nseveral different kinds of provisioners, including local-exec (execute a\nscript on the local machine), remote-exec (execute a script on a remote\nresource), and file (copy files to a remote resource).\nYou can add provisioners to a resource by using a provisioner block.\nFor example, here is how you can use the local-exec provisioner to\nexecute a script on your local machine:\nresource \"aws_instance\" \"example\" {\n  ami           = data.aws_ami.ubuntu.id\n  instance_type = \"t2.micro\" \n \n  provisioner \"local-exec\" {\n    command = \"echo \\\"Hello, World from $(uname -smp)\\\"\" \n  }\n}\nWhen you run terraform apply on this code, it prints “Hello, World\nfrom” and then the local operating system details using the uname\ncommand:\n$ terraform apply \n \n(...) \n \naws_instance.example (local-exec): Hello, World from Darwin \nx86_64 i386 \n \n(...) \n \nApply complete! Resources: 1 added, 0 changed, 0 destroyed.\n7\n\nTrying out a remote-exec provisioner is a little more complicated. To\nexecute code on a remote resource, such as an EC2 Instance, your\nTerraform client must be able to do the following:\nCommunicate with the EC2 Instance over the network\nYou already know how to allow this with a security group.\nAuthenticate to the EC2 Instance\nThe remote-exec provisioner supports SSH and WinRM\nconnections.\nSince the examples in this book have you launch Linux (Ubuntu) EC2\nInstances, you’ll want to use SSH authentication. And that means you’ll\nneed to configure SSH keys. Let’s begin by creating a security group that\nallows inbound connections to port 22, the default port for SSH:\nresource \"aws_security_group\" \"instance\" { \n  ingress {\n    from_port = 22\n    to_port   = 22\n    protocol  = \"tcp\" \n \n    # To make this example easy to try out, we allow all SSH \nconnections.\n    # In real world usage, you should lock this down to solely \ntrusted IPs.\n    cidr_blocks = [\"0.0.0.0/0\"] \n  }\n}\nWith SSH keys, the normal process would be for you to generate an SSH\nkey pair on your computer, upload the public key to AWS, and store the\nprivate key somewhere secure where your Terraform code can access it.\nHowever, to make it easier for you to try out this code, you can use a\nresource called tls_private_key to automatically generate a private\nkey:\n\n# To make this example easy to try out, we generate a private key \nin Terraform.\n# In real-world usage, you should manage SSH keys outside of \nTerraform.\nresource \"tls_private_key\" \"example\" {\n  algorithm = \"RSA\"\n  rsa_bits  = 4096\n}\nThis private key is stored in Terraform state, which is not great for\nproduction use cases but is fine for this learning exercise. Next, upload the\npublic key to AWS using the aws_key_pair resource:\nresource \"aws_key_pair\" \"generated_key\" {\n  public_key = tls_private_key.example.public_key_openssh\n}\nFinally, let’s begin writing the code for the EC2 Instance:\ndata \"aws_ami\" \"ubuntu\" {\n  most_recent = true\n  owners      = [\"099720109477\"] # Canonical \n \n  filter {\n    name   = \"name\"\n    values = [\"ubuntu/images/hvm-ssd/ubuntu-focal-20.04-amd64-\nserver-*\"] \n  }\n} \n \nresource \"aws_instance\" \"example\" {\n  ami                    = data.aws_ami.ubuntu.id\n  instance_type          = \"t2.micro\"\n  vpc_security_group_ids = [aws_security_group.instance.id]\n  key_name               = aws_key_pair.generated_key.key_name\n}\nJust about all of this code should be familiar to you: it’s using the\naws_ami data source to find Ubuntu AMI and using the aws_instance\nresource to deploy that AMI on a t2.micro instance, associating that\ninstance with the security group you created earlier. The only new item is\nthe use of the key_name attribute in the aws_instance resource to\n\ninstruct AWS to associate your public key with this EC2 Instance. AWS\nwill add that public key to the server’s authorized_keys file, which will\nallow you to SSH to that server with the corresponding private key.\nNext, add the remote-exec provisioner to the aws_instance\nresource:\nresource \"aws_instance\" \"example\" {\n  ami                    = data.aws_ami.ubuntu.id\n  instance_type          = \"t2.micro\"\n  vpc_security_group_ids = [aws_security_group.instance.id]\n  key_name               = aws_key_pair.generated_key.key_name \n \n  provisioner \"remote-exec\" {\n    inline = [\"echo \\\"Hello, World from $(uname -smp)\\\"\"] \n  }\n}\nThis looks nearly identical to the local-exec provisioner, except you\nuse an inline argument to pass a list of commands to execute, instead of\na single command argument. Finally, you need to configure Terraform to\nuse SSH to connect to this EC2 Instance when running the remote-exec\nprovisioner. You do this by using a connection block:\nresource \"aws_instance\" \"example\" {\n  ami                    = data.aws_ami.ubuntu.id\n  instance_type          = \"t2.micro\"\n  vpc_security_group_ids = [aws_security_group.instance.id]\n  key_name               = aws_key_pair.generated_key.key_name \n \n  provisioner \"remote-exec\" {\n    inline = [\"echo \\\"Hello, World from $(uname -smp)\\\"\"] \n  } \n \n  connection {\n    type        = \"ssh\"\n    host        = self.public_ip\n    user        = \"ubuntu\"\n    private_key = tls_private_key.example.private_key_pem \n  }\n}\n\nThis connection block tells Terraform to connect to the EC2 Instance’s\npublic IP address using SSH with \"ubuntu\" as the username (this is the\ndefault username for the root user on Ubuntu AMIs) and the autogenerated\nprivate key. If you run terraform apply on this code, you’ll see the\nfollowing:\n$ terraform apply \n \n(...) \n \naws_instance.example: Creating... \naws_instance.example: Still creating... [10s elapsed] \naws_instance.example: Still creating... [20s elapsed] \naws_instance.example: Provisioning with 'remote-exec'... \naws_instance.example (remote-exec): Connecting to remote host via \nSSH... \naws_instance.example (remote-exec): Connecting to remote host via \nSSH... \naws_instance.example (remote-exec): Connecting to remote host via \nSSH... \n \n(... repeats a few more times ...) \n \naws_instance.example (remote-exec): Connecting to remote host via \nSSH... \naws_instance.example (remote-exec): Connected! \naws_instance.example (remote-exec): Hello, World from Linux \nx86_64 x86_64 \n \nApply complete! Resources: 4 added, 0 changed, 0 destroyed.\nThe remote-exec provisioner doesn’t know exactly when the EC2\nInstance will be booted and ready to accept connections, so it will retry the\nSSH connection multiple times until it succeeds or hits a timeout (the\ndefault timeout is five minutes, but you can configure it). Eventually, the\nconnection succeeds, and you get a “Hello, World” from the server.\nNote that, by default, when you specify a provisioner, it is a creation-time\nprovisioner, which means that it runs (a) during terraform apply, and\n(b) only during the initial creation of a resource. The provisioner will not\nrun on any subsequent calls to terraform apply, so creation-time\n\nprovisioners are mainly useful for running initial bootstrap code. If you set\nthe when = destroy argument on a provisioner, it will be a destroy-\ntime provisioner, which will run after you run terraform destroy,\njust before the resource is deleted.\nYou can specify multiple provisioners on the same resource and Terraform\nwill run them one at a time, in order, from top to bottom. You can use the\non_failure argument to instruct Terraform how to handle errors from\nthe provisioner: if set to \"continue\", Terraform will ignore the error and\ncontinue with resource creation or destruction; if set to \"abort\",\nTerraform will abort the creation or destruction.' metadata={'original_pages_range': '464-469', 'source': '129_Provisioners', 'file_path': '/mnt/c/Users/Carlos/Documents/Github/jupiter-iaa-azure/Scripts/RAG/../../data/optimized_chunks/Libro-TF/129_Provisioners.pdf', 'num_pages': 6}", "page_content='PROVISIONERS VERSUS USER DATA\nYou’ve now seen two different ways to execute scripts on a server using\nTerraform: one is to use a remote-exec provisioner, and the other is\nto use a User Data script. I’ve generally found User Data to be the more\nuseful tool for the following reasons:\nA remote-exec provisioner requires that you open up SSH or\nWinRM access to your servers, which is more complicated to\nmanage (as you saw earlier with all the security group and SSH\nkey work) and less secure than User Data, which solely requires\nAWS API access (which you must have anyway when using\nTerraform to deploy to AWS).\nYou can use User Data scripts with ASGs, ensuring that all servers\nin that ASG execute the script during boot, including servers\nlaunched due to an auto scaling or auto recovery event.\nProvisioners take effect only while Terraform is running and don’t\nwork with ASGs at all.\nThe User Data script can be seen in the EC2 console (select an\nInstance, click Actions → Instance Settings → View/Change User\nData), and you can find its execution log on the EC2 Instance itself\n(typically in /var/log/cloud-init*.log), both of which are useful for\ndebugging and neither of which is available with provisioners.\nThe only real advantage to using a provisioner to execute code on an\nEC2 Instance is that User Data scripts are limited to a length of 16 KB,\nwhereas provisioner scripts can be arbitrarily long.\nProvisioners with null_resource\nProvisioners can be defined only within a resource, but sometimes, you\nwant to execute a provisioner without tying it to a specific resource. You\ncan do this using something called the null_resource, which acts just\nlike a normal Terraform resource, except that it doesn’t create anything. By\n\ndefining provisioners on the null_resource, you can run your scripts\nas part of the Terraform lifecycle but without being attached to any “real”\nresource:\nresource \"null_resource\" \"example\" { \n  provisioner \"local-exec\" {\n    command = \"echo \\\"Hello, World from $(uname -smp)\\\"\" \n  }\n}\nThe null_resource even has a handy argument called triggers,\nwhich takes in a map of keys and values. Whenever the values change, the\nnull_resource will be re-created, therefore forcing any provisioners\nwithin it to be reexecuted. For example, if you want to execute a\nprovisioner within a null_resource every single time you run\nterraform apply, you could use the uuid() built-in function, which\nreturns a new, randomly generated UUID each time it’s called, within the\ntriggers argument:\nresource \"null_resource\" \"example\" {\n  # Use UUID to force this null_resource to be recreated on every\n  # call to 'terraform apply'\n  triggers = {\n    uuid = uuid() \n  } \n \n  provisioner \"local-exec\" {\n    command = \"echo \\\"Hello, World from $(uname -smp)\\\"\" \n  }\n}\nNow, every time you call terraform apply, the local-exec\nprovisioner will execute:\n$ terraform apply \n \n(...) \n \nnull_resource.example (local-exec): Hello, World from Darwin \nx86_64 i386' metadata={'original_pages_range': '470-471', 'source': '130_Provisioners_with_null_resource', 'file_path': '/mnt/c/Users/Carlos/Documents/Github/jupiter-iaa-azure/Scripts/RAG/../../data/optimized_chunks/Libro-TF/130_Provisioners_with_null_resource.pdf', 'num_pages': 2}", "page_content='$ terraform apply \n \nnull_resource.example (local-exec): Hello, World from Darwin \nx86_64 i386\nExternal data source\nProvisioners will typically be your go-to for executing scripts from\nTerraform, but they aren’t always the correct fit. Sometimes, what you’re\nreally looking to do is execute a script to fetch some data and make that\ndata available within the Terraform code itself. To do this, you can use the\nexternal data source, which allows an external command that\nimplements a specific protocol to act as a data source.\nThe protocol is as follows:\nYou can pass data from Terraform to the external program using the\nquery argument of the external data source. The external program\ncan read in these arguments as JSON from stdin.\nThe external program can pass data back to Terraform by writing\nJSON to stdout. The rest of your Terraform code can then pull data out\nof this JSON by using the result output attribute of the external data\nsource.\nHere’s an example:\ndata \"external\" \"echo\" {\n  program = [\"bash\", \"-c\", \"cat /dev/stdin\"] \n \n  query = {\n    foo = \"bar\" \n  }\n} \n \noutput \"echo\" {\n  value = data.external.echo.result\n} \n \noutput \"echo_foo\" {' metadata={'original_pages_range': '472', 'source': '131_External_data_source', 'file_path': '/mnt/c/Users/Carlos/Documents/Github/jupiter-iaa-azure/Scripts/RAG/../../data/optimized_chunks/Libro-TF/131_External_data_source.pdf', 'num_pages': 1}", "page_content='value = data.external.echo.result.foo\n}\nThis example uses the external data source to execute a Bash script that\nechoes back to stdout any data it receives on stdin. Therefore, any data you\npass in via the query argument should come back as is via the result\noutput attribute. Here’s what happens when you run terraform apply\non this code:\n$ terraform apply \n \n(...) \n \nApply complete! Resources: 0 added, 0 changed, 0 destroyed. \n \nOutputs: \n \necho = { \n  \"foo\" = \"bar\" \n} \necho_foo = \"bar\"\nYou can see that data.external.<NAME>.result contains the\nJSON returned by the external program and that you can navigate within\nthat JSON using the syntax data.external.<NAME>.result.\n<PATH> (e.g., data.external.echo.result.foo).\nThe external data source is a lovely escape hatch if you need to access\ndata in your Terraform code and there’s no existing data source that knows\nhow to retrieve that data. However, be conservative with your use of\nexternal data sources and all of the other Terraform “escape hatches,”\nsince they make your code less portable and more brittle. For example, the\nexternal data source code you just saw relies on Bash, which means you\nwon’t be able to deploy that Terraform module from Windows.\nConclusion\n\nNow that you’ve seen all of the ingredients of creating production-grade\nTerraform code, it’s time to put them together. The next time you begin to\nwork on a new module, use the following process:\n1. Go through the production-grade infrastructure checklist in Table 8-2,\nand explicitly identify the items you’ll be implementing and the items\nyou’ll be skipping. Use the results of this checklist, plus Table 8-1, to\ncome up with a time estimate for your boss.\n2. Create an examples folder, and write the example code first, using it to\ndefine the best user experience and cleanest API you can think of for\nyour modules. Create an example for each important permutation of\nyour module, and include enough documentation and reasonable\ndefaults to make the example as easy to deploy as possible.\n3. Create a modules folder, and implement the API you came up with as a\ncollection of small, reusable, composable modules. Use a combination\nof Terraform and other tools like Docker, Packer, and Bash to\nimplement these modules. Make sure to pin the versions for all your\ndependencies, including Terraform core, your Terraform providers, and\nTerraform modules you depend on.\n4. Create a test folder, and write automated tests for each example.\nThat last bullet point—writing automated tests for your infrastructure code\n—is what we’ll focus on next, as we move on to Chapter 9.\n1 Douglas R. Hofstadter, Gödel, Escher, Bach: An Eternal Golden Braid, 20th anniversary ed.\n(New York: Basic Books, 1999).\n2 Seth Godin, “Don’t Shave That Yak!” Seth’s Blog, March 5, 2005, https://bit.ly/2OK45uL.\n3 Frederick P. Brooks Jr., The Mythical Man-Month: Essays on Software Engineering,\nanniversary ed. (Reading, MA: Addison-Wesley Professional, 1995).\n4 Robert C. Martin, Clean Code: A Handbook of Agile Software Craftsmanship, 1st ed. (Upper\nSaddle River, NJ: Prentice Hall, 2008).\n5 Peter H. Salus, A Quarter-Century of Unix (New York: Addison-Wesley Professional, 1994).\n6 You can find the full details on publishing modules on the Terraform website.\n\n7 You can find the full list of provisioners on the Terraform website.' metadata={'original_pages_range': '473-475', 'source': '132_Conclusion', 'file_path': '/mnt/c/Users/Carlos/Documents/Github/jupiter-iaa-azure/Scripts/RAG/../../data/optimized_chunks/Libro-TF/132_Conclusion.pdf', 'num_pages': 3}", "page_content='Chapter 9. How to Test\nTerraform Code\nThe DevOps world is full of fear: fear of downtime, fear of data loss, fear of\nsecurity breaches. Every time you go to make a change, you’re always\nwondering, what will this affect? Will it work the same way in every\nenvironment? Will this cause another outage? And if there is an outage,\nhow late into the night will you need to stay up to fix it this time? As\ncompanies grow, there is more and more at stake, which makes the\ndeployment process even scarier, and even more error prone. Many\ncompanies try to mitigate this risk by doing deployments less frequently,\nbut the result is that each deployment is larger and actually more prone to\nbreakage.\nIf you manage your infrastructure as code, you have a better way to\nmitigate risk: tests. The goal of testing is to give you the confidence to\nmake changes. The key word here is confidence: no form of testing can\nguarantee that your code is free of bugs, so it’s more of a game of\nprobability. If you can capture all of your infrastructure and deployment\nprocesses as code, you can test that code in a pre-production environment,\nand if it works there, there’s a high probability that when you use the exact\nsame code in production, it will work there, too. And in a world of fear and\nuncertainty, high probability and high confidence go a long way.\nIn this chapter, I’ll go over the process of testing infrastructure code,\nincluding both manual testing and automated testing, with the bulk of the\nchapter spent on the latter:\nManual tests\nManual testing basics\nCleaning up after tests' metadata={'original_pages_range': '476', 'source': '133_9._How_to_Test_Terraform_Code', 'file_path': '/mnt/c/Users/Carlos/Documents/Github/jupiter-iaa-azure/Scripts/RAG/../../data/optimized_chunks/Libro-TF/133_9._How_to_Test_Terraform_Code.pdf', 'num_pages': 1}", "page_content='Automated tests\nUnit tests\nIntegration tests\nEnd-to-end tests\nOther testing approaches\nEXAMPLE CODE\nAs a reminder, you can find all of the code examples in the book on GitHub.\nManual Tests\nWhen thinking about how to test Terraform code, it can be helpful to draw\nsome parallels with how you would test code written in a general-purpose\nprogramming language such as Ruby. Let’s say you were writing a simple\nweb server in Ruby in web-server.rb:\nclass WebServer < WEBrick::HTTPServlet::AbstractServlet \n  def do_GET(request, response) \n    case request.path \n    when \"/\" \n      response.status = 200 \n      response['Content-Type'] = 'text/plain' \n      response.body = 'Hello, World' \n    when \"/api\" \n      response.status = 201 \n      response['Content-Type'] = 'application/json' \n      response.body = '{\"foo\":\"bar\"}' \n    else \n      response.status = 404 \n      response['Content-Type'] = 'text/plain' \n      response.body = 'Not Found' \n    end \n  end\nend' metadata={'original_pages_range': '477', 'source': '134_Manual_Tests', 'file_path': '/mnt/c/Users/Carlos/Documents/Github/jupiter-iaa-azure/Scripts/RAG/../../data/optimized_chunks/Libro-TF/134_Manual_Tests.pdf', 'num_pages': 1}", "page_content='This code will send a 200 response with the body “Hello, World” for the /\nURL, a 201 response with a JSON body for the /api URL, and a 404 for\nall other URLs. How would you manually test this code? The typical\nanswer is to add a bit of code to run the web server on localhost:\n# This will only run if this script was called directly from the \nCLI, but\n# not if it was required from another file\nif __FILE__ == $0 \n  # Run the server on localhost at port 8000 \n  server = WEBrick::HTTPServer.new :Port => 8000 \n  server.mount '/', WebServer \n \n  # Shut down the server on CTRL+C \n  trap 'INT' do server.shutdown end \n \n  # Start the server \n  server.start\nend\nWhen you run this file from the CLI, it will start the web server on port\n8000:\n$ ruby web-server.rb \n[2019-05-25 14:11:52] INFO  WEBrick 1.3.1 \n[2019-05-25 14:11:52] INFO  ruby 2.3.7 (2018-03-28) \n[universal.x86_64-darwin17] \n[2019-05-25 14:11:52] INFO  WEBrick::HTTPServer#start: pid=19767 \nport=8000\nYou can test this server using a web browser or curl:\n$ curl localhost:8000/ \nHello, World\nManual Testing Basics\nWhat is the equivalent of this sort of manual testing with Terraform code?\nFor example, from the previous chapters, you already have Terraform code\n\nfor deploying an ALB. Here’s a snippet from\nmodules/networking/alb/main.tf:\nresource \"aws_lb\" \"example\" {\n  name               = var.alb_name\n  load_balancer_type = \"application\"\n  subnets            = var.subnet_ids\n  security_groups    = [aws_security_group.alb.id]\n} \n \nresource \"aws_lb_listener\" \"http\" {\n  load_balancer_arn = aws_lb.example.arn\n  port              = local.http_port\n  protocol          = \"HTTP\" \n \n  # By default, return a simple 404 page \n  default_action {\n    type = \"fixed-response\" \n \n    fixed_response {\n      content_type = \"text/plain\"\n      message_body = \"404: page not found\"\n      status_code  = 404 \n    } \n  }\n} \n \nresource \"aws_security_group\" \"alb\" {\n  name = var.alb_name\n} \n \n# (...)\nIf you compare this code to the Ruby code, one difference should be fairly\nobvious: you can’t deploy AWS ALBs, target groups, listeners, security\ngroups, and all the other infrastructure on your own computer.\nThis brings us to key testing takeaway #1: when testing Terraform code, you\ncan’t use localhost. This applies to most IaC tools, not just Terraform. The\nonly practical way to do manual testing with Terraform is to deploy to a real\nenvironment (i.e., deploy to AWS). In other words, the way you’ve been\nmanually running terraform apply and terraform destroy\nthroughout the book is how you do manual testing with Terraform.\n\nThis is one of the reasons why it’s essential to have easy-to-deploy\nexamples in the examples folder for each module, as described in Chapter 8.\nThe easiest way to manually test the alb module is to use the example\ncode you created for it in examples/alb:\nprovider \"aws\" {\n  region = \"us-east-2\"\n} \n \nmodule \"alb\" {\n  source = \"../../modules/networking/alb\" \n \n  alb_name   = \"terraform-up-and-running\"\n  subnet_ids = data.aws_subnets.default.ids\n}\nAs you’ve done many times throughout the book, you deploy this example\ncode using terraform apply:\n$ terraform apply \n \n(...) \n \nApply complete! Resources: 5 added, 0 changed, 0 destroyed. \n \nOutputs: \n \nalb_dns_name = \"hello-world-stage-477699288.us-east-\n2.elb.amazonaws.com\"\nWhen the deployment is done, you can use a tool such as curl to test, for\nexample, that the default action of the ALB is to return a 404:\n$ curl \\ \n  -s \\ \n  -o /dev/null \\ \n  -w \"%{http_code}\" \\ \n  hello-world-stage-477699288.us-east-2.elb.amazonaws.com \n \n404' metadata={'original_pages_range': '478-480', 'source': '135_Manual_Testing_Basics', 'file_path': '/mnt/c/Users/Carlos/Documents/Github/jupiter-iaa-azure/Scripts/RAG/../../data/optimized_chunks/Libro-TF/135_Manual_Testing_Basics.pdf', 'num_pages': 3}", "page_content='VALIDATING INFRASTRUCTURE\nThe examples in this chapter use curl and HTTP requests to validate that the\ninfrastructure is working, because the infrastructure you’re testing includes a load\nbalancer that responds to HTTP requests. For other types of infrastructure, you’ll need\nto replace curl and HTTP requests with a different form of validation. For example, if\nyour infrastructure code deploys a MySQL database, you’ll need to use a MySQL client\nto validate it; if your infrastructure code deploys a VPN server, you’ll need to use a\nVPN client to validate it; if your infrastructure code deploys a server that isn’t listening\nfor requests at all, you might need to SSH to the server and execute some commands\nlocally to test it; and so on. So although you can use the same basic test structure\ndescribed in this chapter with any type of infrastructure, the validation steps will change\ndepending on what you’re testing.\nIn short, when working with Terraform, every developer needs good\nexample code to test and a real deployment environment (e.g., an AWS\naccount) to use as an equivalent to localhost for running those tests. In the\nprocess of manual testing, you’re likely to bring up and tear down a lot of\ninfrastructure, and likely make lots of mistakes along the way, so this\nenvironment should be completely isolated from your other, more stable\nenvironments, such as staging, and especially production.\nTherefore, I strongly recommend that every team sets up an isolated\nsandbox environment, in which developers can bring up and tear down any\ninfrastructure they want without worrying about affecting others. In fact, to\nreduce the chances of conflicts between multiple developers (e.g., two\ndevelopers trying to create a load balancer with the same name), the gold\nstandard is that each developer gets their own completely isolated sandbox\nenvironment. For example, if you’re using Terraform with AWS, the gold\nstandard is for each developer to have their own AWS account that they can\nuse to test anything they want.\nCleaning Up After Tests\nHaving many sandbox environments is essential for developer productivity,\nbut if you’re not careful, you can end up with infrastructure running all over\n1' metadata={'original_pages_range': '481', 'source': '136_Cleaning_Up_After_Tests', 'file_path': '/mnt/c/Users/Carlos/Documents/Github/jupiter-iaa-azure/Scripts/RAG/../../data/optimized_chunks/Libro-TF/136_Cleaning_Up_After_Tests.pdf', 'num_pages': 1}", "page_content='the place, cluttering up all of your environments, and costing you a lot of\nmoney.\nTo keep costs from spiraling out of control, key testing takeaway #2 is:\nregularly clean up your sandbox environments.\nAt a minimum, you should create a culture in which developers clean up\nwhatever they deploy when they are done testing by running terraform\ndestroy. Depending on your deployment environment, you might also be\nable to find tools that you can run on a regular schedule (e.g., a cron job) to\nautomatically clean up unused or old resources, such as cloud-nuke and\naws-nuke.\nFor example, a common pattern is to run cloud-nuke as a cron job once\nper day in each sandbox environment to delete all resources that are more\nthan 48 hours old, based on the assumption that any infrastructure a\ndeveloper fired up for manual testing is no longer necessary after a couple\nof days:\n$ cloud-nuke aws --older-than 48h\nWARNING: LOTS OF CODING AHEAD\nWriting automated tests for infrastructure code is not for the faint of heart. This\nautomated testing section is arguably the most complicated part of the book and does\nnot make for light reading. If you’re just skimming, feel free to skip this part. On the\nother hand, if you really want to learn how to test your infrastructure code, roll up your\nsleeves and get ready to write some code! You don’t need to run any of the Ruby code\n(it’s just there to help build up your mental model), but you’ll want to write and run as\nmuch Go code as you can.\nAutomated Tests\nThe idea with automated testing is to write test code that validates that your\nreal code behaves the way it should. As you’ll see in Chapter 10, you can\nset up a CI server to run these tests after every single commit and then\n\nimmediately revert or fix any commits that cause the tests to fail, thereby\nalways keeping your code in a working state.\nBroadly speaking, there are three types of automated tests:\nUnit tests\nUnit tests verify the functionality of a single, small unit of code. The\ndefinition of unit varies, but in a general-purpose programming\nlanguage, it’s typically a single function or class. Usually, any external\ndependencies—for example, databases, web services, even the\nfilesystem—are replaced with test doubles or mocks that allow you to\nfinely control the behavior of those dependencies (e.g., by returning a\nhardcoded response from a database mock) to test that your code\nhandles a variety of scenarios.\nIntegration tests\nIntegration tests verify that multiple units work together correctly. In a\ngeneral-purpose programming language, an integration test consists of\ncode that validates that several functions or classes work together\ncorrectly. Integration tests typically use a mix of real dependencies and\nmocks: for example, if you’re testing the part of your app that\ncommunicates with the database, you might want to test it with a real\ndatabase, but mock out other dependencies, such as the app’s\nauthentication system.\nEnd-to-end tests\nEnd-to-end tests involve running your entire architecture—for example,\nyour apps, your data stores, your load balancers—and validating that\nyour system works as a whole. Usually, these tests are done from the\nend-user’s perspective, such as using Selenium to automate interacting\nwith your product via a web browser. End-to-end tests typically use real\nsystems everywhere, without any mocks, in an architecture that mirrors\nproduction (albeit with fewer/smaller servers to save money).' metadata={'original_pages_range': '482-483', 'source': '137_Automated_Tests', 'file_path': '/mnt/c/Users/Carlos/Documents/Github/jupiter-iaa-azure/Scripts/RAG/../../data/optimized_chunks/Libro-TF/137_Automated_Tests.pdf', 'num_pages': 2}", "page_content='Each type of test serves a different purpose, and can catch different types of\nbugs, so you’ll likely want to use a mix of all three types. The purpose of\nunit tests is to have tests that run quickly so that you can get fast feedback\non your changes and validate a variety of different permutations to build up\nconfidence that the basic building blocks of your code (the individual units)\nwork as expected. But just because individual units work correctly in\nisolation doesn’t mean that they will work correctly when combined, so you\nneed integration tests to ensure the basic building blocks fit together\ncorrectly. And just because different parts of your system work correctly\ndoesn’t mean they will work correctly when deployed in the real world, so\nyou need end-to-end tests to validate that your code behaves as expected in\nconditions similar to production.\nLet’s now go through how to write each type of test for Terraform code.\nUnit Tests\nTo understand how to write unit tests for Terraform code, it’s helpful to first\nlook at what it takes to write unit tests for a general-purpose programming\nlanguage such as Ruby. Take a look again at the Ruby web server code:\nclass WebServer < WEBrick::HTTPServlet::AbstractServlet \n  def do_GET(request, response) \n    case request.path \n    when \"/\" \n      response.status = 200 \n      response['Content-Type'] = 'text/plain' \n      response.body = 'Hello, World' \n    when \"/api\" \n      response.status = 201 \n      response['Content-Type'] = 'application/json' \n      response.body = '{\"foo\":\"bar\"}' \n    else \n      response.status = 404 \n      response['Content-Type'] = 'text/plain' \n      response.body = 'Not Found' \n    end \n  end\nend\n\nWriting a unit test that calls the do_GET method directly is tricky, as you’d\nhave to either instantiate real WebServer, request, and response\nobjects, or create test doubles of them, both of which require a fair bit of\nwork. When you find it difficult to write unit tests, that’s often a code smell\nand indicates that the code needs to be refactored. One way to refactor this\nRuby code to make unit testing easier is to extract the “handlers”—that is,\nthe code that handles the /, /api, and not found paths—into its own\nHandlers class:\nclass Handlers \n  def handle(path) \n    case path \n    when \"/\" \n      [200, 'text/plain', 'Hello, World'] \n    when \"/api\" \n      [201, 'application/json', '{\"foo\":\"bar\"}'] \n    else \n      [404, 'text/plain', 'Not Found'] \n    end \n  end\nend\nThere are two key properties to notice about this new Handlers class:\nSimple values as inputs\nThe Handlers class does not depend on HTTPServer,\nHTTPRequest, or HTTPResponse. Instead, all of its inputs are\nsimple values, such as the path of the URL, which is a string.\nSimple values as outputs\nInstead of setting values on a mutable HTTPResponse object (a side\neffect), the methods in the Handlers class return the HTTP response\nas a simple value (an array that contains the HTTP status code, content\ntype, and body).\nCode that takes in simple values as inputs and returns simple values as\noutputs is typically easier to understand, update, and test. Let’s first update\n\nthe WebServer class to use the new Handlers class to respond to\nrequests:\nclass WebServer < WEBrick::HTTPServlet::AbstractServlet \n  def do_GET(request, response) \n    handlers = Handlers.new \n    status_code, content_type, body = \nhandlers.handle(request.path) \n \n    response.status = status_code \n    response['Content-Type'] = content_type \n    response.body = body \n  end\nend\nThis code calls the handle method of the Handlers class and sends\nback the status code, content type, and body returned by that method as an\nHTTP response. As you can see, using the Handlers class is clean and\nsimple. This same property will make testing easy, too. Here are three unit\ntests that check each endpoint in the Handlers class:\nclass TestWebServer < Test::Unit::TestCase \n  def initialize(test_method_name) \n    super(test_method_name) \n    @handlers = Handlers.new \n  end \n \n  def test_unit_hello \n    status_code, content_type, body = @handlers.handle(\"/\") \n    assert_equal(200, status_code) \n    assert_equal('text/plain', content_type) \n    assert_equal('Hello, World', body) \n  end \n \n  def test_unit_api \n    status_code, content_type, body = @handlers.handle(\"/api\") \n    assert_equal(201, status_code) \n    assert_equal('application/json', content_type) \n    assert_equal('{\"foo\":\"bar\"}', body) \n  end \n \n  def test_unit_404 \n    status_code, content_type, body = @handlers.handle(\"/invalid-' metadata={'original_pages_range': '484-486', 'source': '138_Unit_Tests', 'file_path': '/mnt/c/Users/Carlos/Documents/Github/jupiter-iaa-azure/Scripts/RAG/../../data/optimized_chunks/Libro-TF/138_Unit_Tests.pdf', 'num_pages': 3}", "page_content='path\") \n    assert_equal(404, status_code) \n    assert_equal('text/plain', content_type) \n    assert_equal('Not Found', body) \n  end\nend\nAnd here’s how you run the tests:\n$ ruby web-server-test.rb \nLoaded suite web-server-test \nFinished in 0.000572 seconds. \n------------------------------------------- \n3 tests, 9 assertions, 0 failures, 0 errors \n100% passed \n-------------------------------------------\nIn 0.0005272 seconds, you can now find out whether your web server code\nworks as expected. That’s the power of unit testing: a fast feedback loop\nthat helps you build confidence in your code.\nUnit testing Terraform code\nWhat is the equivalent of this sort of unit testing with Terraform code? The\nfirst step is to identify what a “unit” is in the Terraform world. The closest\nequivalent to a single function or class in Terraform is a single reusable\nmodule such as the alb module you created in Chapter 8. How would you\ntest this module?\nWith Ruby, to write unit tests, you needed to refactor the code so you could\nrun it without complicated dependencies such as HTTPServer,\nHTTPRequest, or HTTPResponse. If you think about what your\nTerraform code is doing—making API calls to AWS to create the load\nbalancer, listeners, target groups, and so on—you’ll realize that 99% of\nwhat this code is doing is communicating with complicated dependencies!\nThere’s no practical way to reduce the number of external dependencies to\nzero, and even if you could, you’d effectively be left with no code to test.\nThat brings us to key testing takeaway #3: you cannot do pure unit testing\nfor Terraform code.\n2\n\nBut don’t despair. You can still build confidence that your Terraform code\nbehaves as expected by writing automated tests that use your code to deploy\nreal infrastructure into a real environment (e.g., into a real AWS account).\nIn other words, unit tests for Terraform are really integration tests.\nHowever, I prefer to still call them unit tests to emphasize that the goal is to\ntest a single unit (i.e., a single reusable module) to get feedback as quickly\nas possible.\nThis means that the basic strategy for writing unit tests for Terraform is as\nfollows:\n1. Create a small, standalone module.\n2. Create an easy-to-deploy example for that module.\n3. Run terraform apply to deploy the example into a real\nenvironment.\n4. Validate that what you just deployed works as expected. This step is\nspecific to the type of infrastructure you’re testing: for example, for an\nALB, you’d validate it by sending an HTTP request and checking that\nyou receive back the expected response.\n5. Run terraform destroy at the end of the test to clean up.\nIn other words, you do exactly the same steps as you would when doing\nmanual testing, but you capture those steps as code. In fact, that’s a good\nmental model for creating automated tests for your Terraform code: ask\nyourself, “How would I have tested this manually to be confident it works?”\nand then implement that test in code.\nYou can use any programming language you want to write the test code. In\nthis book, all of the tests are written in the Go programming language to\ntake advantage of an open source Go library called Terratest, which\nsupports testing a wide variety of infrastructure-as-code tools (e.g.,\nTerraform, Packer, Docker, Helm) across a wide variety of environments\n(e.g., AWS, Google Cloud, Kubernetes). Terratest is a bit like a Swiss Army\nknife, with hundreds of tools built in that make it significantly easier to test\n\ninfrastructure code, including first-class support for the test strategy just\ndescribed, where you terraform apply some code, validate that it\nworks, and then run terraform destroy at the end to clean up.\nTo use Terratest, you need to do the following:\n1. Install Go (minimum version 1.13).\n2. Create a folder for your test code: e.g., a folder named test.\n3. Run go mod init <NAME> in the folder you just created, where\nNAME is the name to use for this test suite, typically in the format\ngithub.com/<ORG_NAME>/<PROJECT_NAME> (e.g., go mod\ninit github.com/brikis98/terraform-up-and-\nrunning). This should create a go.mod file, which is used to track\nthe dependencies of your Go code.\nAs a quick sanity check that your environment is set up correctly, create\ngo_sanity_test.go in your new folder with the following contents:\npackage test \n \nimport (\n \"fmt\"\n \"testing\"\n) \n \nfunc TestGoIsWorking(t *testing.T) {\n fmt.Println()\n fmt.Println(\"If you see this text, it's working!\")\n fmt.Println()\n}\nRun this test using the go test command:\ngo test -v\nThe -v flag means verbose, which ensures that the test always shows all\nlog output. You should see output that looks something like this:\n\n=== RUN   TestGoIsWorking \n \nIf you see this text, it's working! \n \n--- PASS: TestGoIsWorking (0.00s) \nPASS \nok   github.com/brikis98/terraform-up-and-running-code \n0.192s\nIf that’s working, feel free to delete go_sanity_test.go, and move on to\nwriting a unit test for the alb module. Create alb_example_test.go in your\ntest folder with the following skeleton of a unit test:\npackage test \n \nimport (\n \"testing\"\n) \n \nfunc TestAlbExample(t *testing.T) {\n}\nThe first step is to direct Terratest to where your Terraform code resides by\nusing the terraform.Options type:\npackage test \n \nimport (\n \"github.com/gruntwork-io/terratest/modules/terraform\"\n \"testing\"\n) \n \nfunc TestAlbExample(t *testing.T) {\n opts := &terraform.Options{\n  // You should update this relative path to point \nat your alb\n  // example directory!\n  TerraformDir: \"../examples/alb\",\n }\n}\nNote that to test the alb module, you actually test the example code in\nyour examples folder (you should update the relative path in\n\nTerraformDir to point to the folder where you created that example).\nThe next step in the automated test is to run terraform init and\nterraform apply to deploy the code. Terratest has handy helpers for\ndoing that:\nfunc TestAlbExample(t *testing.T) {\n opts := &terraform.Options{\n  // You should update this relative path to point \nat your alb\n  // example directory!\n  TerraformDir: \"../examples/alb\",\n } \n \n terraform.Init(t, opts)\n terraform.Apply(t, opts)\n}\nIn fact, running init and apply is such a common operation with\nTerratest that there is a convenient InitAndApply helper method that\ndoes both in one command:\nfunc TestAlbExample(t *testing.T) {\n opts := &terraform.Options{\n  // You should update this relative path to point \nat your alb\n  // example directory!\n  TerraformDir: \"../examples/alb\",\n } \n \n // Deploy the example\n terraform.InitAndApply(t, opts)\n}\nThe preceding code is already a fairly useful unit test, since it will run\nterraform init and terraform apply and fail the test if those\ncommands don’t complete successfully (e.g., due to a problem with your\nTerraform code). However, you can go even further by making HTTP\nrequests to the deployed load balancer and checking that it returns the data\nyou expect. To do that, you need a way to get the domain name of the\n\ndeployed load balancer. Fortunately, that’s available as an output variable in\nthe alb example:\noutput \"alb_dns_name\" {\n  value       = module.alb.alb_dns_name\n  description = \"The domain name of the load balancer\"\n}\nTerratest has helpers built in to read outputs from your Terraform code:\nfunc TestAlbExample(t *testing.T) {\n opts := &terraform.Options{\n  // You should update this relative path to point \nat your alb\n  // example directory!\n  TerraformDir: \"../examples/alb\",\n } \n \n // Deploy the example\n terraform.InitAndApply(t, opts) \n \n // Get the URL of the ALB\n albDnsName := terraform.OutputRequired(t, opts, \n\"alb_dns_name\")\n url := fmt.Sprintf(\"http://%s\", albDnsName)\n}\nThe OutputRequired function returns the output of the given name, or\nit fails the test if that output doesn’t exist or is empty. The preceding code\nbuilds a URL from this output using the fmt.Sprintf function that’s\nbuilt into Go (don’t forget to import the fmt package). The next step is to\nmake some HTTP requests to this URL using the http_helper package\n(make sure to add github.com/gruntwork-\nio/terratest/modules/http-helper as an import):\nfunc TestAlbExample(t *testing.T) {\n opts := &terraform.Options{\n  // You should update this relative path to point \nat your alb\n  // example directory!\n  TerraformDir: \"../examples/alb\",\n\n} \n \n // Deploy the example\n terraform.InitAndApply(t, opts) \n \n // Get the URL of the ALB\n albDnsName := terraform.OutputRequired(t, opts, \n\"alb_dns_name\")\n url := fmt.Sprintf(\"http://%s\", albDnsName) \n \n // Test that the ALB's default action is working and \nreturns a 404\n expectedStatus := 404\n expectedBody := \"404: page not found\"\n maxRetries := 10\n timeBetweenRetries := 10 * time.Second \n \n http_helper.HttpGetWithRetry(\n  t,\n  url,\n  nil,\n  expectedStatus,\n  expectedBody,\n  maxRetries,\n  timeBetweenRetries,\n )\n}\nThe http_helper.HttpGetWithRetry method will make an HTTP\nGET request to the URL you pass in and check that the response has the\nexpected status code and body. If it doesn’t, the method will retry up to the\nspecified maximum number of retries, with the specified amount of time\nbetween retries. If it eventually achieves the expected response, the test will\npass; if the maximum number of retries is reached without the expected\nresponse, the test will fail. This sort of retry logic is very common in\ninfrastructure testing, as there is usually a period of time between when\nterraform apply finishes and when the deployed infrastructure is\ncompletely ready (i.e., it takes time for health checks to pass, DNS updates\nto propagate, and so on), and as you don’t know exactly how long that’ll\ntake, the best option is to retry until it works or you hit a timeout.\n\nThe last thing you need to do is to run terraform destroy at the end\nof the test to clean up. As you can guess, there is a Terratest helper for this:\nterraform.Destroy. However, if you call terraform.Destroy at\nthe very end of the test, if any of the code before that causes a test failure\n(e.g., HttpGetWithRetry fails because the ALB is misconfigured), the\ntest code will exit before getting to terraform.Destroy, and the\ninfrastructure deployed for the test will never be cleaned up.\nTherefore, you want to ensure that you always run\nterraform.Destroy, even if the test fails. In many programming\nlanguages, this is done with a try / finally or try / ensure construct,\nbut in Go, this is done by using the defer statement, which will guarantee\nthat the code you pass to it will be executed when the surrounding function\nreturns (no matter how that return happens):\nfunc TestAlbExample(t *testing.T) {\n opts := &terraform.Options{\n  // You should update this relative path to point \nat your alb\n  // example directory!\n  TerraformDir: \"../examples/alb\",\n } \n \n // Clean up everything at the end of the test\n defer terraform.Destroy(t, opts) \n \n // Deploy the example\n terraform.InitAndApply(t, opts) \n \n // Get the URL of the ALB\n albDnsName := terraform.OutputRequired(t, opts, \n\"alb_dns_name\")\n url := fmt.Sprintf(\"http://%s\", albDnsName) \n \n // Test that the ALB's default action is working and \nreturns a 404\n expectedStatus := 404\n expectedBody := \"404: page not found\"\n maxRetries := 10\n timeBetweenRetries := 10 * time.Second \n \n http_helper.HttpGetWithRetry(\n\nt,\n  url,\n  nil,\n  expectedStatus,\n  expectedBody,\n  maxRetries,\n  timeBetweenRetries,\n )\n}\nNote that the defer is added early in the code, even before the call to\nterraform .Ini tAndApply, to ensure that nothing can cause the test\nto fail before getting to the defer statement and preventing it from\nqueueing up the call to terraform.Destroy.\nOK, this unit test is finally ready to run!\nTERRATEST VERSION\nThe test code in this book was written with Terratest v0.39.0. Terratest is still a pre-1.0.0\ntool, so newer releases may contain backward-incompatible changes. To ensure the test\nexamples in this book work as written, I recommend installing Terratest specifically at\nversion v0.39.0, and not the latest version. To do that, go into go.mod and add the\nfollowing to the end of the file:\nrequire github.com/gruntwork-io/terratest v0.39.0\nSince this is a brand-new Go project, as a one-time action, you need to tell\nGo to download dependencies (including Terratest). The easiest way to do\nthat at this stage is to run the following:\ngo mod tidy\nThis will download all your dependencies and create a go.sum file to lock\nthe exact versions you used.\nNext, since this test deploys infrastructure to AWS, before running the test,\nyou need to authenticate to your AWS account as usual (see “Other AWS\n\nAuthentication Options”). You saw earlier in this chapter that you should do\nmanual testing in a sandbox account; for automated testing, this is even\nmore important, so I recommend authenticating to a totally separate\naccount. As your automated test suite grows, you might be spinning up\nhundreds or thousands of resources in every test suite, so keeping them\nisolated from everything else is essential.\nI typically recommend that teams have a completely separate environment\n(e.g., completely separate AWS account) just for automated testing—\nseparate even from the sandbox environments you use for manual testing.\nThat way, you can safely delete all resources that are more than a few hours\nold in the testing environment, based on the assumption that no test will run\nthat long.\nAfter you’ve authenticated to an AWS account that you can safely use for\ntesting, you can run the test, as follows:\n$ go test -v -timeout 30m \n \nTestAlbExample 2019-05-26T13:29:32+01:00 command.go:53: \nRunning command terraform with args [init -upgrade=false] \n \n(...) \n \nTestAlbExample 2019-05-26T13:29:33+01:00 command.go:53: \nRunning command terraform with args [apply -input=false -\nlock=false] \n \n(...) \n \nTestAlbExample 2019-05-26T13:32:06+01:00 command.go:121: \nApply complete! Resources: 5 added, 0 changed, 0 destroyed. \n \n(...) \n \nTestAlbExample 2019-05-26T13:32:06+01:00 command.go:53: \nRunning command terraform with args [output -no-color \nalb_dns_name] \n \n(...) \n \nTestAlbExample 2019-05-26T13:38:32+01:00 http_helper.go:27: \nMaking an HTTP GET call to URL\n\nhttp://terraform-up-and-running-1892693519.us-east-\n2.elb.amazonaws.com \n \n(...) \n \nTestAlbExample 2019-05-26T13:38:32+01:00 command.go:53: \nRunning command terraform with args \n[destroy -auto-approve -input=false -lock=false] \n \n(...) \n \nTestAlbExample 2019-05-26T13:39:16+01:00 command.go:121: \nDestroy complete! Resources: 5 destroyed. \n \n(...) \n \nPASS \nok   terraform-up-and-running 229.492s\nNote the use of the -timeout 30m argument with go test. By default,\nGo imposes a time limit of 10 minutes for tests, after which it forcibly kills\nthe test run, not only causing the tests to fail but also preventing the cleanup\ncode (i.e., terraform destroy) from running. This ALB test should\ntake closer to five minutes, but whenever running a Go test that deploys real\ninfrastructure, it’s safer to set an extra-long timeout to avoid the test being\nkilled partway through and leaving all sorts of infrastructure still running.\nThe test will produce a lot of log output, but if you read through it carefully,\nyou should be able to spot all of the key stages of the test:\n1. Running terraform init\n2. Running terraform apply\n3. Reading output variables using terraform output\n4. Repeatedly making HTTP requests to the ALB\n5. Running terraform destroy\nIt’s nowhere near as fast as the Ruby unit tests, but in less than five minutes,\nyou can now automatically find out whether your alb module works as' metadata={'original_pages_range': '487-497', 'source': '139_Unit_testing_Terraform_code', 'file_path': '/mnt/c/Users/Carlos/Documents/Github/jupiter-iaa-azure/Scripts/RAG/../../data/optimized_chunks/Libro-TF/139_Unit_testing_Terraform_code.pdf', 'num_pages': 11}", "page_content='expected. This is about as fast of a feedback loop as you can get with\ninfrastructure in AWS, and it should give you a lot of confidence that your\ncode works as expected.\nDependency injection\nLet’s now see what it would take to add a unit test for some slightly more\ncomplicated code. Going back to the Ruby web server example once more,\nconsider what would happen if you needed to add a new /web-service\nendpoint that made HTTP calls to an external dependency:\nclass Handlers \n  def handle(path) \n    case path \n    when \"/\" \n      [200, 'text/plain', 'Hello, World'] \n    when \"/api\" \n      [201, 'application/json', '{\"foo\":\"bar\"}'] \n    when \"/web-service\" \n      # New endpoint that calls a web service \n      uri = URI(\"http://www.example.org\") \n      response = Net::HTTP.get_response(uri) \n      [response.code.to_i, response['Content-Type'], \nresponse.body] \n    else \n      [404, 'text/plain', 'Not Found'] \n    end \n  end\nend\nThe updated Handlers class now handles the /web-service URL by\nmaking an HTTP GET to example.org and proxying the response.\nWhen you curl this endpoint, you get the following:\n$ curl localhost:8000/web-service \n \n<!doctype html> \n<html> \n<head> \n    <title>Example Domain</title> \n    <-- (...) --> \n</head>\n\n<body> \n<div> \n    <h1>Example Domain</h1> \n    <p> \n      This domain is established to be used for illustrative \n      examples in documents. You may use this domain in \n      examples without prior coordination or asking for \npermission. \n    </p> \n    <!-- (...) --> \n</div> \n</body> \n</html>\nHow would you add a unit test for this new method? If you tried to test the\ncode as is, your unit tests would be subject to the behavior of an external\ndependency (in this case, example.org). This has a number of\ndownsides:\nIf that dependency has an outage, your tests will fail, even though\nthere’s nothing wrong with your code.\nIf that dependency changed its behavior from time to time (e.g.,\nreturned a different response body), your tests would fail from time to\ntime, and you’d need to constantly keep updating the test code, even\nthough there’s nothing wrong with the implementation.\nIf that dependency were slow, your tests would be slow, which negates\none of the main benefits of unit tests, the fast feedback loop.\nIf you wanted to test that your code handles various corner cases based\non how that dependency behaves (e.g., does the code handle\nredirects?), you’d have no way to do it without control of that external\ndependency.\nAlthough working with real dependencies might make sense for integration\nand end-to-end tests, with unit tests, you should try to minimize external\ndependencies as much as possible. The typical strategy for doing this is\ndependency injection, in which you make it possible to pass in (or “inject”)\n\nexternal dependencies from outside your code, rather than hardcoding them\nwithin your code.\nFor example, the Handlers class shouldn’t need to deal with all of the\ndetails of how to call a web service. Instead, you can extract that logic into\na separate WebService class:\nclass WebService \n  def initialize(url) \n    @uri = URI(url) \n  end \n \n  def proxy \n    response = Net::HTTP.get_response(@uri) \n    [response.code.to_i, response['Content-Type'], response.body] \n  end\nend\nThis class takes a URL as an input and exposes a proxy method to proxy\nthe HTTP GET response from that URL. You can then update the\nHandlers class to take a WebService instance as an input and use that\ninstance in the web_service method:\nclass Handlers \n  def initialize(web_service) \n    @web_service = web_service \n  end \n \n  def handle(path) \n    case path \n    when \"/\" \n      [200, 'text/plain', 'Hello, World'] \n    when \"/api\" \n      [201, 'application/json', '{\"foo\":\"bar\"}'] \n    when \"/web-service\" \n      # New endpoint that calls a web service \n      @web_service.proxy \n    else \n      [404, 'text/plain', 'Not Found'] \n    end \n  end\nend\n\nNow, in your implementation code, you can inject a real WebService\ninstance that makes HTTP calls to example.org:\nclass WebServer < WEBrick::HTTPServlet::AbstractServlet \n  def do_GET(request, response) \n    web_service = WebService.new(\"http://www.example.org\") \n    handlers = Handlers.new(web_service) \n \n    status_code, content_type, body = \nhandlers.handle(request.path) \n \n    response.status = status_code \n    response['Content-Type'] = content_type \n    response.body = body \n  end\nend\nIn your test code, you can create a mock version of the WebService class\nthat allows you to specify a mock response to return:\nclass MockWebService \n  def initialize(response) \n    @response = response \n  end \n \n  def proxy \n    @response \n  end\nend\nAnd now you can create an instance of this MockWebService class and\ninject it into the Handlers class in your unit tests:\n  def test_unit_web_service \n    expected_status = 200 \n    expected_content_type = 'text/html' \n    expected_body = 'mock example.org' \n    mock_response = [expected_status, expected_content_type, \nexpected_body] \n \n    mock_web_service = MockWebService.new(mock_response) \n    handlers = Handlers.new(mock_web_service)\n\nstatus_code, content_type, body = handlers.handle(\"/web-\nservice\") \n    assert_equal(expected_status, status_code) \n    assert_equal(expected_content_type, content_type) \n    assert_equal(expected_body, body) \n  end\nRerun the tests to make sure it all still works:\n$ ruby web-server-test.rb \nLoaded suite web-server-test \nStarted \n... \n \nFinished in 0.000645 seconds. \n-------------------------------------------- \n4 tests, 12 assertions, 0 failures, 0 errors \n100% passed \n--------------------------------------------\nFantastic. Using dependency injection to minimize external dependencies\nallows you to write fast, reliable tests and check all the various corner cases.\nAnd since the three test cases you added earlier are still passing, you can be\nconfident that your refactoring hasn’t broken anything.\nLet’s now turn our attention back to Terraform and see what dependency\ninjection looks like with Terraform modules, starting with the hello-\nworld-app module. If you haven’t already, the first step is to create an\neasy-to-deploy example for it in the examples folder:\nprovider \"aws\" {\n  region = \"us-east-2\"\n} \n \nmodule \"hello_world_app\" {\n  source = \"../../../modules/services/hello-world-app\" \n \n  server_text = \"Hello, World\"\n  environment = \"example\" \n \n  db_remote_state_bucket = \"(YOUR_BUCKET_NAME)\"\n  db_remote_state_key    = \"examples/terraform.tfstate\"\n\ninstance_type      = \"t2.micro\"\n  min_size           = 2\n  max_size           = 2\n  enable_autoscaling = false\n  ami                = data.aws_ami.ubuntu.id\n} \n \ndata \"aws_ami\" \"ubuntu\" {\n  most_recent = true\n  owners      = [\"099720109477\"] # Canonical \n \n  filter {\n    name   = \"name\"\n    values = [\"ubuntu/images/hvm-ssd/ubuntu-focal-20.04-amd64-\nserver-*\"] \n  }\n}\nThe dependency problem becomes apparent when you spot the parameters\ndb_remote_state_bucket and db_remote_state_key: the\nhello-world-app module assumes that you’ve already deployed the\nmysql module and requires that you pass in the details of the S3 bucket\nwhere the mysql module is storing state using these two parameters. The\ngoal here is to create a unit test for the hello-world-app module, and\nalthough a pure unit test with 0 external dependencies isn’t possible with\nTerraform, it’s still a good idea to minimize external dependencies\nwhenever possible.\nOne of the first steps with minimizing dependencies is to make it clearer\nwhat dependencies your module has. A file-naming convention you might\nwant to adopt is to move all of the data sources and resources that represent\nexternal dependencies into a separate dependencies.tf file. For example,\nhere’s what modules/services/hello-world-app/dependencies.tf would look\nlike:\ndata \"terraform_remote_state\" \"db\" {\n  backend = \"s3\" \n \n  config = {\n    bucket = var.db_remote_state_bucket\n    key    = var.db_remote_state_key\n\nregion = \"us-east-2\" \n  }\n} \n \ndata \"aws_vpc\" \"default\" {\n  default = true\n} \n \ndata \"aws_subnets\" \"default\" { \n  filter {\n    name   = \"vpc-id\"\n    values = [data.aws_vpc.default.id] \n  }\n}\nThis convention makes it easier for users of your code to know, at a glance,\nwhat this code depends on in the outside world. In the case of the hello-\nworld-app module, you can quickly see that it depends on a database,\nVPC, and subnets. So, how can you inject these dependencies from outside\nthe module so that you can replace them at test time? You already know the\nanswer to this: input variables.\nFor each of these dependencies, you should add a new input variable in\nmodules/services/hello-world-app/variables.tf:\nvariable \"vpc_id\" {\n  description = \"The ID of the VPC to deploy into\"\n  type        = string\n  default     = null\n} \n \nvariable \"subnet_ids\" {\n  description = \"The IDs of the subnets to deploy into\"\n  type        = list(string)\n  default     = null\n} \n \nvariable \"mysql_config\" {\n  description = \"The config for the MySQL DB\"\n  type        = object({\n    address = string\n    port    = number \n  })\n\ndefault     = null\n}\nThere’s now an input variable for the VPC ID, subnet IDs, and MySQL\nconfig. Each variable specifies a default, so they are optional variables\nthat the user can set to something custom or omit to get the default\nvalue. The default for each variable is null.\nNote that the mysql_config variable uses the object type constructor\nto create a nested type with address and port keys. This type is\nintentionally designed to match the output types of the mysql module:\noutput \"address\" {\n  value       = aws_db_instance.example.address\n  description = \"Connect to the database at this endpoint\"\n} \n \noutput \"port\" {\n  value       = aws_db_instance.example.port\n  description = \"The port the database is listening on\"\n}\nOne of the advantages of doing this is that, as soon as the refactor is\ncomplete, one of the ways you’ll be able to use the hello-world-app\nand mysql modules together is as follows:\nmodule \"hello_world_app\" {\n  source = \"../../../modules/services/hello-world-app\" \n \n  server_text            = \"Hello, World\"\n  environment            = \"example\" \n \n  # Pass all the outputs from the mysql module straight through!\n  mysql_config = module.mysql \n \n  instance_type      = \"t2.micro\"\n  min_size           = 2\n  max_size           = 2\n  enable_autoscaling = false\n  ami                = data.aws_ami.ubuntu.id\n}\n\nmodule \"mysql\" {\n  source = \"../../../modules/data-stores/mysql\" \n \n  db_name     = var.db_name\n  db_username = var.db_username\n  db_password = var.db_password\n}\nBecause the type of mysql_config matches the type of the mysql\nmodule outputs, you can pass them all straight through in one line. And if\nthe types are ever changed and no longer match, Terraform will give you an\nerror right away so that you know to update them. This is not only function\ncomposition at work but also type-safe function composition.\nBut before that can work, you’ll need to finish refactoring the code.\nBecause the MySQL configuration can be passed in as an input, this means\nthat the db_remote_state_bucket and db_remote_state_key\nvariables should now be optional, so set their default values to null:\nvariable \"db_remote_state_bucket\" {\n  description = \"The name of the S3 bucket for the DB's Terraform \nstate\"\n  type        = string\n  default     = null\n} \n \nvariable \"db_remote_state_key\" {\n  description = \"The path in the S3 bucket for the DB's Terraform \nstate\"\n  type        = string\n  default     = null\n}\nNext, use the count parameter to optionally create the three data sources\nin modules/services/hello-world-app/dependencies.tf based on whether the\ncorresponding input variable is set to null:\ndata \"terraform_remote_state\" \"db\" {\n  count = var.mysql_config == null ? 1 : 0 \n \n  backend = \"s3\"\n\nconfig = {\n    bucket = var.db_remote_state_bucket\n    key    = var.db_remote_state_key\n    region = \"us-east-2\" \n  }\n} \n \ndata \"aws_vpc\" \"default\" {\n  count   = var.vpc_id == null ? 1 : 0\n  default = true\n} \n \ndata \"aws_subnets\" \"default\" {\n  count = var.subnet_ids == null ? 1 : 0 \n  filter {\n    name   = \"vpc-id\"\n    values = [data.aws_vpc.default.id] \n  }\n}\nNow you need to update any references to these data sources to\nconditionally use either the input variable or the data source. Let’s capture\nthese as local values:\nlocals {\n  mysql_config = (\n    var.mysql_config == null \n      ? data.terraform_remote_state.db[0].outputs \n      : var.mysql_config \n  ) \n \n  vpc_id = (\n    var.vpc_id == null \n      ? data.aws_vpc.default[0].id \n      : var.vpc_id \n  ) \n \n  subnet_ids = (\n    var.subnet_ids == null \n      ? data.aws_subnets.default[0].ids \n      : var.subnet_ids \n  )\n}\n\nNote that because the data sources use the count parameters, they are now\narrays, so any time you reference them, you need to use array lookup syntax\n(i.e., [0]). Go through the code, and anywhere you find a reference to one\nof these data sources, replace it with a reference to one of the local variables\nyou just added instead. Start by updating the aws_subnets data source to\nuse local.vpc_id:\ndata \"aws_subnets\" \"default\" {\n  count = var.subnet_ids == null ? 1 : 0 \n  filter {\n    name   = \"vpc-id\"\n    values = [local.vpc_id] \n  }\n}\nThen, set the subnet_ids parameter of the alb module to\nlocal.subnet_ids:\nmodule \"alb\" {\n  source = \"../../networking/alb\" \n \n  alb_name   = \"hello-world-${var.environment}\"\n  subnet_ids = local.subnet_ids\n}\nIn the asg module, make the following updates: set the subnet_ids\nparameter to local.subnet_ids, and in the user_data variables,\nupdate db_address and db_port to read data from\nlocal.mysql_config.\nmodule \"asg\" {\n  source = \"../../cluster/asg-rolling-deploy\" \n \n  cluster_name  = \"hello-world-${var.environment}\"\n  ami           = var.ami\n  instance_type = var.instance_type \n \n  user_data = templatefile(\"${path.module}/user-data.sh\", {\n    server_port = var.server_port\n    db_address  = local.mysql_config.address\n\ndb_port     = local.mysql_config.port\n    server_text = var.server_text \n  }) \n \n  min_size           = var.min_size\n  max_size           = var.max_size\n  enable_autoscaling = var.enable_autoscaling \n \n  subnet_ids        = local.subnet_ids\n  target_group_arns = [aws_lb_target_group.asg.arn]\n  health_check_type = \"ELB\" \n \n  custom_tags = var.custom_tags\n}\nFinally, update the vpc_id parameter of the aws_lb_target_group\nto use local.vpc_id:\nresource \"aws_lb_target_group\" \"asg\" {\n  name     = \"hello-world-${var.environment}\"\n  port     = var.server_port\n  protocol = \"HTTP\"\n  vpc_id   = local.vpc_id \n \n  health_check {\n    path                = \"/\"\n    protocol            = \"HTTP\"\n    matcher             = \"200\"\n    interval            = 15\n    timeout             = 3\n    healthy_threshold   = 2\n    unhealthy_threshold = 2 \n  }\n}\nWith these updates, you can now choose to inject the VPC ID, subnet IDs,\nand/or MySQL config parameters into the hello-world-app module,\nor omit any of those parameters, and the module will use an appropriate\ndata source to fetch those values by itself. Let’s update the “Hello, World”\napp example to allow the MySQL config to be injected but omit the VPC\nID and subnet ID parameters because using the default VPC is good enough\n\nfor testing. Add a new input variable to examples/hello-world-\napp/variables.tf:\nvariable \"mysql_config\" {\n  description = \"The config for the MySQL DB\" \n \n  type = object({\n    address = string\n    port    = number \n  }) \n \n  default = {\n    address = \"mock-mysql-address\"\n    port    = 12345 \n  }\n}\nPass this variable through to the hello-world-app module in\nexamples/hello-world-app/main.tf:\nmodule \"hello_world_app\" {\n  source = \"../../../modules/services/hello-world-app\" \n \n  server_text = \"Hello, World\"\n  environment = \"example\" \n \n  mysql_config = var.mysql_config \n \n  instance_type      = \"t2.micro\"\n  min_size           = 2\n  max_size           = 2\n  enable_autoscaling = false\n  ami                = data.aws_ami.ubuntu.id\n}\nYou can now set this mysql_config variable in a unit test to any value\nyou want. Create a unit test in test/hello_world_app_example_test.go with\nthe following contents:\nfunc TestHelloWorldAppExample(t *testing.T) {\n opts := &terraform.Options{\n  // You should update this relative path to point \nat your\n\n// hello-world-app example directory!\n  TerraformDir: \"../examples/hello-world-\napp/standalone\",\n } \n \n // Clean up everything at the end of the test\n defer terraform.Destroy(t, opts)\n terraform.InitAndApply(t, opts) \n \n albDnsName := terraform.OutputRequired(t, opts, \n\"alb_dns_name\")\n url := fmt.Sprintf(\"http://%s\", albDnsName) \n \n maxRetries := 10\n timeBetweenRetries := 10 * time.Second \n \n http_helper.HttpGetWithRetryWithCustomValidation(\n  t,\n  url,\n  nil,\n  maxRetries,\n  timeBetweenRetries,\n  func(status int, body string) bool {\n   return status == 200 &&\n    strings.Contains(body, \"Hello, \nWorld\")\n  },\n )\n}\nThis code is nearly identical to the unit test for the alb example, with only\ntwo differences:\nThe TerraformDir setting is pointing to the hello-world-app\nexample instead of the alb example (be sure to update the path as\nnecessary for your filesystem).\nInstead of using http_helper.HttpGetWithRetry to check for\na 404 response, the test is using the\nhttp_helper.HttpGetWithRetryWithCustomValidatio\nn method to check for a 200 response and a body that contains the text\n“Hello, World.” That’s because the User Data script of the hello-\n\nworld-app module returns a 200 OK response that includes not only\nthe server text but also other text, including HTML.\nThere’s just one new thing you’ll need to add to this test—set the\nmysql_config variable:\n opts := &terraform.Options{\n  // You should update this relative path to point \nat your\n  // hello-world-app example directory!\n  TerraformDir: \"../examples/hello-world-\napp/standalone\", \n \n  Vars: map[string]interface{}{\n   \"mysql_config\": map[string]interface{}{\n    \"address\": \"mock-value-for-test\",\n    \"port\":    3306,\n   },\n  },\n }\nThe Vars parameter in terraform.Options allows you to set\nvariables in your Terraform code. This code is passing in some mock data\nfor the mysql_config variable. Alternatively, you could set this value to\nanything you want: for example, you could fire up a small, in-memory\ndatabase at test time and set the address to that database’s IP.\nRun this new test using go test, specifying the -run argument to run\njust this test (otherwise, Go’s default behavior is to run all tests in the\ncurrent folder, including the ALB example test you created earlier):\n$ go test -v -timeout 30m -run TestHelloWorldAppExample \n \n(...) \n \nPASS \nok   terraform-up-and-running 204.113s\nIf all goes well, the test will run terraform apply, make repeated\nHTTP requests to the load balancer, and, as soon as it gets back the' metadata={'original_pages_range': '498-512', 'source': '140_Dependency_injection', 'file_path': '/mnt/c/Users/Carlos/Documents/Github/jupiter-iaa-azure/Scripts/RAG/../../data/optimized_chunks/Libro-TF/140_Dependency_injection.pdf', 'num_pages': 15}", "page_content='expected response, run terraform destroy to clean everything up. All\ntold, it should take only a few minutes, and you now have a reasonable unit\ntest for the “Hello, World” app.\nRunning tests in parallel\nIn the previous section, you ran just a single test using the -run argument\nof the go test command. If you had omitted that argument, Go would’ve\nrun all of your tests—sequentially. Although four to five minutes to run a\nsingle test isn’t too bad for testing infrastructure code, if you have dozens of\ntests, and each one runs sequentially, it could take hours to run your entire\ntest suite. To shorten the feedback loop, you want to run as many tests in\nparallel as you can.\nTo instruct Go to run your tests in parallel, the only change you need to\nmake is to add t.Parallel() to the top of each test. Here it is in\ntest/hello_world_app_example_test.go:\nfunc TestHelloWorldAppExample(t *testing.T) {\n t.Parallel() \n \n opts := &terraform.Options{\n  // You should update this relative path to point \nat your\n  // hello-world-app example directory!\n  TerraformDir: \"../examples/hello-world-\napp/standalone\", \n \n  Vars: map[string]interface{}{\n   \"mysql_config\": map[string]interface{}{\n    \"address\": \"mock-value-for-test\",\n    \"port\":    3306,\n   },\n  },\n } \n \n // (...)\n}\nAnd similarly in test/alb_example_test.go:\n\nfunc TestAlbExample(t *testing.T) {\n t.Parallel() \n \n opts := &terraform.Options{\n  // You should update this relative path to point \nat your alb\n  // example directory!\n  TerraformDir: \"../examples/alb\",\n } \n \n // (...)\n}\nIf you run go test now, both of those tests will execute in parallel.\nHowever, there’s one gotcha: some of the resources created by those tests—\nfor example, the ASG, security group, and ALB—use the same name,\nwhich will cause the tests to fail due to the name clashes. Even if you\nweren’t using t.Parallel() in your tests, if multiple people on your\nteam were running the same tests or if you had tests running in a CI\nenvironment, these sorts of name clashes would be inevitable.\nThis leads to key testing takeaway #4: you must namespace all of your\nresources.\nThat is, design modules and examples so that the name of every resource is\n(optionally) configurable. With the alb example, this means that you need\nto make the name of the ALB configurable. Add a new input variable in\nexamples/alb/variables.tf with a reasonable default:\nvariable \"alb_name\" {\n  description = \"The name of the ALB and all its resources\"\n  type        = string\n  default     = \"terraform-up-and-running\"\n}\nNext, pass this value through to the alb module in examples/alb/main.tf:\nmodule \"alb\" {\n  source = \"../../modules/networking/alb\" \n \n  alb_name   = var.alb_name\n\nsubnet_ids = data.aws_subnets.default.ids\n}\nNow, set this variable to a unique value in test/alb_example_test.go:\npackage test \n \nimport (\n \"fmt\"\n \"github.com/stretchr/testify/require\" \n \n \"github.com/gruntwork-io/terratest/modules/http-helper\"\n \"github.com/gruntwork-io/terratest/modules/random\"\n \"github.com/gruntwork-io/terratest/modules/terraform\"\n \"testing\"\n \"time\"\n) \n \nfunc TestAlbExample(t *testing.T) {\n t.Parallel() \n \n opts := &terraform.Options{\n  // You should update this relative path to point \nat your alb\n  // example directory!\n  TerraformDir: \"../examples/alb\", \n \n  Vars: map[string]interface{}{\n   \"alb_name\": fmt.Sprintf(\"test-%s\", \nrandom.UniqueId()),\n  },\n } \n \n // (...)\n}\nThis code sets the alb_name var to test-<RANDOM_ID>, where\nRANDOM_ID is a random unique ID returned by the\nrandom.UniqueId() helper in Terratest. This helper returns a\nrandomized, six-character base-62 string. The idea is that it’s a short\nidentifier you can add to the names of most resources without hitting\nlength-limit issues but random enough to make conflicts very unlikely (626\n\n= 56+ billion combinations). This ensures that you can run a huge number\nof ALB tests in parallel with no concern of having a name conflict.\nMake a similar change to the “Hello, World” app example, first by adding a\nnew input variable in examples/hello-world-app/variables.tf:\nvariable \"environment\" {\n  description = \"The name of the environment we're deploying to\"\n  type        = string\n  default     = \"example\"\n}\nThen by passing that variable through to the hello-world-app module:\nmodule \"hello_world_app\" {\n  source = \"../../../modules/services/hello-world-app\" \n \n  server_text = \"Hello, World\" \n \n  environment = var.environment \n \n  mysql_config = var.mysql_config \n \n  instance_type      = \"t2.micro\"\n  min_size           = 2\n  max_size           = 2\n  enable_autoscaling = false\n  ami                = data.aws_ami.ubuntu.id\n}\nFinally, setting environment to a value that includes\nrandom.UniqueId() in test/hello_world_app_example_test.go:\nfunc TestHelloWorldAppExample(t *testing.T) {\n t.Parallel() \n \n opts := &terraform.Options{\n  // You should update this relative path to point \nat your\n  // hello-world-app example directory!\n  TerraformDir: \"../examples/hello-world-\napp/standalone\",\n\nVars: map[string]interface{}{\n   \"mysql_config\": map[string]interface{}{\n    \"address\": \"mock-value-for-test\",\n    \"port\":    3306,\n   },\n   \"environment\": fmt.Sprintf(\"test-%s\", \nrandom.UniqueId()),\n  },\n } \n \n // (...)\n}\nWith these changes complete, it should now be safe to run all your tests in\nparallel:\n$ go test -v -timeout 30m \n \nTestAlbExample 2019-05-26T17:57:21+01:00 (...) \nTestHelloWorldAppExample 2019-05-26T17:57:21+01:00 (...) \nTestAlbExample 2019-05-26T17:57:21+01:00 (...) \nTestHelloWorldAppExample 2019-05-26T17:57:21+01:00 (...) \nTestHelloWorldAppExample 2019-05-26T17:57:21+01:00 (...) \n \n(...) \n \nPASS \nok   terraform-up-and-running 216.090s\nYou should see both tests running at the same time so that the entire test\nsuite takes roughly as long as the slowest of the tests, rather than the\ncombined time of all the tests running back to back.\nNote that, by default, the number of tests Go will run in parallel is equal to\nhow many CPUs you have on your computer. So if you only have one CPU,\nthen by default, the tests will still run serially, rather than in parallel. You\ncan override this setting by setting the GOMAXPROCS environment variable\nor by passing the -parallel argument to the go test command. For\nexample, to force Go to run up to two tests in parallel, you would run the\nfollowing:\n$ go test -v -timeout 30m -parallel 2' metadata={'original_pages_range': '513-517', 'source': '141_Running_tests_in_parallel', 'file_path': '/mnt/c/Users/Carlos/Documents/Github/jupiter-iaa-azure/Scripts/RAG/../../data/optimized_chunks/Libro-TF/141_Running_tests_in_parallel.pdf', 'num_pages': 5}", "page_content='RUNNING TESTS IN PARALLEL IN THE SAMEFOLDER\nOne other type of parallelism to take into account is what happens if you try to run\nmultiple automated tests in parallel against the same Terraform folder. For example,\nperhaps you’d want to run several different tests against examples/hello-world-app,\nwhere each test sets different values for the input variables before running terraform\napply. If you try this, you’ll hit a problem: the tests will end up clashing because they\nall try to run terraform init and end up overwriting one another’s .terraform\nfolder and Terraform state files.\nIf you want to run multiple tests against the same folder in parallel, the easiest solution\nis to have each test copy that folder to a unique temporary folder, and run Terraform in\nthe temporary folder to avoid conflicts. Terratest, of course, has a built-in helper to do\nthis for you, and it even does it in a way that ensures that relative file paths within those\nTerraform modules work correctly: check out the\ntest_structure.CopyTerraformFolderToTemp method and its\ndocumentation for details.\nIntegration Tests\nNow that you’ve got some unit tests in place, let’s move on to integration\ntests. Again, it’s helpful to start with the Ruby web server example to build\nup some intuition that you can later apply to the Terraform code. To do an\nintegration test of the Ruby web server code, you need to do the following:\n1. Run the web server on localhost so that it listens on a port.\n2. Send HTTP requests to the web server.\n3. Validate you get back the responses you expect.\nLet’s create a helper method in web-server-test.rb that implements these\nsteps:\n  def do_integration_test(path, check_response) \n    port = 8000 \n    server = WEBrick::HTTPServer.new :Port => port \n    server.mount '/', WebServer \n \n    begin \n      # Start the web server in a separate thread so it\n\n# doesn't block the test \n      thread = Thread.new do \n        server.start \n      end \n \n      # Make an HTTP request to the web server at the \n      # specified path \n      uri = URI(\"http://localhost:#{port}#{path}\") \n      response = Net::HTTP.get_response(uri) \n \n      # Use the specified check_response lambda to validate \n      # the response \n      check_response.call(response) \n    ensure \n      # Shut the server and thread down at the end of the \n      # test \n      server.shutdown \n      thread.join \n    end \n  end\nThe do_integration_test method configures the web server on port\n8000, starts it in a background thread (so the web server doesn’t block the\ntest from running), sends an HTTP GET to the path specified, passes the\nHTTP response to the specified check_response function for\nvalidation, and at the end of the test, shuts down the web server. Here’s how\nyou can use this method to write an integration test for the / endpoint of the\nweb server:\n  def test_integration_hello \n    do_integration_test('/', lambda { |response| \n      assert_equal(200, response.code.to_i) \n      assert_equal('text/plain', response['Content-Type']) \n      assert_equal('Hello, World', response.body) \n    }) \n  end\nThis method calls the do_integration_test method with the / path\nand passes it a lambda (essentially, an inline function) that checks the\nresponse was a 200 OK with the body “Hello, World.” The integration tests\nfor the other endpoints are analogous. Let’s run all of the tests:\n\n$ ruby web-server-test.rb \n \n(...) \n \nFinished in 0.221561 seconds. \n-------------------------------------------- \n8 tests, 24 assertions, 0 failures, 0 errors \n100% passed \n--------------------------------------------\nNote that before, with solely unit tests, the test suite took 0.000572 seconds\nto run, but now, with integration tests, it takes 0.221561 seconds, a\nslowdown of roughly 387 times. Of course, 0.221561 seconds is still\nblazing fast, but that’s only because the Ruby web server code is\nintentionally a minimal example that doesn’t do much. The important thing\nhere is not the absolute numbers but the relative trend: integration tests are\ntypically slower than unit tests. I’ll come back to this point later.\nLet’s now turn our attention to integration tests for Terraform code. If a\n“unit” in Terraform is a single module, an integration test that validates how\nseveral units work together would need to deploy several modules and see\nthat they work correctly. In the previous section, you deployed the “Hello,\nWorld” app example with mock data instead of a real MySQL DB. For an\nintegration test, let’s deploy the MySQL module for real and make sure the\n“Hello, World” app integrates with it correctly. You should already have\njust such code under live/stage/data-stores/mysql and\nlive/stage/services/hello-world-app. That is, you can create an integration\ntest for (parts of) your staging environment.\nOf course, as mentioned earlier in the chapter, all automated tests should\nrun in an isolated AWS account. So while you’re testing the code that is\nmeant for staging, you should authenticate to an isolated testing account\nand run the tests there. If your modules have anything in them hardcoded\nfor the staging environment, this is the time to make those values\nconfigurable so you can inject test-friendly values. In particular, in\nlive/stage/data-stores/mysql/variables.tf, expose the database name via a\nnew db_name input variable:\n\nvariable \"db_name\" {\n  description = \"The name to use for the database\"\n  type        = string\n  default     = \"example_database_stage\"\n}\nPass that value through to the mysql module in live/stage/data-\nstores/mysql/main.tf:\nmodule \"mysql\" {\n  source = \"../../../../modules/data-stores/mysql\" \n \n  db_name     = var.db_name\n  db_username = var.db_username\n  db_password = var.db_password\n}\nLet’s now create the skeleton of the integration test in\ntest/hello_world_integration_test.go and fill in the implementation details\nlater:\n// Replace these with the proper paths to your modules\nconst dbDirStage = \"../live/stage/data-stores/mysql\"\nconst appDirStage = \"../live/stage/services/hello-world-app\" \n \nfunc TestHelloWorldAppStage(t *testing.T) {\n t.Parallel() \n \n // Deploy the MySQL DB\n dbOpts := createDbOpts(t, dbDirStage)\n defer terraform.Destroy(t, dbOpts)\n terraform.InitAndApply(t, dbOpts) \n \n // Deploy the hello-world-app\n helloOpts := createHelloOpts(dbOpts, appDirStage)\n defer terraform.Destroy(t, helloOpts)\n terraform.InitAndApply(t, helloOpts) \n \n // Validate the hello-world-app works\n validateHelloApp(t, helloOpts)\n}\n\nThe test is structured as follows: deploy mysql, deploy the hello-\nworld-app, validate the app, undeploy the hello-world-app (runs at\nthe end due to defer), and, finally, undeploy mysql (runs at the end due\nto defer). The createDbOpts, create HelloOpts, and\nvalidateHelloApp methods don’t exist yet, so let’s implement them\none at a time, starting with the createDbOpts method:\nfunc createDbOpts(t *testing.T, terraformDir string) \n*terraform.Options {\n uniqueId := random.UniqueId() \n \n return &terraform.Options{\n  TerraformDir: terraformDir, \n \n  Vars: map[string]interface{}{\n   \"db_name\":     fmt.Sprintf(\"test%s\", \nuniqueId),\n   \"db_username\": \"admin\",\n   \"db_password\": \"password\",\n  },\n }\n}\nNot much new so far: the code points terraform.Options at the\npassed-in directory and sets the db_name, db_username, and\ndb_password variables.\nThe next step is to deal with where this mysql module will store its state.\nUp to now, the backend configuration has been set to hardcoded values:\n  backend \"s3\" {\n    # Replace this with your bucket name!\n    bucket         = \"terraform-up-and-running-state\"\n    key            = \"stage/data-stores/mysql/terraform.tfstate\"\n    region         = \"us-east-2\" \n \n    # Replace this with your DynamoDB table name!\n    dynamodb_table = \"terraform-up-and-running-locks\"\n    encrypt        = true \n  }\n\nThese hardcoded values are a big problem for testing, because if you don’t\nchange them, you’ll end up overwriting the real state file for staging! One\noption is to use Terraform workspaces (as discussed in “Isolation via\nWorkspaces”), but that would still require access to the S3 bucket in the\nstaging account, whereas you should be running tests in a totally separate\nAWS account. The better option is to use partial configuration, as\nintroduced in “Limitations with Terraform’s Backends”. Move the entire\nbackend configuration into an external file, such as backend.hcl:\nbucket         = \"terraform-up-and-running-state\"\nkey            = \"stage/data-stores/mysql/terraform.tfstate\"\nregion         = \"us-east-2\"\ndynamodb_table = \"terraform-up-and-running-locks\"\nencrypt        = true\nleaving the backend configuration in live/stage/data-stores/mysql/main.tf\nempty:\n  backend \"s3\" { \n  }\nWhen you’re deploying the mysql module to the real staging environment,\nyou tell Terraform to use the backend configuration in backend.hcl via the\n-backend-config argument:\n$ terraform init -backend-config=backend.hcl\nWhen you’re running tests on the mysql module, you can tell Terratest to\npass in test-time-friendly values using the BackendConfig parameter of\nterraform.Options:\nfunc createDbOpts(t *testing.T, terraformDir string) \n*terraform.Options {\n uniqueId := random.UniqueId() \n \n bucketForTesting := \"YOUR_S3_BUCKET_FOR_TESTING\"\n bucketRegionForTesting := \n\"YOUR_S3_BUCKET_REGION_FOR_TESTING\"\n\ndbStateKey := fmt.Sprintf(\"%s/%s/terraform.tfstate\", \nt.Name(), uniqueId) \n \n return &terraform.Options{\n  TerraformDir: terraformDir, \n \n  Vars: map[string]interface{}{\n   \"db_name\":     fmt.Sprintf(\"test%s\", \nuniqueId),\n   \"db_username\": \"admin\",\n   \"db_password\": \"password\",\n  }, \n \n  BackendConfig: map[string]interface{}{\n   \"bucket\":  bucketForTesting,\n   \"region\":  bucketRegionForTesting,\n   \"key\":     dbStateKey,\n   \"encrypt\": true,\n  },\n }\n}\nYou’ll need to update the bucketForTesting and\nbucketRegionForTesting variables with your own values. You can\ncreate a single S3 bucket in your test AWS account to use as a backend,\nas the key configuration (the path within the bucket) includes the\nuniqueId, which should be unique enough to have a different value for\neach test.\nThe next step is to make some updates to the hello-world-app module\nin the staging environment. Open live/stage/services/hello-world-\napp/variables.tf, and expose variables for db_remote_state_bucket,\ndb_remote_state_key, and environment:\nvariable \"db_remote_state_bucket\" {\n  description = \"The name of the S3 bucket for the database's \nremote state\"\n  type        = string\n} \n \nvariable \"db_remote_state_key\" {\n  description = \"The path for the database's remote state in S3\"\n  type        = string\n\n} \n \nvariable \"environment\" {\n  description = \"The name of the environment we're deploying to\"\n  type        = string\n  default     = \"stage\"\n}\nPass those values through to the hello-world-app module in\nlive/stage/services/hello-world-app/main.tf:\nmodule \"hello_world_app\" {\n  source = \"../../../../modules/services/hello-world-app\" \n \n  server_text            = \"Hello, World\" \n \n  environment            = var.environment\n  db_remote_state_bucket = var.db_remote_state_bucket\n  db_remote_state_key    = var.db_remote_state_key \n \n  instance_type      = \"t2.micro\"\n  min_size           = 2\n  max_size           = 2\n  enable_autoscaling = false\n  ami                = data.aws_ami.ubuntu.id\n}\nNow you can implement the createHelloOpts method:\nfunc createHelloOpts(\n dbOpts *terraform.Options,\n terraformDir string) *terraform.Options { \n \n return &terraform.Options{\n  TerraformDir: terraformDir, \n \n  Vars: map[string]interface{}{\n   \"db_remote_state_bucket\": \ndbOpts.BackendConfig[\"bucket\"],\n   \"db_remote_state_key\":    \ndbOpts.BackendConfig[\"key\"],\n   \"environment\":            \ndbOpts.Vars[\"db_name\"],\n  },\n\n}\n}\nNote that db_remote_state_bucket and db_remote_state_key\nare set to the values used in the BackendConfig for the mysql module\nto ensure that the hello-world-app module is reading from the exact\nsame state to which the mysql module just wrote. The environment\nvariable is set to the db_name just so all the resources are namespaced the\nsame way.\nFinally, you can implement the validateHelloApp method:\nfunc validateHelloApp(t *testing.T, helloOpts *terraform.Options) \n{\n albDnsName := terraform.OutputRequired(t, helloOpts, \n\"alb_dns_name\")\n url := fmt.Sprintf(\"http://%s\", albDnsName) \n \n maxRetries := 10\n timeBetweenRetries := 10 * time.Second \n \n http_helper.HttpGetWithRetryWithCustomValidation(\n  t,\n  url,\n  nil,\n  maxRetries,\n  timeBetweenRetries,\n  func(status int, body string) bool {\n   return status == 200 &&\n    strings.Contains(body, \"Hello, \nWorld\")\n  },\n )\n}\nThis method uses the http_helper package, just as with the unit tests,\nexcept this time, it’s with the\nhttp_helper.HttpGetWithRetryWithCustomValidation\nmethod that allows you to specify custom validation rules for the HTTP\nresponse status code and body. This is necessary to check that the HTTP\nresponse contains the string “Hello, World,” rather than equals that string' metadata={'original_pages_range': '518-526', 'source': '142_Integration_Tests', 'file_path': '/mnt/c/Users/Carlos/Documents/Github/jupiter-iaa-azure/Scripts/RAG/../../data/optimized_chunks/Libro-TF/142_Integration_Tests.pdf', 'num_pages': 9}", "page_content='exactly, as the User Data script in the hello-world-app module returns\nan HTML response with other text in it as well.\nAlright, run the integration test to see whether it worked:\n$ go test -v -timeout 30m -run \"TestHelloWorldAppStage\" \n \n(...) \n \nPASS \nok   terraform-up-and-running 795.63s\nExcellent, you now have an integration test that you can use to check that\nseveral of your modules work correctly together. This integration test is\nmore complicated than the unit test, and it takes more than twice as long\n(10–15 minutes rather than 4–5 minutes). In general, there’s not much that\nyou can do to make things faster—the bottleneck here is how long AWS\ntakes to deploy and undeploy RDS, ASGs, ALBs, etc.—but in certain\ncircumstances, you might be able to make the test code do less using test\nstages.\nTest stages\nIf you look at the code for your integration test, you may notice that it\nconsists of five distinct “stages”:\n1. Run terraform apply on the mysql module.\n2. Run terraform apply on the hello-world-app module.\n3. Run validations to make sure everything is working.\n4. Run terraform destroy on the hello-world-app module.\n5. Run terraform destroy on the mysql module.\nWhen you run these tests in a CI environment, you’ll want to run all of the\nstages, from start to finish. However, if you’re running these tests in your\nlocal dev environment while iteratively making changes to the code,\nrunning all of these stages is unnecessary. For example, if you’re making\n\nchanges only to the hello-world-app module, rerunning this entire test\nafter every change means you’re paying the price of deploying and\nundeploying the mysql module, even though none of your changes affect\nit. That adds 5 to 10 minutes of pure overhead to every test run.\nIdeally, the workflow would look more like this:\n1. Run terraform apply on the mysql module.\n2. Run terraform apply on the hello-world-app module.\n3. Now, you start doing iterative development:\na. Make a change to the hello-world-app module.\nb. Rerun terraform apply on the hello-world-app\nmodule to deploy your updates.\nc. Run validations to make sure everything is working.\nd. If everything works, move on to the next step. If not, go back to\nstep 3a.\n4. Run terraform destroy on the hello-world-app module.\n5. Run terraform destroy on the mysql module.\nHaving the ability to quickly do that inner loop in step 3 is the key to fast,\niterative development with Terraform. To support this, you need to break\nyour test code into stages, in which you can choose the stages to execute\nand those that you can skip.\nTerratest supports this natively with the test_structure package. The\nidea is that you wrap each stage of your test in a function with a name, and\nyou can then direct Terratest to skip some of those names by setting\nenvironment variables. Each test stage stores test data on disk so that it can\nbe read back from disk on subsequent test runs. Let’s try this out on\ntest/hello_world_integration_test.go, writing the skeleton of the test first\nand then filling in the underlying methods later:\n\nfunc TestHelloWorldAppStageWithStages(t *testing.T) {\n t.Parallel() \n \n // Store the function in a short variable name solely to \nmake the\n // code examples fit better in the book.\n stage := test_structure.RunTestStage \n \n // Deploy the MySQL DB\n defer stage(t, \"teardown_db\", func() { teardownDb(t, \ndbDirStage) })\n stage(t, \"deploy_db\", func() { deployDb(t, dbDirStage) }) \n \n // Deploy the hello-world-app\n defer stage(t, \"teardown_app\", func() { teardownApp(t, \nappDirStage) })\n stage(t, \"deploy_app\", func() { deployApp(t, dbDirStage, \nappDirStage) }) \n \n // Validate the hello-world-app works\n stage(t, \"validate_app\", func() { validateApp(t, \nappDirStage) })\n}\nThe structure is the same as before—deploy mysql, deploy hello-\nworld-app, validate hello-world-app, undeploy hello-world-\napp (runs at the end due to defer), undeploy mysql (runs at the end due\nto defer)—except now, each stage is wrapped in\ntest_structure.RunTestStage. The RunTestStage method\ntakes three arguments:\nt\nThe first argument is the t value that Go passes as an argument to every\nautomated test. You can use this value to manage test state. For\nexample, you can fail the test by calling t.Fail().\nStage name\nThe second argument allows you to specify the name for this test stage.\nYou’ll see an example shortly of how to use this name to skip test\nstages.\n\nThe code to execute\nThe third argument is the code to execute for this test stage. This can be\nany function.\nLet’s now implement the functions for each test stage, starting with\ndeployDb:\nfunc deployDb(t *testing.T, dbDir string) {\n dbOpts := createDbOpts(t, dbDir) \n \n // Save data to disk so that other test stages executed \nat a later\n // time can read the data back in\n test_structure.SaveTerraformOptions(t, dbDir, dbOpts) \n \n terraform.InitAndApply(t, dbOpts)\n}\nJust as before, to deploy mysql, the code calls createDbOpts and\nterraform .Ini tAn dApply. The only new thing is that, in between\nthose two steps, there is a call to\ntest_structure.SaveTerraformOptions. This writes the data in\ndbOpts to disk so that other test stages can read it later on. For example,\nhere’s the implementation of the teardownDb function:\nfunc teardownDb(t *testing.T, dbDir string) {\n dbOpts := test_structure.LoadTerraformOptions(t, dbDir)\n defer terraform.Destroy(t, dbOpts)\n}\nThis function uses test_structure.LoadTerraformOptions to\nload the dbOpts data from disk that was earlier saved by the deployDb\nfunction. The reason you need to pass this data via the hard drive rather\nthan passing it in memory is that you can run each test stage as part of a\ndifferent test run—and therefore, as part of a different process. As you’ll\nsee a little later in this chapter, on the first few runs of go test, you\nmight want to run deployDb but skip teardownDb, and then in later\n\nruns do the opposite, running teardownDb but skipping deployDb. To\nensure that you’re using the same database across all those test runs, you\nmust store that database’s information on disk.\nLet’s now implement the deployHelloApp function:\nfunc deployApp(t *testing.T, dbDir string, helloAppDir string) {\n dbOpts := test_structure.LoadTerraformOptions(t, dbDir)\n helloOpts := createHelloOpts(dbOpts, helloAppDir) \n \n // Save data to disk so that other test stages executed \nat a later\n // time can read the data back in\n test_structure.SaveTerraformOptions(t, helloAppDir, \nhelloOpts) \n \n terraform.InitAndApply(t, helloOpts)\n}\nThis function reuses the createHelloOpts function from before and\ncalls terraform.InitAndApply on it. Again, the only new behavior\nis the use of test_structure.LoadTerraformOptions to load\ndbOpts from disk and the use of\ntest_structure.SaveTerraformOptions to save helloOpts\nto disk. At this point, you can probably guess what the implementation of\nthe teardownApp method looks like:\nfunc teardownApp(t *testing.T, helloAppDir string) {\n helloOpts := test_structure.LoadTerraformOptions(t, \nhelloAppDir)\n defer terraform.Destroy(t, helloOpts)\n}\nAnd the implementation of the validateApp method:\nfunc validateApp(t *testing.T, helloAppDir string) {\n helloOpts := test_structure.LoadTerraformOptions(t, \nhelloAppDir)\n validateHelloApp(t, helloOpts)\n}\n\nSo, overall, the test code is identical to the original integration test, except\neach stage is wrapped in a call to test_structure.RunTestStage,\nand you need to do a little work to save and load data to and from disk.\nThese simple changes unlock an important ability: you can instruct Terratest\nto skip any test stage called foo by setting the environment variable\nSKIP_foo=true. Let’s go through a typical coding workflow to see how\nthis works.\nYour first step will be to run the test but to skip both of the teardown stages\nso that the mysql and hello-world-app modules stay deployed at the\nend of the test. Because the teardown stages are called teardown_db and\nteardown_app, you need to set the SKIP_teardown_db and\nSKIP_teardown_app environment variables, respectively, to direct\nTerratest to skip those two stages:\n$ SKIP_teardown_db=true \\ \n  SKIP_teardown_app=true \\ \n  go test -timeout 30m -run 'TestHelloWorldAppStageWithStages' \n \n(...) \n \nThe 'SKIP_deploy_db' environment variable is not set, \nso executing stage 'deploy_db'. \n \n(...) \n \nThe 'deploy_app' environment variable is not set, \nso executing stage 'deploy_db'. \n \n(...) \n \nThe 'validate_app' environment variable is not set, \nso executing stage 'deploy_db'. \n \n(...) \n \nThe 'teardown_app' environment variable is set, \nso skipping stage 'deploy_db'. \n \n(...) \n \nThe 'teardown_db' environment variable is set,\n\nso skipping stage 'deploy_db'. \n \n(...) \n \nPASS \nok   terraform-up-and-running 423.650s\nNow you can start iterating on the hello-world-app module, and each\ntime you make a change, you can rerun the tests, but this time, direct them\nto skip not only the teardown stages but also the mysql module deploy\nstage (because mysql is still running) so that the only things that execute\nare deploy app and the validations for the hello-world-app\nmodule:\n$ SKIP_teardown_db=true \\ \n  SKIP_teardown_app=true \\ \n  SKIP_deploy_db=true \\ \n  go test -timeout 30m -run 'TestHelloWorldAppStageWithStages' \n \n(...) \n \nThe 'SKIP_deploy_db' environment variable is set, \nso skipping stage 'deploy_db'. \n \n(...) \n \nThe 'deploy_app' environment variable is not set, \nso executing stage 'deploy_db'. \n \n(...) \n \nThe 'validate_app' environment variable is not set, \nso executing stage 'deploy_db'. \n \n(...) \n \nThe 'teardown_app' environment variable is set, \nso skipping stage 'deploy_db'. \n \n(...) \n \nThe 'teardown_db' environment variable is set, \nso skipping stage 'deploy_db'.\n\n(...) \n \nPASS \nok   terraform-up-and-running 13.824s\nNotice how fast each of these test runs is now: instead of waiting 10 to 15\nminutes after every change, you can try out new changes in 10 to 60\nseconds (depending on the change). Given that you’re likely to rerun these\nstages dozens or even hundreds of times during development, the time\nsavings can be massive.\nOnce the hello-world-app module changes are working the way you\nexpect, it’s time to clean everything up. Run the tests once more, this time\nskipping the deploy and validation stages so that only the teardown stages\nare executed:\n$ SKIP_deploy_db=true \\ \n  SKIP_deploy_app=true \\ \n  SKIP_validate_app=true \\ \n  go test -timeout 30m -run 'TestHelloWorldAppStageWithStages' \n \n(...) \n \nThe 'SKIP_deploy_db' environment variable is set, \nso skipping stage 'deploy_db'. \n \n(...) \n \nThe 'SKIP_deploy_app' environment variable is set, \nso skipping stage 'deploy_app'. \n \n(...) \n \nThe 'SKIP_validate_app' environment variable is set, \nso skipping stage 'validate_app'. \n \n(...) \n \nThe 'SKIP_teardown_app' environment variable is not set, \nso executing stage 'teardown_app'. \n \n(...) \n \nThe 'SKIP_teardown_db' environment variable is not set,' metadata={'original_pages_range': '527-534', 'source': '143_Test_stages', 'file_path': '/mnt/c/Users/Carlos/Documents/Github/jupiter-iaa-azure/Scripts/RAG/../../data/optimized_chunks/Libro-TF/143_Test_stages.pdf', 'num_pages': 8}", "page_content='so executing stage 'teardown_db'. \n \n(...) \n \nPASS \nok   terraform-up-and-running 340.02s\nUsing test stages lets you get rapid feedback from your automated tests,\ndramatically increasing the speed and quality of iterative development. It\nwon’t make any difference in how long tests take in your CI environment,\nbut the impact on the development environment is huge.\nRetries\nAfter you start running automated tests for your infrastructure code on a\nregular basis, you’re likely to run into a problem: flaky tests. That is, tests\noccasionally will fail for transient reasons, such as an EC2 Instance\noccasionally failing to launch, or a Terraform eventual consistency bug, or a\nTLS handshake error talking to S3. The infrastructure world is a messy\nplace, so you should expect intermittent failures in your tests and handle\nthem accordingly.\nTo make your tests a bit more resilient, you can add retries for known\nerrors. For example, while writing this book, I’d occasionally get the\nfollowing type of error, especially when running many tests in parallel:\n* error loading the remote state: RequestError: send request \nfailed \nPost https://xxx.amazonaws.com/: dial tcp xx.xx.xx.xx:443: \nconnect: connection refused\nTo make tests more reliable in the face of such errors, you can enable retries\nin Terratest using the MaxRetries, TimeBetweenRetries, and\nRetryableTerraformErrors arguments of\nterraform.Options:\nfunc createHelloOpts(\n dbOpts *terraform.Options,\n terraformDir string) *terraform.Options {\n\nreturn &terraform.Options{\n  TerraformDir: terraformDir, \n \n  Vars: map[string]interface{}{\n   \"db_remote_state_bucket\": \ndbOpts.BackendConfig[\"bucket\"],\n   \"db_remote_state_key\":    \ndbOpts.BackendConfig[\"key\"],\n   \"environment\":            \ndbOpts.Vars[\"db_name\"],\n  }, \n \n  // Retry up to 3 times, with 5 seconds between \nretries,\n  // on known errors\n  MaxRetries:         3,\n  TimeBetweenRetries: 5 * time.Second,\n  RetryableTerraformErrors: map[string]string{\n   \"RequestError: send request failed\": \n\"Throttling issue?\",\n  },\n }\n}\nIn the RetryableTerraformErrors argument, you can specify a map\nof known errors that warrant a retry: the keys of the map are the error\nmessages to look for in the logs (you can use regular expressions here), and\nthe values are additional information to display in the logs when Terratest\nmatches one of these errors and kicks off a retry. Now, whenever your test\ncode hits one of these known errors, you should see a message in your logs,\nfollowed by a sleep of TimeBetweenRetries, and then your command\nwill rerun:\n$ go test -v -timeout 30m \n \n(...) \n \nRunning command terraform with args [apply -input=false -\nlock=false \n-auto-approve] \n \n(...)' metadata={'original_pages_range': '535-536', 'source': '144_Retries', 'file_path': '/mnt/c/Users/Carlos/Documents/Github/jupiter-iaa-azure/Scripts/RAG/../../data/optimized_chunks/Libro-TF/144_Retries.pdf', 'num_pages': 2}", "page_content='* error loading the remote state: RequestError: send request \nfailed \nPost https://s3.amazonaws.com/: dial tcp 11.22.33.44:443: \nconnect: connection refused \n \n(...) \n \n'terraform [apply]' failed with the error 'exit status code 1' \nbut this error was expected and warrants a retry. Further \ndetails: \nIntermittent error, possibly due to throttling? \n \n(...) \n \nRunning command terraform with args [apply -input=false -\nlock=false \n-auto-approve]\nEnd-to-End Tests\nNow that you have unit tests and integration tests in place, the final type of\ntests that you might want to add are end-to-end tests. With the Ruby web\nserver example, end-to-end tests might consist of deploying the web server\nand any data stores it depends on and testing it from the web browser using\na tool such as Selenium. The end-to-end tests for Terraform infrastructure\nwill look similar: deploy everything into an environment that mimics\nproduction, and test it from the end-user’s perspective.\nAlthough you could write your end-to-end tests using the exact same\nstrategy as the integration tests—that is, create a few dozen test stages to\nrun terraform apply, do some validations, and then run terraform\ndestroy—this is rarely done in practice. The reason for this has to do\nwith the test pyramid, which you can see in Figure 9-1.\n\nFigure 9-1. The test pyramid.\nThe idea of the test pyramid is that you should typically be aiming for a\nlarge number of unit tests (the bottom of the pyramid), a smaller number of\nintegration tests (the middle of the pyramid), and an even smaller number of\nend-to-end tests (the top of the pyramid). This is because, as you go up the\npyramid, the cost and complexity of writing the tests, the brittleness of the\ntests, and the runtime of the tests all increase.\nThat gives us key testing takeaway #5: smaller modules are easier and faster\nto test.\nYou saw in the previous sections that it required a fair amount of work with\nnamespacing, dependency injection, retries, error handling, and test stages\nto test even a relatively simple hello-world-app module. With larger\nand more complicated infrastructure, this only becomes more difficult.\nTherefore, you want to do as much of your testing as low in the pyramid as\nyou can because the bottom of the pyramid offers the fastest, most reliable\nfeedback loop.\n\nIn fact, by the time you get to the top of the test pyramid, running tests to\ndeploy a complicated architecture from scratch becomes untenable for two\nmain reasons:\nToo slow\nDeploying your entire architecture from scratch and then undeploying it\nall again can take a very long time: on the order of several hours. Test\nsuites that take that long provide relatively little value because the\nfeedback loop is simply too slow. You’d probably run such a test suite\nonly overnight, which means in the morning you’ll get a report about a\ntest failure, you’ll investigate for a while, submit a fix, and then wait for\nthe next day to see whether it worked. That limits you to roughly one\nbug fix attempt per day. In these sorts of situations, what actually\nhappens is developers begin blaming others for test failures, convince\nmanagement to deploy despite the test failures, and eventually ignore\nthe test failures entirely.\nToo brittle\nAs mentioned in the previous section, the infrastructure world is messy.\nAs the amount of infrastructure you’re deploying goes up, the odds of\nhitting an intermittent, flaky issue goes up as well. For example,\nsuppose that a single resource (such as an EC2 Instance) has a one-in-a-\nthousand chance (0.1%) of failing due to an intermittent error (actual\nfailure rates in the DevOps world are likely higher). This means that the\nprobability that a test that deploys a single resource runs without any\nintermittent errors is 99.9%. So what about a test that deploys two\nresources? For that test to succeed, you need both resources to deploy\nwithout intermittent errors, and to calculate those odds, you multiply the\nprobabilities: 99.9% × 99.9% = 99.8%. With three resources, the odds\nare 99.9% × 99.9% × 99.9% = 99.7%. With N resources, the formula is\n99.9%.\nSo now let’s consider different types of automated tests. If you had a\nunit test of a single module that deployed, say, 20 resources, the odds of\nsuccess are 99.9% = 98.0%. This means that 2 test runs out of 100\nN\n20\n\nwill fail; if you add a few retries, you can typically make these tests\nfairly reliable. Now, suppose that you had an integration test of 3\nmodules that deployed 60 resources. Now the odds of success are\n99.9% = 94.1%. Again, with enough retry logic, you can typically\nmake these tests stable enough to be useful. So what happens if you\nwant to write an end-to-end test that deploys your entire infrastructure,\nwhich consists of 30 modules, or about 600 resources? The odds of\nsuccess are 99.9% = 54.9%. This means that nearly half of your test\nruns will fail for transient reasons!\nYou’ll be able to handle some of these errors with retries, but it quickly\nturns into a never-ending game of whack-a-mole. You add a retry for a\nTLS handshake timeout, only to be hit by an APT repo downtime in\nyour Packer template; you add retries to the Packer build, only to have\nthe build fail due to a Terraform eventual-consistency bug; just as you\nare applying the Band-Aid to that, the build fails due to a brief GitHub\noutage. And because end-to-end tests take so long, you get only one\nattempt, maybe two, per day to fix these issues.\nIn practice, very few companies with complicated infrastructure run end-to-\nend tests that deploy everything from scratch. Instead, the more common\ntest strategy for end-to-end tests works as follows:\n1. One time, you pay the cost of deploying a persistent, production-like\nenvironment called “test,” and you leave that environment running.\n2. Every time someone makes a change to your infrastructure code, the\nend-to-end test does the following:\na. Applies the infrastructure change to the test environment.\nb. Runs validations against the test environment (e.g., uses Selenium\nto test your code from the end-user’s perspective) to make sure\neverything is working.\nBy changing your end-to-end test strategy to applying only incremental\nchanges, you’re reducing the number of resources that are being deployed\n60\n600' metadata={'original_pages_range': '537-540', 'source': '145_End-to-End_Tests', 'file_path': '/mnt/c/Users/Carlos/Documents/Github/jupiter-iaa-azure/Scripts/RAG/../../data/optimized_chunks/Libro-TF/145_End-to-End_Tests.pdf', 'num_pages': 4}", "page_content='at test time from several hundred to just a handful so that these tests will be\nfaster and less brittle.\nMoreover, this approach to end-to-end testing more closely mimics how\nyou’ll be deploying those changes in production. After all, it’s not like you\ntear down and bring up your production environment from scratch to roll\nout each change. Instead, you apply each change incrementally, so this style\nof end-to-end testing offers a huge advantage: you can test not only that\nyour infrastructure works correctly but also that the deployment process for\nthat infrastructure works correctly, too.\nOther Testing Approaches\nMost of this chapter has focused on testing your Terraform code by doing a\nfull apply and destroy cycle. This is the gold standard of testing, but\nthere are three other types of automated tests you can use:\nStatic analysis\nPlan testing\nServer testing\nJust as unit, integration, and end-to-end tests each catch different types of\nbugs, each of the testing approaches just mentioned will catch different\ntypes of bugs as well, so you’ll most likely want to use several of these\ntechniques together to get the best results. Let’s go through these new\ncategories one at a time.\nStatic analysis\nStatic analysis is the most basic way to test your Terraform code: you parse\nthe code and analyze it without actually executing it in any way. Table 9-1\nshows some of the tools in this space that work with Terraform and how\nthey compare in terms of popularity and maturity, based on stats I gathered\nfrom GitHub in February 2022.' metadata={'original_pages_range': '541', 'source': '146_Other_Testing_Approaches', 'file_path': '/mnt/c/Users/Carlos/Documents/Github/jupiter-iaa-azure/Scripts/RAG/../../data/optimized_chunks/Libro-TF/146_Other_Testing_Approaches.pdf', 'num_pages': 1}", "page_content='at test time from several hundred to just a handful so that these tests will be\nfaster and less brittle.\nMoreover, this approach to end-to-end testing more closely mimics how\nyou’ll be deploying those changes in production. After all, it’s not like you\ntear down and bring up your production environment from scratch to roll\nout each change. Instead, you apply each change incrementally, so this style\nof end-to-end testing offers a huge advantage: you can test not only that\nyour infrastructure works correctly but also that the deployment process for\nthat infrastructure works correctly, too.\nOther Testing Approaches\nMost of this chapter has focused on testing your Terraform code by doing a\nfull apply and destroy cycle. This is the gold standard of testing, but\nthere are three other types of automated tests you can use:\nStatic analysis\nPlan testing\nServer testing\nJust as unit, integration, and end-to-end tests each catch different types of\nbugs, each of the testing approaches just mentioned will catch different\ntypes of bugs as well, so you’ll most likely want to use several of these\ntechniques together to get the best results. Let’s go through these new\ncategories one at a time.\nStatic analysis\nStatic analysis is the most basic way to test your Terraform code: you parse\nthe code and analyze it without actually executing it in any way. Table 9-1\nshows some of the tools in this space that work with Terraform and how\nthey compare in terms of popularity and maturity, based on stats I gathered\nfrom GitHub in February 2022.\n\nThe simplest of these tools is terraform validate, which is built into\nTerraform itself, which can catch syntax issues. For example, if you forgot\nto set the alb_name parameter in examples/alb, and you ran validate,\nyou would get output similar to the following:\n$ terraform validate \n \n│ Error: Missing required argument \n│\nTable 9-1. A comparison of popular static analysis tools for Terraform\nterraform val\nidate tfsec tflint Terrascan\nBrief descriptionBuilt-in Terraform\ncommand\nSpot potential\nsecurity issues\nPluggable\nTerraform linter\nDetect complia\nand security\nviolations\nLicense (same as\nTerraform)\nMIT MPL 2.0 Apache 2.0\nBacking company(same as\nTerraform)\nAqua Security (none) Accurics\nStars (same as\nTerraform)\n3,874 2,853 2,768\nContributors (same as\nTerraform)\n96 77 63\nFirst release (same as\nTerraform)\n2019 2016 2017\nLatest release (same as\nTerraform)\nv1.1.2 v0.34.1 v1.13.0\nBuilt-in checks Syntax checks onlyAWS, Azure, GCP,\nKubernetes,\nDigitalOcean, etc.\nAWS, Azure, and\nGCP\nAWS, Azure, G\nKubernetes, etc\nCustom checks Not supported Defined in YAML\nor JSON\nDefined in a Go\nplugin\nDefined in Reg\n\n│ \n│   on main.tf line 20, in module \"alb\": \n│   20: module \"alb\" { \n│ \n│ The argument \"alb_name\" is required, but no definition was \nfound.\nNote that validate is limited solely to syntactic checks, whereas the\nother tools allow you to enforce other types of policies. For example, you\ncan use tools such as tfsec and tflint to enforce policies, such as:\nSecurity groups cannot be too open: e.g., block inbound rules that\nallow access from all IPs (CIDR block 0.0.0.0/0).\nAll EC2 Instances must follow a specific tagging convention.\nThe idea here is to define your policies as code, so you can enforce your\nsecurity, compliance, and reliability requirements as code. In the next few\nsections, you’ll see several other policy as code tools.\nStrengths of static analysis tools\nThey run fast.\nThey are easy to use.\nThey are stable (no flaky tests).\nYou don’t need to authenticate to a real provider (e.g., to a real AWS\naccount).\nYou don’t have to deploy/undeploy real resources.\nWeaknesses of static analysis tools\nThey are very limited in the types of errors they can catch. Namely,\nthey can only catch errors that can be determined from statically\nreading the code, without executing it: e.g., syntax errors, type\nerrors, and a small subset of business logic errors. For example, you\ncan detect a policy violation for static values, such as a security' metadata={'original_pages_range': '541-543', 'source': '147_Static_analysis', 'file_path': '/mnt/c/Users/Carlos/Documents/Github/jupiter-iaa-azure/Scripts/RAG/../../data/optimized_chunks/Libro-TF/147_Static_analysis.pdf', 'num_pages': 3}", "page_content='group hardcoded to allow inbound access from CIDR block\n0.0.0.0/0, but you can’t detect policy violations from dynamic\nvalues, such as the same security group but with the inbound CIDR\nblock being read in from a variable or file.\nThese tests aren’t checking functionality, so it’s possible for all the\nchecks to pass and the infrastructure still doesn’t work!\nPlan testing\nAnother way to test your code is to run terraform plan and to analyze\nthe plan output. Since you’re executing the code, this is more than static\nanalysis, but it’s less than a unit or integration test, as you’re not executing\nthe code fully: in particular, plan executes the read steps (e.g., fetching\nstate, executing data sources) but not the write steps (e.g., creating or\nmodifying resources). Table 9-2 shows some of the tools that do plan\ntesting and how they compare in terms of popularity and maturity, based on\nstats I gathered from GitHub in February 2022.\n\nSince you’re already familiar with Terratest, let’s take a quick look at how\nyou can use it to do plan testing on the code in examples/alb. If you ran\nterraform plan manually, here’s a snippet of the output you’d get:\nTerraform will perform the following actions: \n \n  # module.alb.aws_lb.example will be created \n  + resource \"aws_lb\" \"example\" { \n      + arn                        = (known after apply) \n      + load_balancer_type         = \"application\" \n      + name                       = \"test-4Ti6CP\" \n      (...) \n    } \n \n  (...) \nTable 9-2. A comparison of popular plan testing tools for Terraform\nTerratest\nOpen Policy\nAgent (OPA)\nHashiCorp\nSentinel Checkov\nBrief descriptionGo library for IaC\ntesting\nGeneral-purpose\npolicy engine\nPolicy-as-code for\nHashiCorp\nenterprise products\nPolicy-as-code \neveryone\nLicense Apache 2.0 Apache 2.0 Commercial /\nproprietary license\nApache 2.0\nBacking companyGruntwork Styra HashiCorp Bridgecrew\nStars 5,888 6,207 (not open source)3,758\nContributors 157 237 (not open source)199\nFirst release 2016 2016 2017 2019\nLatest release v0.40.0 v0.37.1 v0.18.5 2.0.810\nBuilt-in checks None None None AWS, Azure, G\nKubernetes, etc\nCustom checks Defined in Go Defined in RegoDefined in SentinelDefined in Pyth\nor YAML\n\nPlan: 5 to add, 0 to change, 0 to destroy.\nHow can you test this output programmatically? Here’s the basic structure\nof a test that uses Terratest’s InitAndPlan helper to run init and plan\nautomatically:\nfunc TestAlbExamplePlan(t *testing.T) { \n t.Parallel() \n \n albName := fmt.Sprintf(\"test-%s\", random.UniqueId()) \n \n opts := &terraform.Options{ \n  // You should update this relative path to point \nat your alb \n  // example directory! \n  TerraformDir: \"../examples/alb\", \n  Vars: map[string]interface{}{ \n   \"alb_name\": albName, \n  }, \n } \n \n planString := terraform.InitAndPlan(t, opts)\n}\nEven this minimal test offers some value, in that it validates that your code\ncan successfully run plan, which checks that the syntax is valid and that\nall the read API calls work. But you can go even further. One small\nimprovement is to check that you get the expected counts at the end of the\nplan: “5 to add, 0 to change, 0 to destroy.” You can do this using the\nGetResourceCount helper\n // An example of how to check the plan output's \nadd/change/destroy counts\n resourceCounts := terraform.GetResourceCount(t, \nplanString) \n require.Equal(t, 5, resourceCounts.Add) \n require.Equal(t, 0, resourceCounts.Change) \n require.Equal(t, 0, resourceCounts.Destroy)\n\nYou can do an even more thorough check by using the\nInitAndPlanAndShowWithStructNoLogTempPlanFile helper\nto parse the plan output into a struct, which gives you programmatic\naccess to all the values and changes in that plan output. For example, you\ncould check that the plan output includes the aws_lb resource at address\nmodule.alb.aws_lb.example and that the name attribute of this\nresource is set to the expected value, as follows:\n // An example of how to check specific values in the plan \noutput\n planStruct := \n  \nterraform.InitAndPlanAndShowWithStructNoLogTempPlanFile(t, opts) \n \n alb, exists := \n  \nplanStruct.ResourcePlannedValuesMap[\"module.alb.aws_lb.example\"] \n require.True(t, exists, \"aws_lb resource must exist\") \n \n name, exists := alb.AttributeValues[\"name\"] \n require.True(t, exists, \"missing name parameter\") \n require.Equal(t, albName, name)\nThe strength of Terratest’s approach to plan testing is that it’s extremely\nflexible, as you can write arbitrary Go code to check whatever you want.\nBut this very same factor is also, in some ways, a weakness, as it makes it\nharder to get started.\nSome teams prefer a more declarative language for defining their policies as\ncode. In the last few years, Open Policy Agent (OPA) has become a popular\npolicy-as-code tool, as it allows your to capture you company’s policies as\ncode in a declarative language called Rego.\nFor example, many companies have tagging policies they want to enforce.\nA common one with Terraform code is to ensure that every resource that is\nmanaged by Terraform has a ManagedBy = terraform tag. Here is a\nsimple policy called enforce_tagging.rego you could use to check for this\ntag:\n\npackage terraform \n \nallow { \n   resource_change := input.resource_changes[_] \n   resource_change.change.after.tags[\"ManagedBy\"]\n}\nThis policy will look through the changes in a terraform plan output,\nextract the tag ManagedBy, and set an OPA variable called allow to\ntrue if that tag is set or undefined otherwise.\nNow, consider the following Terraform module:\nresource \"aws_instance\" \"example\" {\n  ami           = data.aws_ami.ubuntu.id\n  instance_type = \"t2.micro\"\n}\nThis module is not setting the required ManagedBy tag. How can we catch\nthat with OPA?\nThe first step is to run terraform plan and to save the output to a plan\nfile:\n$ terraform plan -out tfplan.binary\nOPA only operates on JSON, so the next step is to convert the plan file to\nJSON using the terraform show command:\n$ terraform show -json tfplan.binary > tfplan.json\nFinally, you can run the opa eval command to check this plan file\nagainst the enforce_tagging.rego policy:\n$ opa eval \\ \n  --data enforce_tagging.rego \\ \n  --input tfplan.json \\ \n  --format pretty \\ \n  data.terraform.allow\n\nundefined\nSince the ManagedBy tag was not set, the output from OPA is\nundefined. Now, try setting the ManagedBy tag:\nresource \"aws_instance\" \"example\" {\n  ami           = data.aws_ami.ubuntu.id\n  instance_type = \"t2.micro\" \n \n  tags = {\n    ManagedBy = \"terraform\" \n  }\n}\nRerun terraform plan, terraform show, and opa eval:\n$ terraform plan -out tfplan.binary \n \n$ terraform show -json tfplan.binary > tfplan.json \n \n$ opa eval \\ \n  --data enforce_tagging.rego \\ \n  --input tfplan.json \\ \n  --format pretty \\ \n  data.terraform.allow \n \ntrue\nThis time, the output is true, which means the policy has passed.\nUsing tools like OPA, you can enforce your company’s requirements by\ncreating a library of such policies and setting up a CI/CD pipeline that runs\nthese policies against your Terraform modules after every commit.\nStrengths of plan testing tools\nThey run fast—not quite as fast as pure static analysis but much\nfaster than unit or integration tests.\nThey are somewhat easy to use—not quite as easy as pure static\nanalysis but much easier than unit or integration tests.' metadata={'original_pages_range': '544-549', 'source': '148_Plan_testing', 'file_path': '/mnt/c/Users/Carlos/Documents/Github/jupiter-iaa-azure/Scripts/RAG/../../data/optimized_chunks/Libro-TF/148_Plan_testing.pdf', 'num_pages': 6}", "page_content='They are stable (few flaky tests)—not quite as stable as pure static\nanalysis but much more stable than unit or integration tests.\nYou don’t have to deploy/undeploy real resources.\nWeaknesses of plan testing tools\nThey are limited in the types of errors they can catch. They can\ncatch more than pure static analysis but nowhere near as many\nerrors as unit and integration testing.\nYou have to authenticate to a real provider (e.g., to a real AWS\naccount). This is required for plan to work.\nThese tests aren’t checking functionality, so it’s possible for all the\nchecks to pass and the infrastructure still doesn’t work!\nServer testing\nThere are a set of testing tools that are focused on testing that your servers\n(including virtual servers) have been properly configured. I’m not aware of\nany common name for these sorts of tools, so I’ll call it server testing.\nThese are not general-purpose tools for testing all aspects of your Terraform\ncode. In fact, most of these tools were originally built to be used with\nconfiguration management tools, such as Chef and Puppet, which were\nentirely focused on launching servers. However, as Terraform has grown in\npopularity, it’s now very common to use it to launch servers, and these tools\ncan be helpful for validating that the servers you launched are working.\nTable 9-3 shows some of the tools that do server testing and how they\ncompare in terms of popularity and maturity, based on stats I gathered from\nGitHub in February 2022.\n\nTable 9-3. A comparison of popular server testing tools\nInSpec Serverspec Goss\nBrief descriptionAuditing and testing\nframework\nRSpec tests for your\nservers\nQuick and easy server\ntesting/validation\nLicense Apache 2.0 MIT Apache 2.0\nBacking companyChef (none) (none)\nStars 2,472 2,426 4,607\nContributors 279 128 89\nFirst release 2016 2013 2015\nLatest release v4.52.9 v2.42.0 v0.3.16\nBuilt-in checks None None None\nCustom checks Defined in a Ruby-\nbased DSL\nDefined in a Ruby-\nbased DSL\nDefined in YAML\nMost of these tools provide a simple domain-specific language (DSL) for\nchecking that the servers you’ve deployed conform to some sort of\nspecification. For example, if you were testing a Terraform module that\ndeployed an EC2 Instance, you could use the following inspec code to\nvalidate that the Instance has proper permissions on specific files, has\ncertain dependencies installed, and is listening on a specific port:\ndescribe file('/etc/myapp.conf') do \n  it { should exist } \n  its('mode') { should cmp 0644 }\nend \n \ndescribe apache_conf do \n  its('Listen') { should cmp 8080 }\nend \n \ndescribe port(8080) do \n  it { should be_listening }\nend\n\nStrengths of server testing tools\nThey make it easy to validate specific properties of servers. The\nDSLs these tools offer are much easier to use for common checks\nthan doing it all from scratch.\nYou can build up a library of policy checks. Because each individual\ncheck is quick to write, per the previous bullet point, these tools\ntend to be a good way to validate a checklist of requirements,\nespecially around compliance (e.g., PCI compliance, HIPAA\ncompliance, etc.).\nThey can catch many types of errors. Since you actually have to run\napply and you validate a real, running server, these types of tests\ncatch far more types of errors than pure static analysis or plan\ntesting.\nWeaknesses of server testing tools\nThey are not as fast. These tests only work on servers that are\ndeployed, so you have to run the full apply (and perhaps\ndestroy) cycle, which can take a long time.\nThey are not as stable (some flaky tests). Since you have to run\napply and wait for real servers to deploy, you will hit various\nintermittent issues and occasionally have flaky tests.\nYou have to authenticate to a real provider (e.g., to a real AWS\naccount). This is required for the apply to work to deploy the\nservers, plus, these server testing tools all require additional\nauthentication methods—e.g., SSH—to connect to the servers\nyou’re testing.\nYou have to deploy/undeploy real resources. This takes time and\ncosts money.' metadata={'original_pages_range': '550-552', 'source': '149_Server_testing', 'file_path': '/mnt/c/Users/Carlos/Documents/Github/jupiter-iaa-azure/Scripts/RAG/../../data/optimized_chunks/Libro-TF/149_Server_testing.pdf', 'num_pages': 3}", "page_content='They only thoroughly check that servers work and not other types of\ninfrastructure.\nThese tests aren’t checking functionality, so it’s possible for all the\nchecks to pass and the infrastructure still doesn’t work!\nConclusion\nEverything in the infrastructure world is continuously changing: Terraform,\nPacker, Docker, Kubernetes, AWS, Google Cloud, Azure, and so on are all\nmoving targets. This means that infrastructure code rots very quickly. Or to\nput it another way:\nInfrastructure code without automated tests is broken.\nI mean this both as an aphorism and as a literal statement. Every single time\nI’ve gone to write infrastructure code, no matter how much effort I’ve put\ninto keeping the code clean, testing it manually, and doing code reviews, as\nsoon as I’ve taken the time to write automated tests, I’ve found numerous\nnontrivial bugs. Something magical happens when you take the time to\nautomate the testing process and, almost without exception, it flushes out\nproblems that you otherwise would’ve never found yourself (but your\ncustomers would’ve). And not only do you find these bugs when you first\nadd automated tests, but if you run your tests after every commit, you’ll\nkeep finding bugs over time, especially as the DevOps world changes all\naround you.\nThe automated tests I’ve added to my infrastructure code have caught bugs\nnot only in my own code but also in the tools I was using, including\nnontrivial bugs in Terraform, Packer, Elasticsearch, Kafka, AWS, and so on.\nWriting automated tests as shown in this chapter is not easy: it takes\nconsiderable effort to write these tests, it takes even more effort to maintain\nthem and add enough retry logic to make them reliable, and it takes still\nmore effort to keep your test environment clean to keep costs in check. But\nit’s all worth it.\n\nWhen I build a module to deploy a data store, for example, after every\ncommit to that repo, my tests fire up a dozen copies of that data store in\nvarious configurations, write data, read data, and then tear everything back\ndown. Each time those tests pass, that gives me huge confidence that my\ncode still works. If nothing else, the automated tests let me sleep better.\nThose hours I spent dealing with retry logic and eventual consistency pay\noff in the hours I won’t be spending at 3 a.m. dealing with an outage.\n\nTHIS BOOK HAS TESTS, TOO!\nAll of the code examples in this book have tests, too. You can find all of the code\nexamples, and all of their corresponding tests, at GitHub.\nThroughout this chapter, you saw the basic process of testing Terraform\ncode, including the following key takeaways:\nWhen testing Terraform code, you can’t use localhost\nTherefore, you need to do all of your manual testing by deploying real\nresources into one or more isolated sandbox environments.\nYou cannot do pure unit testing for Terraform code\nTherefore, you have to do all of your automated testing by writing code\nthat deploys real resources into one or more isolated sandbox\nenvironments.\nRegularly clean up your sandbox environments\nOtherwise, the environments will become unmanageable, and costs will\nspiral out of control.\nYou must namespace all of your resources\nThis ensures that multiple tests running in parallel do not conflict with\none another.\nSmaller modules are easier and faster to test\nThis was one of the key takeaways in Chapter 8, and it’s worth\nrepeating in this chapter, too: smaller modules are easier to create,\nmaintain, use, and test.\nYou also saw a number of different testing approaches throughout this\nchapter: unit testing, integration testing, end-to-end testing, static analysis,\n\nand so on. Table 9-4 shows the trade-offs between these different types of\ntests.\nSo which testing approach should you use? The answer is: a mix of all of\nthem! Each type of test has strengths and weaknesses, so you have to\ncombine multiple types of tests to be confident your code works as\nexpected. That doesn’t mean that you use all the different types of tests in\nequal proportion: recall the test pyramid and how, in general, you’ll\ntypically want lots of unit tests, fewer integration tests, and only a small\nnumber of high-value end-to-end tests. Moreover, you don’t have to add all\nthe different types of tests at once. Instead, pick the ones that give you the\nbest bang for your buck and add those first. Almost any testing is better\nTable 9-4. A comparison of testing approaches (more black squares is better)\nStatic analysisPlan testing Server testingUnit tests\nFast to run ■■■■■ ■■■■□ ■■■□□ ■■□□□\nCheap to run ■■■■■ ■■■■□ ■■■□□ ■■□□□\nStable and reliable■■■■■ ■■■■□ ■■■□□ ■■□□□\nEasy to use ■■■■■ ■■■■□ ■■■□□ ■■□□□\nCheck syntax ■■■■■ ■■■■■ ■■■■■ ■■■■■\nCheck policies ■■□□□ ■■■■□ ■■■■□ ■■■■■\nCheck servers work□□□□□ □□□□□ ■■■■■ ■■■■■\nCheck other\ninfrastructure\nworks\n□□□□□ □□□□□ ■■□□□ ■■■■□\nCheck all the\ninfrastructure\nworks together\n□□□□□ □□□□□ □□□□□ ■□□□□\n\nthan none, so if all you can add for now is static analysis, then use that as a\nstarting point, and build on top of it incrementally.\nLet’s now move on to Chapter 10, where you’ll see how to incorporate\nTerraform code and your automated test code into your team’s workflow,\nincluding how to manage environments, how to configure a CI/CD pipeline,\nand more.\n1 AWS doesn’t charge anything extra for additional AWS accounts, and if you use AWS\nOrganizations, you can create multiple “child” accounts that all roll up their billing to a single\nroot account, as you saw in Chapter 7.\n2 In limited cases, it is possible to override the endpoints Terraform uses to communicate with\nproviders, such as overriding the endpoints Terraform uses to talk to AWS to instead talk to a\nmocking tool called LocalStack. This works for a small number of endpoints, but most\nTerraform code makes hundreds of different API calls to the underlying provider, and mocking\nout all of them is impractical. Moreover, even if you do mock them all out, it’s not clear that\nthe resulting unit test can give you much confidence that your code works correctly: e.g., if you\ncreate mock endpoints for ASGs and ALBs, your terraform apply might succeed, but\ndoes that tell you anything useful about whether your code would have actually deployed a\nworking app on top of that infrastructure?' metadata={'original_pages_range': '553-557', 'source': '150_Conclusion', 'file_path': '/mnt/c/Users/Carlos/Documents/Github/jupiter-iaa-azure/Scripts/RAG/../../data/optimized_chunks/Libro-TF/150_Conclusion.pdf', 'num_pages': 5}", "page_content='Chapter 10. How to Use\nTerraform as a Team\nAs you’ve been reading this book and working through the code samples,\nyou’ve most likely been working by yourself. In the real world, you’ll most\nlikely be working as part of a team, which introduces a number of new\nchallenges. You may need to find a way to convince your team to use\nTerraform and other infrastructure-as-code (IaC) tools. You may need to\ndeal with multiple people concurrently trying to understand, use, and\nmodify the Terraform code you write. And you may need to figure out how\nto fit Terraform into the rest of your tech stack and make it a part of your\ncompany’s workflow.\nIn this chapter, I’ll dive into the key processes you need to put in place to\nmake Terraform and IaC work for your team:\nAdopting infrastructure as code in your team\nA workflow for deploying application code\nA workflow for deploying infrastructure code\nPutting it all together\nLet’s go through these topics one at a time.\nEXAMPLE CODE\nAs a reminder, you can find all of the code examples in the book on GitHub.\nAdopting IaC in Your Team' metadata={'original_pages_range': '558', 'source': '151_10._How_to_Use_Terraform_as_a_Team_y_1_mas', 'file_path': '/mnt/c/Users/Carlos/Documents/Github/jupiter-iaa-azure/Scripts/RAG/../../data/optimized_chunks/Libro-TF/151_10._How_to_Use_Terraform_as_a_Team_y_1_mas.pdf', 'num_pages': 1}", "page_content='If your team is used to managing all of your infrastructure by hand,\nswitching to infrastructure as code requires more than just introducing a\nnew tool or technology. It also requires changing the culture and processes\nof the team. Changing culture and process is a significant undertaking,\nespecially at larger companies. Because every team’s culture and process is\na little different, there’s no one-size-fits-all way to do it, but here are a few\ntips that will be useful in most situations:\nConvince your boss\nWork incrementally\nGive your team the time to learn\nConvince Your Boss\nI’ve seen this story play out many times at many companies: a developer\ndiscovers Terraform, becomes inspired by what it can do, shows up to work\nfull of enthusiasm and excitement, shows Terraform to everyone… and the\nboss says “no.” The developer, of course, becomes frustrated and\ndiscouraged. Why doesn’t everyone else see the benefits of this? We could\nautomate everything! We could avoid so many bugs! How else can we pay\ndown all this tech debt? How can you all be so blind??\nThe problem is that although this developer sees all the benefits of adopting\nan IaC tool such as Terraform, they aren’t seeing all the costs. Here are just\na few of the costs of adopting IaC:\nSkills gap\nThe move to IaC means that your Ops team will need to spend most of\nits time writing large amounts of code: Terraform modules, Go tests,\nChef recipes, and so on. Whereas some Ops engineers are comfortable\nwith coding all day and will love the change, others will find this a\ntough transition. Many Ops engineers and sysadmins are used to making\nchanges manually, with perhaps an occasional short script here or there,\nand the move to doing software engineering nearly full time might\nrequire learning a number of new skills or hiring new people.\n\nNew tools\nSoftware developers can become attached to the tools they use; some\nare nearly religious about it. Every time you introduce a new tool, some\ndevelopers will be thrilled at the opportunity to learn something new,\nbut others will prefer to stick to what they know and may resist having\nto invest lots of time and energy learning new languages and techniques.\nChange in mindset\nIf your team members are used to managing infrastructure manually,\nthey are used to making all of their changes directly: for example, by\nSSHing to a server and executing a few commands. The move to IaC\nrequires a shift in mindset where you make all of your changes\nindirectly, first by editing code, then checking it in, and then letting\nsome automated process apply the changes. This layer of indirection can\nbe frustrating; for simple tasks, it’ll feel slower than the direct option,\nespecially when you’re still learning a new IaC tool and are not efficient\nwith it.\nOpportunity cost\nIf you choose to invest your time and resources in one project, you are\nimplicitly choosing not to invest that time and resources in other\nprojects. What projects will have to be put on hold so that you can\nmigrate to IaC? How important are those projects?\nSome developers on your team will look at this list and become excited. But\nmany others will groan—including your boss. Learning new skills,\nmastering new tools, and adopting new mindsets may or may not be\nbeneficial, but one thing is certain: it is not free. Adopting IaC is a\nsignificant investment, and as with any investment, you need to consider\nnot only the potential upside but also the potential downsides.\nYour boss in particular will be sensitive to the opportunity cost. One of the\nkey responsibilities of any manager is to make sure the team is working on\n\nthe highest-priority projects. When you show up and excitedly start talking\nabout Terraform, what your boss might really be hearing is, “Oh no, this\nsounds like a massive undertaking. How much time is it going to take?” It’s\nnot that your boss is blind to what Terraform can do, but if you are spending\ntime on that, you might not have time to deploy the new app the search\nteam has been asking about for months, or to prepare for the Payment Card\nIndustry (PCI) audit, or to dig into the outage from last week. So, if you\nwant to convince your boss that your team should adopt IaC, your goal is\nnot to prove that it has value but that it will bring more value to your team\nthan anything else you could work on during that time.\nOne of the least effective ways to do this is to just list the features of your\nfavorite IaC tool: for example, Terraform is declarative, it’s popular, it’s\nopen source. This is one of many areas where developers would do well to\nlearn from salespeople. Most salespeople know that focusing on features is\ntypically an ineffective way to sell products. A slightly better technique is to\nfocus on benefits: that is, instead of talking about what a product can do\n(“product X can do Y!”), you should talk about what the customer can do\nby using that product (“you can do Y by using product X!”). In other words,\nshow the customer what new superpowers your product can give them.\nFor example, instead of telling your boss that Terraform is declarative, talk\nabout how your infrastructure will be far easier to maintain. Instead of\ntalking about the fact that Terraform is popular, talk about how you’ll be\nable to leverage lots of existing modules and plugins to get things done\nfaster. And instead of explaining to your boss that Terraform is open source,\nhelp your boss see how much easier it will be to hire new developers for the\nteam from a large, active open source community.\nFocusing on benefits is a great start, but the best salespeople know an even\nmore effective strategy: focus on the problems. If you watch a great\nsalesperson talking to a customer, you’ll notice that it’s actually the\ncustomer that does most of the talking. The salesperson spends most of their\ntime listening and looking for one specific thing: What is the key problem\nthat customer is trying to solve? What’s the biggest pain point? Instead of\ntrying to sell some sort of features or benefits, the best salespeople try to\n\nsolve their customer’s problems. If that solution happens to include the\nproduct they are selling, all the better, but the real focus is on problem\nsolving, not selling.\nTalk to your boss and try to understand the most important problems they\nare working on that quarter or that year. You might find that those problems\nwould not be solved by IaC. And that’s OK! It might be slightly heretical\nfor the author of a book on Terraform to say this, but not every team needs\nIaC. Adopting IaC has a relatively high cost, and although it will pay off in\nthe long term for some scenarios, it won’t for others; for example, if you’re\nat a tiny startup with just one Ops person, or you’re working on a prototype\nthat might be thrown away in a few months, or you’re just working on a\nside project for fun, managing infrastructure by hand is often the right\nchoice. Sometimes, even if IaC would be a great fit for your team, it won’t\nbe the highest priority, and given limited resources, working on other\nprojects might still be the right choice.\nIf you do find that one of the key problems your boss is focused on can be\nsolved with IaC, then your goal is to show your boss what that world looks\nlike. For example, perhaps the biggest issue your boss is focused on this\nquarter is improving uptime. You’ve had numerous outages the last few\nmonths, many hours of downtime, customers are complaining, and the CEO\nis breathing down your manager’s neck, checking in daily to see how things\nare going. You dig in and find out that more than half of these outages were\ncaused by a manual error during deployment: e.g., someone accidentally\nskipped an important step during the rollout process, or a server was\nmisconfigured, or the infrastructure in staging didn’t match what you had in\nproduction.\nNow, when you talk to your boss, instead of talking about Terraform\nfeatures or benefits, lead with the following: “I have an idea for how to\nreduce our outages by half.” I guarantee this will get your boss’s attention.\nUse this opportunity to paint a picture for your boss of a world in which\nyour deployment process is fully automated, reliable, and repeatable so that\nthe manual errors that caused half of your previous outages are no longer\npossible. Not only that, but if deployment is automated, you can also add' metadata={'original_pages_range': '559-562', 'source': '152_Convince_Your_Boss', 'file_path': '/mnt/c/Users/Carlos/Documents/Github/jupiter-iaa-azure/Scripts/RAG/../../data/optimized_chunks/Libro-TF/152_Convince_Your_Boss.pdf', 'num_pages': 4}", "page_content='automated tests, reducing outages further and allowing the whole company\nto deploy twice as often. Let your boss dream of being the one to tell the\nCEO that they’ve managed to cut outages in half and double deployments.\nAnd then mention that, based on your research, you believe you can deliver\nthis future world using Terraform.\nThere’s no guarantee that your boss will say yes, but your odds are quite a\nbit higher with this approach. And your odds get even better if you work\nincrementally.\nWork Incrementally\nOne of the most important lessons I’ve learned in my career is that most\nlarge software projects fail. Whereas roughly 3 out of 4 small IT projects\n(less than $1 million) are completed successfully, only 1 out of 10 large\nprojects (greater than $10 million) are completed on time and on budget,\nand more than one-third of large projects are never completed at all.\nThis is why I always get worried when I see a team try to not only adopt\nIaC but to do so all at once, across a huge amount of infrastructure, across\nevery team, and often as part of an even bigger initiative. I can’t help but\nshake my head when I see the CEO or CTO of a large company give\nmarching orders that everything must be migrated to the cloud, the old\ndatacenters must be shut down, and that everyone will “do DevOps”\n(whatever that means), all within six months. I’m not exaggerating when I\nsay that I’ve seen this pattern several dozen times, and without exception,\nevery single one of these initiatives has failed. Inevitably, two to three years\nlater, every one of these companies is still working on the migration, the old\ndatacenter is still running, and no one can tell whether they are really doing\nDevOps.\nIf you want to successfully adopt IaC, or if you want to succeed at any other\ntype of migration project, the only sane way to do it is incrementally. The\nkey to incrementalism is not just splitting up the work into a series of small\nsteps but splitting up the work in such a way that every step brings its own\nvalue—even if the later steps never happen.\n1\n\nTo understand why this is so important, consider the opposite, false\nincrementalism.  Suppose that you do a huge migration project, broken up\ninto several small steps, but the project doesn’t offer any real value until the\nvery final step is completed. For example, the first step is to rewrite the\nfrontend, but you don’t launch it, because it relies on a new backend. Then,\nyou rewrite the backend, but you don’t launch that either, because it doesn’t\nwork until data is migrated to a new data store. And then, finally, the last\nstep is to do the data migration. Only after this last step do you finally\nlaunch everything and begin realizing any value from doing all this work.\nWaiting until the very end of a project to get any value is a big risk. If that\nproject is canceled or put on hold or significantly changed partway through,\nyou might get zero value out of it, despite a lot of investment.\nIn fact, this is exactly what happens with many large migration projects.\nThe project is big to begin with, and like most software projects, it takes\nmuch longer than expected. During that time, market conditions change, or\nthe original stakeholders lose patience (e.g., the CEO was OK with\nspending three months to clean up tech debt, but after 12 months, it’s time\nto begin shipping new products), and the project ends up getting canceled\nbefore completion. With false incrementalism, this gives you the worst\npossible outcome: you’ve paid a huge cost and received absolutely nothing\nin return.\nTherefore, incrementalism is essential. You want each part of the project to\ndeliver some value so that even if the project doesn’t finish, no matter what\nstep you got to, it was still worth doing. The best way to accomplish this is\nto focus on solving one, small, concrete problem at a time. For example,\ninstead of trying to do a “big bang” migration to the cloud, try to identify\none, small, specific app or team that is struggling, and work to migrate just\nthem. Or instead of trying to do a “big bang” move to “DevOps,” try to\nidentify a single, small, concrete problem (e.g., outages during deployment)\nand put in place a solution for that specific problem (e.g., automate the most\nproblematic deployment with Terraform).\nIf you can get a quick win by fixing one real, concrete problem right away,\nand making one team successful, you’ll begin to build momentum. That\n2' metadata={'original_pages_range': '563-564', 'source': '153_Work_Incrementally', 'file_path': '/mnt/c/Users/Carlos/Documents/Github/jupiter-iaa-azure/Scripts/RAG/../../data/optimized_chunks/Libro-TF/153_Work_Incrementally.pdf', 'num_pages': 2}", "page_content='team can become your cheerleader and help convince other teams to\nmigrate, too. Fixing the specific deployment issue can make the CEO happy\nand get you support to use IaC for more projects. This will allow you to go\nfor another quick win, and another one after that. And if you can keep\nrepeating this process—delivering value early and often—you’ll be far\nmore likely to succeed at the larger migration effort. But even if the larger\nmigration doesn’t work out, at least one team is more successful now and\none deployment process works better, so it was still worth the investment.\nGive Your Team the Time to Learn\nI hope that, at this point, it’s clear that adopting IaC can be a significant\ninvestment. It’s not something that will happen overnight. It’s not\nsomething that will happen magically, just because the manager gives you a\nnod. It will happen only through a deliberate effort of getting everyone on\nboard, making learning resources (e.g., documentation, video tutorials, and,\nof course, this book!) available, and providing dedicated time for team\nmembers to ramp up.\nIf your team doesn’t get the time and resources that it needs, then your IaC\nmigration is unlikely to be successful. No matter how nice your code is, if\nyour entire team isn’t on board with it, here’s how it will play out:\n1. One developer on the team is passionate about IaC and spends a few\nmonths writing beautiful Terraform code and using it to deploy lots of\ninfrastructure.\n2. The developer is happy and productive, but unfortunately, the rest of\nthe team did not get the time to learn and adopt Terraform.\n3. Then, the inevitable happens: an outage. One of your team members\nneeds to deal with it, and they have two options: either (A) fix the\noutage the way they’ve always done it, by making changes manually,\nwhich takes a few minutes, or (B) fix the outage by using Terraform,\nbut they aren’t familiar with it, so this could take hours or days. Your' metadata={'original_pages_range': '565', 'source': '154_Give_Your_Team_the_Time_to_Learn', 'file_path': '/mnt/c/Users/Carlos/Documents/Github/jupiter-iaa-azure/Scripts/RAG/../../data/optimized_chunks/Libro-TF/154_Give_Your_Team_the_Time_to_Learn.pdf', 'num_pages': 1}", "page_content='team members are probably reasonable, rational people and will\nalmost always choose option A.\n4. Now, as a result of the manual change, the Terraform code no longer\nmatches what’s actually deployed. Therefore, next time someone on\nyour team tries to use Terraform, there’s a chance that they will get a\nweird error. If they do, they will lose trust in the Terraform code and\nonce again fall back to option A, making more manual changes. This\nmakes the code even more out of sync with reality, so the odds of the\nnext person getting a weird Terraform error are even higher, and you\nquickly get into a cycle in which team members make more and more\nmanual changes.\n5. In a remarkably short time, everyone is back to doing everything\nmanually, the Terraform code is completely unusable, and the months\nspent writing it are a total waste.\nThis scenario isn’t hypothetical but something I’ve seen happen at many\ndifferent companies. They have large, expensive codebases full of beautiful\nTerraform code that are just gathering dust. To avoid this scenario, you need\nto not only convince your boss that you should use Terraform but also give\neveryone on the team the time they need to learn the tool and internalize\nhow to use it so that when the next outage happens, it’s easier to fix it in\ncode than it is to do it by hand.\nOne thing that can help teams adopt IaC faster is to have a well-defined\nprocess for using it. When you’re learning or using IaC on a small team,\nrunning it ad hoc on a developer’s computer is good enough. But as your\ncompany and IaC usage grows, you’ll want to define a more systematic,\nrepeatable, automated workflow for how deployments happen.\nA Workflow for Deploying Application Code\nIn this section, I’ll introduce a typical workflow for taking application code\n(e.g., a Ruby on Rails or Java/Spring app) from development all the way to\nproduction. This workflow is reasonably well understood in the DevOps' metadata={'original_pages_range': '566', 'source': '155_A_Workflow_for_Deploying_Application_Code', 'file_path': '/mnt/c/Users/Carlos/Documents/Github/jupiter-iaa-azure/Scripts/RAG/../../data/optimized_chunks/Libro-TF/155_A_Workflow_for_Deploying_Application_Code.pdf', 'num_pages': 1}", "page_content='industry, so you’ll probably be familiar with parts of it. Later in this chapter,\nI’ll talk about a workflow for taking infrastructure code (e.g., Terraform\nmodules) from development to production. This workflow is not nearly as\nwell known in the industry, so it will be helpful to compare that workflow\nside by side with the application workflow to understand how to translate\neach application code step to an analogous infrastructure code step.\nHere’s what the application code workflow looks like:\n1. Use version control.\n2. Run the code locally.\n3. Make code changes.\n4. Submit changes for review.\n5. Run automated tests.\n6. Merge and release.\n7. Deploy.\nLet’s go through these steps one at a time.\nUse Version Control\nAll of your code should be in version control. No exceptions. It was the #1\nitem on the classic Joel Test when Joel Spolsky created it more than 20\nyears ago, and the only things that have changed since then are that (a) with\ntools like GitHub, it’s easier than ever to use version control and (b) you\ncan represent more and more things as code. This includes documentation\n(e.g., a README written in Markdown), application configuration (e.g., a\nconfig file written in YAML), specifications (e.g., test code written with\nRSpec), tests (e.g., automated tests written with JUnit), databases (e.g.,\nschema migrations written in ActiveRecord), and of course, infrastructure.\nAs in the rest of this book, I’m going to assume that you’re using Git for\nversion control. For example, here is how you can check out the code repo' metadata={'original_pages_range': '567', 'source': '156_Use_Version_Control', 'file_path': '/mnt/c/Users/Carlos/Documents/Github/jupiter-iaa-azure/Scripts/RAG/../../data/optimized_chunks/Libro-TF/156_Use_Version_Control.pdf', 'num_pages': 1}", "page_content='for this book:\n$ git clone https://github.com/brikis98/terraform-up-and-running-\ncode.git\nBy default, this checks out the main branch of your repo, but you’ll most\nlikely do all of your work in a separate branch. Here’s how you can create a\nbranch called example-feature and switch to it by using the git\ncheckout command:\n$ cd terraform-up-and-running-code \n$ git checkout -b example-feature \nSwitched to a new branch 'example-feature'\nRun the Code Locally\nNow that the code is on your computer, you can run it locally. You may\nrecall the Ruby web server example from Chapter 9, which you can run as\nfollows:\n$ cd code/ruby/10-terraform/team \n$ ruby web-server.rb \n \n[2019-06-15 15:43:17] INFO  WEBrick 1.3.1 \n[2019-06-15 15:43:17] INFO  ruby 2.3.7 (2018-03-28) \n[universal.x86_64-darwin17] \n[2019-06-15 15:43:17] INFO  WEBrick::HTTPServer#start: pid=28618 \nport=8000\nNow you can manually test it with curl:\n$ curl http://localhost:8000 \nHello, World\nAlternatively, you can run the automated tests:\n$ ruby web-server-test.rb \n \n(...)' metadata={'original_pages_range': '568', 'source': '157_Run_the_Code_Locally', 'file_path': '/mnt/c/Users/Carlos/Documents/Github/jupiter-iaa-azure/Scripts/RAG/../../data/optimized_chunks/Libro-TF/157_Run_the_Code_Locally.pdf', 'num_pages': 1}", "page_content='Finished in 0.633175 seconds. \n-------------------------------------------- \n8 tests, 24 assertions, 0 failures, 0 errors \n100% passed \n--------------------------------------------\nThe key thing to notice is that both manual and automated tests for\napplication code can run completely locally on your own computer. You’ll\nsee later in this chapter that this is not true for the same part of the\nworkflow for infrastructure changes.\nMake Code Changes\nNow that you can run the application code, you can begin making changes.\nThis is an iterative process in which you make a change, rerun your manual\nor automated tests to see whether the change worked, make another change,\nrerun the tests, and so on.\nFor example, you can change the output of web-server.rb to “Hello, World\nv2,” restart the server, and see the result:\n$ curl http://localhost:8000 \nHello, World v2\nYou might also update and rerun the automated tests. The idea in this part of\nthe workflow is to optimize the feedback loop so that the time between\nmaking a change and seeing whether it worked is minimized.\nAs you work, you should regularly be committing your code, with clear\ncommit messages explaining the changes you’ve made:\n$ git commit -m \"Updated Hello, World text\"\nSubmit Changes for Review\nEventually, the code and tests will work the way you want them to, so it’s\ntime to submit your changes for a code review. You can do this with a\nseparate code review tool (e.g., Phabricator or Review Board) or, if you’re' metadata={'original_pages_range': '569', 'source': '158_Make_Code_Changes', 'file_path': '/mnt/c/Users/Carlos/Documents/Github/jupiter-iaa-azure/Scripts/RAG/../../data/optimized_chunks/Libro-TF/158_Make_Code_Changes.pdf', 'num_pages': 1}", "page_content='Finished in 0.633175 seconds. \n-------------------------------------------- \n8 tests, 24 assertions, 0 failures, 0 errors \n100% passed \n--------------------------------------------\nThe key thing to notice is that both manual and automated tests for\napplication code can run completely locally on your own computer. You’ll\nsee later in this chapter that this is not true for the same part of the\nworkflow for infrastructure changes.\nMake Code Changes\nNow that you can run the application code, you can begin making changes.\nThis is an iterative process in which you make a change, rerun your manual\nor automated tests to see whether the change worked, make another change,\nrerun the tests, and so on.\nFor example, you can change the output of web-server.rb to “Hello, World\nv2,” restart the server, and see the result:\n$ curl http://localhost:8000 \nHello, World v2\nYou might also update and rerun the automated tests. The idea in this part of\nthe workflow is to optimize the feedback loop so that the time between\nmaking a change and seeing whether it worked is minimized.\nAs you work, you should regularly be committing your code, with clear\ncommit messages explaining the changes you’ve made:\n$ git commit -m \"Updated Hello, World text\"\nSubmit Changes for Review\nEventually, the code and tests will work the way you want them to, so it’s\ntime to submit your changes for a code review. You can do this with a\nseparate code review tool (e.g., Phabricator or Review Board) or, if you’re\n\nusing GitHub, you can create a pull request. There are several different\nways to create a pull request. One of the easiest is to git push your\nexample-feature branch back to origin (that is, back to GitHub\nitself), and GitHub will automatically print out a pull request URL in the\nlog output:\n$ git push origin example-feature \n \n(...) \n \nremote: Resolving deltas: 100% (1/1), completed with 1 local \nobject. \nremote: \nremote: Create a pull request for 'example-feature' on GitHub by \nvisiting: \nremote:      https://github.com/<OWNER>/<REPO>/pull/new/example-\nfeature \nremote:\nOpen that URL in your browser, fill out the pull request title and\ndescription, and then click Create. Your team members will now be able to\nreview the changes, as shown in Figure 10-1.' metadata={'original_pages_range': '569-570', 'source': '159_Submit_Changes_for_Review', 'file_path': '/mnt/c/Users/Carlos/Documents/Github/jupiter-iaa-azure/Scripts/RAG/../../data/optimized_chunks/Libro-TF/159_Submit_Changes_for_Review.pdf', 'num_pages': 2}", "page_content='Figure 10-1. Your team members can review your code changes in a GitHub pull request.\nRun Automated Tests\nYou should set up commit hooks to run automated tests for every commit\nyou push to your version control system. The most common way to do this\nis to use a continuous integration (CI) server, such as Jenkins, CircleCI, or\nGitHub Actions. Most popular CI servers have integrations built in\nspecifically for GitHub, so not only does every commit automatically run\ntests, but the output of those tests shows up in the pull request itself, as\nshown in Figure 10-2.\nYou can see in Figure 10-2 that CircleCI has run unit tests, integration tests,\nend-to-end tests, and some static analysis checks (in the form of security\n\nvulnerability scanning using a tool called snyk) against the code in the\nbranch, and everything passed.\n\n' metadata={'original_pages_range': '571-573', 'source': '160_Run_Automated_Tests', 'file_path': '/mnt/c/Users/Carlos/Documents/Github/jupiter-iaa-azure/Scripts/RAG/../../data/optimized_chunks/Libro-TF/160_Run_Automated_Tests.pdf', 'num_pages': 3}", "page_content='Figure 10-2. GitHub pull request showing automated test results from CircleCI.\nMerge and Release\nYour team members should review your code changes, looking for potential\nbugs, enforcing coding guidelines (more on this later in the chapter),\nchecking that the existing tests passed, and ensuring that you’ve added tests\nfor any new behavior. If everything looks good, your code can be merged\ninto the main branch.\nThe next step is to release the code. If you’re using immutable\ninfrastructure practices (as discussed in “Server Templating Tools”),\nreleasing application code means packaging that code into a new,\nimmutable, versioned artifact. Depending on how you want to package and\ndeploy your application, the artifact can be a new Docker image, a new\nvirtual machine image (e.g., new AMI), a new .jar file, a new .tar file, etc.\nWhatever format you pick, make sure the artifact is immutable (i.e., you\nnever change it) and that it has a unique version number (so you can\ndistinguish this artifact from all of the others).\nFor example, if you are packaging your application using Docker, you can\nstore the version number in a Docker tag. You could use the ID of the\ncommit (the sha1 hash) as the tag so that you can map the Docker image\nyou’re deploying back to the exact code it contains:\n$ commit_id=$(git rev-parse HEAD) \n$ docker build -t brikis98/ruby-web-server:$commit_id .\nThe preceding code will build a new Docker image called\nbrikis98/ruby-web-server and tag it with the ID of the most\nrecent commit, which will look something like\n92e3c6380ba6d1e8c9134452ab6e26154e6ad849. Later on, if\nyou’re debugging an issue in a Docker image, you can see the exact code it\ncontains by checking out the commit ID the Docker image has as a tag:\n$ git checkout 92e3c6380ba6d1e8c9134452ab6e26154e6ad849 \nHEAD is now at 92e3c63 Updated Hello, World text' metadata={'original_pages_range': '574', 'source': '161_Merge_and_Release', 'file_path': '/mnt/c/Users/Carlos/Documents/Github/jupiter-iaa-azure/Scripts/RAG/../../data/optimized_chunks/Libro-TF/161_Merge_and_Release.pdf', 'num_pages': 1}", "page_content='One downside to commit IDs is that they aren’t very readable or\nmemorable. An alternative is to create a Git tag:\n$ git tag -a \"v0.0.4\" -m \"Update Hello, World text\" \n$ git push --follow-tags\nA tag is a pointer to a specific Git commit but with a friendlier name. You\ncan use this Git tag on your Docker images:\n$ git_tag=$(git describe --tags) \n$ docker build -t brikis98/ruby-web-server:$git_tag .\nThus, when you’re debugging, check out the code at a specific tag:\n$ git checkout v0.0.4 \nNote: checking out 'v0.0.4'. \n(...) \nHEAD is now at 92e3c63 Updated Hello, World text\nDeploy\nNow that you have a versioned artifact, it’s time to deploy it. There are\nmany different ways to deploy application code, depending on the type of\napplication, how you package it, how you want to run it, your architecture,\nwhat tools you’re using, and so on. Here are a few of the key\nconsiderations:\nDeployment tooling\nDeployment strategies\nDeployment server\nPromotion across environments\nDeployment tooling\nThere are many different tools that you can use to deploy your application,\ndepending on how you package it and how you want to run it. Here are a' metadata={'original_pages_range': '575', 'source': '162_Deploy_y_1_mas', 'file_path': '/mnt/c/Users/Carlos/Documents/Github/jupiter-iaa-azure/Scripts/RAG/../../data/optimized_chunks/Libro-TF/162_Deploy_y_1_mas.pdf', 'num_pages': 1}", "page_content='few examples:\nTerraform\nAs you’ve seen in this book, you can use Terraform to deploy certain\ntypes of applications. For example, in earlier chapters, you created a\nmodule called asg-rolling-deploy that could do a zero-\ndowntime rolling deployment across an ASG. If you package your\napplication as an AMI (e.g., using Packer), you could deploy new AMI\nversions with the asg-rolling-deploy module by updating the\nami parameter in your Terraform code and running terraform\napply.\nOrchestration tools\nThere are a number of orchestration tools designed to deploy and\nmanage applications, such as Kubernetes (arguably the most popular\nDocker orchestration tool), Amazon ECS, HashiCorp Nomad, and\nApache Mesos. In Chapter 7, you saw an example of how to use\nKubernetes to deploy Docker containers.\nScripts\nTerraform and most orchestration tools support only a limited set of\ndeployment strategies (discussed in the next section). If you have more\ncomplicated requirements, you may have to write custom scripts to\nimplement these requirements.\nDeployment strategies\nThere are a number of different strategies that you can use for application\ndeployment, depending on your requirements. Suppose that you have five\ncopies of the old version of your app running, and you want to roll out a\nnew version. Here are a few of the most common strategies you can use:\nRolling deployment with replacement\n\nTake down one of the old copies of the app, deploy a new copy to\nreplace it, wait for the new copy to come up and pass health checks,\nstart sending the new copy live traffic, and then repeat the process until\nall of the old copies have been replaced. Rolling deployment with\nreplacement ensures that you never have more than five copies of the\napp running, which can be useful if you have limited capacity (e.g., if\neach copy of the app runs on a physical server) or if you’re dealing with\na stateful system where each app has a unique identity (e.g., this is often\nthe case with consensus systems, such as Apache ZooKeeper). Note that\nthis deployment strategy can work with larger batch sizes (you can\nreplace more than one copy of the app at a time if you can handle the\nload and won’t lose data with fewer apps running) and that during\ndeployment, you will have both the old and new versions of the app\nrunning at the same time.\nRolling deployment without replacement\nDeploy one new copy of the app, wait for the new copy to come up and\npass health checks, start sending the new copy live traffic, undeploy an\nold copy of the app, and then repeat the process until all the old copies\nhave been replaced. Rolling deployment without replacement works\nonly if you have flexible capacity (e.g., your apps run in the cloud,\nwhere you can spin up new virtual servers any time you want) and if\nyour application can tolerate more than five copies of it running at the\nsame time. The advantage is that you never have less than five copies of\nthe app running, so you’re not running at a reduced capacity during\ndeployment. Note that this deployment strategy can also work with\nlarger batch sizes (if you have the capacity for it, you can deploy five\nnew copies all at once) and that during deployment, you will have both\nthe old and new versions of the app running at the same time.\nBlue-green deployment\nDeploy five new copies of the app, wait for all of them to come up and\npass health checks, shift all live traffic to the new copies at the same\ntime, and then undeploy the old copies. Blue-green deployment works\n\nonly if you have flexible capacity (e.g., your apps run in the cloud,\nwhere you can spin up new virtual servers any time you want) and if\nyour application can tolerate more than five copies of it running at the\nsame time. The advantage is that only one version of your app is visible\nto users at any given time and that you never have less than five copies\nof the app running, so you’re not running at a reduced capacity during\ndeployment.\nCanary deployment\nDeploy one new copy of the app, wait for it to come up and pass health\nchecks, start sending live traffic to it, and then pause the deployment.\nDuring the pause, compare the new copy of the app, called the “canary,”\nto one of the old copies, called the “control.” You can compare the\ncanary and control across a variety of dimensions: CPU usage, memory\nusage, latency, throughput, error rates in the logs, HTTP response codes,\nand so on. Ideally, there’s no way to tell the two servers apart, which\nshould give you confidence that the new code works just fine. In that\ncase, you unpause the deployment and use one of the rolling\ndeployment strategies to complete it. On the other hand, if you spot any\ndifferences, then that may be a sign of problems in the new code, and\nyou can cancel the deployment and undeploy the canary before the\nproblem becomes worse.\nThe name comes from the “canary in a coal mine” concept, where\nminers would take canary birds with them down into the tunnels, and if\nthe tunnels filled with dangerous gases (e.g., carbon monoxide), those\ngases would affect the canary before the miners, thus providing an early\nwarning to the miners that something was wrong and that they needed\nto exit immediately, before more damage was done. The canary\ndeployment offers similar benefits, giving you a systematic way to test\nnew code in production in a way that, if something goes wrong, you get\na warning early on, when it has affected only a small portion of your\nusers and you still have enough time to react and prevent further\ndamage.' metadata={'original_pages_range': '576-578', 'source': '163_Deployment_strategies', 'file_path': '/mnt/c/Users/Carlos/Documents/Github/jupiter-iaa-azure/Scripts/RAG/../../data/optimized_chunks/Libro-TF/163_Deployment_strategies.pdf', 'num_pages': 3}", "page_content='Canary deployments are often combined with feature toggles, in which\nyou wrap all new features in an if-statement. By default, the if-statement\ndefaults to false, so the new feature is toggled off when you initially\ndeploy the code. Because all new functionality is off, when you deploy\nthe canary server, it should behave identically to the control, and any\ndifferences can be automatically flagged as a problem and trigger a\nrollback. If there were no problems, later on you can enable the feature\ntoggle for a portion of your users via an internal web interface. For\nexample, you might initially enable the new feature only for employees;\nif that works well, you can enable it for 1% of users; if that’s still\nworking well, you can ramp it up to 10%; and so on. If at any point\nthere’s a problem, you can use the feature toggle to ramp the feature\nback down. This process allows you to separate deployment of new\ncode from release of new features.\nDeployment server\nYou should run the deployment from a CI server and not from a developer’s\ncomputer. This has the following benefits:\nFully automated\nTo run deployments from a CI server, you’ll be forced to fully automate\nall deployment steps. This ensures that your deployment process is\ncaptured as code, that you don’t miss any steps accidentally due to\nmanual error, and that the deployment is fast and repeatable.\nConsistent environment\nIf developers run deployments from their own computers, you’ll run\ninto bugs due to differences in how their computer is configured: for\nexample, different operating systems, different dependency versions\n(different versions of Terraform), different configurations, and\ndifferences in what’s actually being deployed (e.g., the developer\naccidentally deploys a change that wasn’t committed to version control).\nYou can eliminate all of these issues by deploying everything from the\nsame CI server.' metadata={'original_pages_range': '579', 'source': '164_Deployment_server', 'file_path': '/mnt/c/Users/Carlos/Documents/Github/jupiter-iaa-azure/Scripts/RAG/../../data/optimized_chunks/Libro-TF/164_Deployment_server.pdf', 'num_pages': 1}", "page_content='Better permissions management\nInstead of giving every developer permissions to deploy, you can give\nsolely the CI server those permissions (especially for the production\nenvironment). It’s a lot easier to enforce good security practices for a\nsingle server than it is to do for numerous developers with production\naccess.\nPromotion across environments\nIf you’re using immutable infrastructure practices, the way to roll out new\nchanges is to promote the exact same versioned artifact from one\nenvironment to another. For example, if you have dev, staging, and\nproduction environments, to roll out v0.0.4 of your app, you would do\nthe following:\n1. Deploy v0.0.4 of the app to dev.\n2. Run your manual and automated tests in dev.\n3. If v0.0.4 works well in dev, repeat steps 1 and 2 to deploy v0.0.4\nto staging (this is known as promoting the artifact).\n4. If v0.0.4 works well in staging, repeat steps 1 and 2 again to\npromote v0.0.4 to prod.\nBecause you’re running the exact same artifact everywhere, there’s a good\nchance that if it works in one environment, it will work in another. And if\nyou do hit any issues, you can roll back anytime by deploying an older\nartifact version.\nA Workflow for Deploying Infrastructure\nCode\nNow that you’ve seen the workflow for deploying application code, it’s\ntime to dive into the workflow for deploying infrastructure code. In this' metadata={'original_pages_range': '580', 'source': '165_Promotion_across_environments_y_1_mas', 'file_path': '/mnt/c/Users/Carlos/Documents/Github/jupiter-iaa-azure/Scripts/RAG/../../data/optimized_chunks/Libro-TF/165_Promotion_across_environments_y_1_mas.pdf', 'num_pages': 1}", "page_content='section, when I say “infrastructure code,” I mean code written with any IaC\ntool (including, of course, Terraform) that you can use to deploy arbitrary\ninfrastructure changes beyond a single application: for example, deploying\ndatabases, load balancers, network configurations, DNS settings, and so on.\nHere’s what the infrastructure code workflow looks like:\n1. Use version control\n2. Run the code locally\n3. Make code changes\n4. Submit changes for review\n5. Run automated tests\n6. Merge and release\n7. Deploy\nOn the surface, it looks identical to the application workflow, but under the\nhood, there are important differences. Deploying infrastructure code\nchanges is more complicated, and the techniques are not as well understood,\nso being able to relate each step back to the analogous step from the\napplication code workflow should make it easier to follow along. Let’s dive\nin.\nUse Version Control\nJust as with your application code, all of your infrastructure code should be\nin version control. This means that you’ll use git clone to check out\nyour code, just as before. However, version control for infrastructure code\nhas a few extra requirements:\nLive repo and modules repo\nGolden Rule of Terraform\nThe trouble with branches' metadata={'original_pages_range': '581', 'source': '166_Use_Version_Control', 'file_path': '/mnt/c/Users/Carlos/Documents/Github/jupiter-iaa-azure/Scripts/RAG/../../data/optimized_chunks/Libro-TF/166_Use_Version_Control.pdf', 'num_pages': 1}", "page_content='Live repo and modules repo\nAs discussed in Chapter 4, you will typically want at least two separate\nversion control repositories for your Terraform code: one repo for modules\nand one repo for live infrastructure. The repository for modules is where\nyou create your reusable, versioned modules, such as all the modules you\nbuilt in the previous chapters of this book (cluster/asg-rolling-\ndeploy, data-stores/mysql, networking/alb, and\nservices/hello-world-app). The repository for live infrastructure\ndefines the live infrastructure you’ve deployed in each environment (dev,\nstage, prod, etc.).\nOne pattern that works well is to have one infrastructure team in your\ncompany that specializes in creating reusable, robust, production-grade\nmodules. This team can create remarkable leverage for your company by\nbuilding a library of modules that implement the ideas from Chapter 8; that\nis, each module has a composable API, is thoroughly documented\n(including executable documentation in the examples folder), has a\ncomprehensive suite of automated tests, is versioned, and implements all of\nyour company’s requirements from the production-grade infrastructure\nchecklist (i.e., security, compliance, scalability, high availability,\nmonitoring, and so on).\nIf you build such a library (or you buy one off the shelf), all the other\nteams at your company will be able to consume these modules, a bit like a\nservice catalog, to deploy and manage their own infrastructure, without (a)\neach team having to spend months assembling that infrastructure from\nscratch or (b) the Ops team becoming a bottleneck because it must deploy\nand manage the infrastructure for every team. Instead, the Ops team can\nspend most of its time writing infrastructure code, and all of the other teams\nwill be able to work independently, using these modules to get themselves\nup and running. And because every team is using the same canonical\nmodules under the hood, as the company grows and requirements change,\nthe Ops team can push out new versions of the modules to all teams,\nensuring everything stays consistent and maintainable.\n3' metadata={'original_pages_range': '582', 'source': '167_Live_repo_and_modules_repo', 'file_path': '/mnt/c/Users/Carlos/Documents/Github/jupiter-iaa-azure/Scripts/RAG/../../data/optimized_chunks/Libro-TF/167_Live_repo_and_modules_repo.pdf', 'num_pages': 1}", "page_content='Or it will be maintainable, as long as you follow the Golden Rule of\nTerraform.\nThe Golden Rule of Terraform\nHere’s a quick way to check the health of your Terraform code: go into your\nlive repository, pick several folders at random, and run terraform plan\nin each one. If the output is always “no changes,” that’s great, because it\nmeans that your infrastructure code matches what’s actually deployed. If the\noutput sometimes shows a small diff, and you hear the occasional excuse\nfrom your team members (“Oh, right, I tweaked that one thing by hand and\nforgot to update the code”), your code doesn’t match reality, and you might\nsoon be in trouble. If terraform plan fails completely with weird\nerrors, or every plan shows a gigantic diff, your Terraform code has no\nrelation at all to reality and is likely useless.\nThe gold standard, or what you’re really aiming for, is what I call The\nGolden Rule of Terraform:\nThe main branch of the live repository should be a 1:1 representation of\nwhat’s actually deployed in production.\nLet’s break this sentence down, starting at the end and working our way\nback:\n“…what’s actually deployed”\nThe only way to ensure that the Terraform code in the live repository is\nan up-to-date representation of what’s actually deployed is to never\nmake out-of-band changes. After you begin using Terraform, do not\nmake changes via a web UI, or manual API calls, or any other\nmechanism. As you saw in Chapter 5, out-of-band changes not only\nlead to complicated bugs, but they also void many of the benefits you\nget from using IaC in the first place.\n“…a 1:1 representation…”' metadata={'original_pages_range': '583', 'source': '168_The_Golden_Rule_of_Terraform', 'file_path': '/mnt/c/Users/Carlos/Documents/Github/jupiter-iaa-azure/Scripts/RAG/../../data/optimized_chunks/Libro-TF/168_The_Golden_Rule_of_Terraform.pdf', 'num_pages': 1}", "page_content='If I browse your live repository, I should be able to see, from a quick\nscan, what resources have been deployed in what environments. That is,\nevery resource should have a 1:1 match with some line of code checked\ninto the live repo. This seems obvious at first glance, but it’s\nsurprisingly easy to get it wrong. One way to get it wrong, as I just\nmentioned, is to make out-of-band changes so that the code is there, but\nthe live infrastructure is different. A more subtle way to get it wrong is\nto use Terraform workspaces to manage environments so that the live\ninfrastructure is there, but the code isn’t. That is, if you use workspaces,\nyour live repo will have only one copy of the code, even though you\nmay have 3 or 30 environments deployed with it. From merely looking\nat the code, there will be no way to know what’s actually deployed,\nwhich will lead to mistakes and make maintenance complicated.\nTherefore, as described in “Isolation via Workspaces”, instead of using\nworkspaces to manage environments, you want each environment\ndefined in a separate folder, using separate files, so that you can see\nexactly what environments have been deployed just by browsing the\nlive repository. Later in this chapter, you’ll see how to do this with\nminimal copying and pasting.\n“The main branch…”\nYou should have to look at only a single branch to understand what’s\nactually deployed in production. Typically, that branch will be main.\nThis means that all changes that affect the production environment\nshould go directly into main (you can create a separate branch but only\nto create a pull request with the intention of merging that branch into\nmain), and you should run terraform apply only for the\nproduction environment against the main branch. In the next section,\nI’ll explain why.\nThe trouble with branches\nIn Chapter 3, you saw that you can use the locking mechanisms built into\nTerraform backends to ensure that if two team members are running\n\nterraform apply at the same time on the same set of Terraform\nconfigurations, their changes do not overwrite each other. Unfortunately,\nthis only solves part of the problem. Even though Terraform backends\nprovide locking for Terraform state, they cannot help you with locking at\nthe level of the Terraform code itself. In particular, if two team members are\ndeploying the same code to the same environment but from different\nbranches, you’ll run into conflicts that locking can’t prevent.\nFor example, suppose that one of your team members, Anna, makes some\nchanges to the Terraform configurations for an app called “foo” that\nconsists of a single EC2 Instance:\nresource \"aws_instance\" \"foo\" {\n  ami           = data.aws_ami.ubuntu.id\n  instance_type = \"t2.micro\"\n}\nThe app is getting a lot of traffic, so Anna decides to change the\ninstance_type from t2.micro to t2.medium:\nresource \"aws_instance\" \"foo\" {\n  ami           = data.aws_ami.ubuntu.id\n  instance_type = \"t2.medium\"\n}\nHere’s what Anna sees when she runs terraform plan:\n$ terraform plan \n \n(...) \n \nTerraform will perform the following actions: \n \n  # aws_instance.foo will be updated in-place \n  ~ resource \"aws_instance\" \"foo\" { \n        ami                          = \"ami-0fb653ca2d3203ac1\" \n        id                           = \"i-096430d595c80cb53\" \n        instance_state               = \"running\" \n      ~ instance_type                = \"t2.micro\" -> \"t2.medium\" \n        (...) \n    }\n\nPlan: 0 to add, 1 to change, 0 to destroy.\nThose changes look good, so she deploys them to staging.\nIn the meantime, Bill comes along and also starts making changes to the\nTerraform configurations for the same app but on a different branch. All\nBill wants to do is to add a tag to the app:\nresource \"aws_instance\" \"foo\" {\n  ami           = data.aws_ami.ubuntu.id\n  instance_type = \"t2.micro\" \n \n  tags = {\n    Name = \"foo\" \n  }\n}\nNote that Anna’s changes are already deployed in staging, but because they\nare on a different branch, Bill’s code still has the instance_type set to\nthe old value of t2.micro. Here’s what Bill sees when he runs the plan\ncommand (the following log output is truncated for readability):\n$ terraform plan \n \n(...) \n \nTerraform will perform the following actions: \n \n  # aws_instance.foo will be updated in-place \n  ~ resource \"aws_instance\" \"foo\" { \n        ami                          = \"ami-0fb653ca2d3203ac1\" \n        id                           = \"i-096430d595c80cb53\" \n        instance_state               = \"running\" \n      ~ instance_type                = \"t2.medium\" -> \"t2.micro\" \n      + tags                         = { \n          + \"Name\" = \"foo\" \n        } \n        (...) \n    } \n \nPlan: 0 to add, 1 to change, 0 to destroy.' metadata={'original_pages_range': '584-586', 'source': '169_The_trouble_with_branches', 'file_path': '/mnt/c/Users/Carlos/Documents/Github/jupiter-iaa-azure/Scripts/RAG/../../data/optimized_chunks/Libro-TF/169_The_trouble_with_branches.pdf', 'num_pages': 3}", "page_content='Uh oh, he’s about to undo Anna’s instance_type change! If Anna is\nstill testing in staging, she’ll be very confused when the server suddenly\nredeploys and starts behaving differently. The good news is that if Bill\ndiligently reads the plan output, he can spot the error before it affects\nAnna. Nevertheless, the point of the example is to highlight what happens\nwhen you deploy changes to a shared environment from different branches.\nThe locking from Terraform backends doesn’t help here, because the\nconflict has nothing to do with concurrent modifications to the state file;\nBill and Anna might be applying their changes weeks apart, and the\nproblem would be the same. The underlying cause is that branching and\nTerraform are a bad combination. Terraform is implicitly a mapping from\nTerraform code to infrastructure deployed in the real world. Because there’s\nonly one real world, it doesn’t make much sense to have multiple branches\nof your Terraform code. So for any shared environment (e.g., stage, prod),\nalways deploy from a single branch.\nRun the Code Locally\nNow that you’ve got the code checked out onto your computer, the next\nstep is to run it. The gotcha with Terraform is that, unlike application code,\nyou don’t have “localhost”; for example, you can’t deploy an AWS ASG\nonto your own laptop. As discussed in “Manual Testing Basics”, the only\nway to manually test Terraform code is to run it in a sandbox environment,\nsuch as an AWS account dedicated for developers (or better yet, one AWS\naccount for each developer).\nOnce you have a sandbox environment, to test manually, you run\nterraform apply:\n$ terraform apply \n \n(...) \n \nApply complete! Resources: 5 added, 0 changed, 0 destroyed. \n \nOutputs:' metadata={'original_pages_range': '587', 'source': '170_Run_the_Code_Locally', 'file_path': '/mnt/c/Users/Carlos/Documents/Github/jupiter-iaa-azure/Scripts/RAG/../../data/optimized_chunks/Libro-TF/170_Run_the_Code_Locally.pdf', 'num_pages': 1}", "page_content='alb_dns_name = \"hello-world-stage-477699288.us-east-\n2.elb.amazonaws.com\"\nAnd you verify the deployed infrastructure works by using tools such as\ncurl:\n$ curl hello-world-stage-477699288.us-east-2.elb.amazonaws.com \nHello, World\nTo run automated tests written in Go, you use go test in a sandbox\naccount dedicated to testing:\n$ go test -v -timeout 30m \n \n(...) \n \nPASS \nok   terraform-up-and-running 229.492s\nMake Code Changes\nNow that you can run your Terraform code, you can iteratively begin to\nmake changes, just as with application code. Every time you make a\nchange, you can rerun terraform apply to deploy those changes and\nrerun curl to see whether those changes worked:\n$ curl hello-world-stage-477699288.us-east-2.elb.amazonaws.com \nHello, World v2\nOr you can rerun go test to make sure the tests are still passing:\n$ go test -v -timeout 30m \n \n(...) \n \nPASS \nok   terraform-up-and-running 229.492s' metadata={'original_pages_range': '588', 'source': '171_Make_Code_Changes', 'file_path': '/mnt/c/Users/Carlos/Documents/Github/jupiter-iaa-azure/Scripts/RAG/../../data/optimized_chunks/Libro-TF/171_Make_Code_Changes.pdf', 'num_pages': 1}", "page_content='The only difference from application code is that infrastructure code tests\ntypically take longer, so you’ll want to put more thought into how you can\nshorten the test cycle so that you can get feedback on your changes as\nquickly as possible. In “Test stages”, you saw that you can use these test\nstages to rerun only specific stages of a test suite, dramatically shortening\nthe feedback loop.\nAs you make changes, be sure to regularly commit your work:\n$ git commit -m \"Updated Hello, World text\"\nSubmit Changes for Review\nAfter your code is working the way you expect, you can create a pull\nrequest to get your code reviewed, just as you would with application code.\nYour team will review your code changes, looking for bugs as well as\nenforcing coding guidelines. Whenever you’re writing code as a team,\nregardless of what type of code you’re writing, you should define\nguidelines for everyone to follow. One of my favorite definitions of “clean\ncode” comes from an interview I did with Nick Dellamaggiore for my\nearlier book, Hello, Startup:\nIf I look at a single file and it’s written by 10 different engineers, it should\nbe almost indistinguishable which part was written by which person. To\nme, that is clean code.\nThe way you do that is through code reviews and publishing your style\nguide, your patterns, and your language idioms. Once you learn them,\neverybody is way more productive because you all know how to write\ncode the same way. At that point, it’s more about what you’re writing and\nnot how you write it.\n—Nick Dellamaggiore, Infrastructure Lead at Coursera\nThe Terraform coding guidelines that make sense for each team will be\ndifferent, so here, I’ll list a few of the common ones that are useful for most\nteams:' metadata={'original_pages_range': '589', 'source': '172_Submit_Changes_for_Review', 'file_path': '/mnt/c/Users/Carlos/Documents/Github/jupiter-iaa-azure/Scripts/RAG/../../data/optimized_chunks/Libro-TF/172_Submit_Changes_for_Review.pdf', 'num_pages': 1}", "page_content='Documentation\nAutomated tests\nFile layout\nStyle guide\nDocumentation\nIn some sense, Terraform code is, in and of itself, a form of documentation.\nIt describes in a simple language exactly what infrastructure you deployed\nand how that infrastructure is configured. However, there is no such thing\nas self-documenting code. Although well-written code can tell you what it\ndoes, no programming language that I’m aware of (including Terraform)\ncan tell you why it does it.\nThis is why all software, including IaC, needs documentation beyond the\ncode itself. There are several types of documentation that you can consider\nand have your team members require as part of code reviews:\nWritten documentation\nMost Terraform modules should have a README that explains what\nthe module does, why it exists, how to use it, and how to modify it. In\nfact, you may want to write the README first, before any of the actual\nTerraform code, because that will force you to consider what you’re\nbuilding and why you’re building it before you dive into the code and\nget lost in the details of how to build it. Spending 20 minutes writing a\nREADME can often save you hours of writing code that solves the\nwrong problem. Beyond the basic README, you might also want to\nhave tutorials, API documentation, wiki pages, and design documents\nthat go deeper into how the code works and why it was built this way.\nCode documentation\nWithin the code itself, you can use comments as a form of\ndocumentation. Terraform treats any text that begins with a hash (#) as a\ncomment. Don’t use comments to explain what the code does; the code\n4' metadata={'original_pages_range': '590', 'source': '173_Documentation', 'file_path': '/mnt/c/Users/Carlos/Documents/Github/jupiter-iaa-azure/Scripts/RAG/../../data/optimized_chunks/Libro-TF/173_Documentation.pdf', 'num_pages': 1}", "page_content='should do that itself. Only include comments to offer information that\ncan’t be expressed in code, such as how the code is meant to be used or\nwhy the code uses a particular design choice. Terraform also allows\nevery input and output variable to declare a description parameter,\nwhich is a great place to describe how that variable should be used.\nExample code\nAs discussed in Chapter 8, every Terraform module should include\nexample code that shows how that module is meant to be used. This is a\ngreat way to highlight the intended usage patterns and give your users a\nway to try your module without having to write any code, and it’s the\nmain way to add automated tests for the module.\nAutomated tests\nAll of Chapter 9 focuses on testing Terraform code, so I won’t repeat any of\nthat here, other than to say that infrastructure code without tests is broken.\nTherefore, one of the most important comments you can make in any code\nreview is “How did you test this?”\nFile layout\nYour team should define conventions for where Terraform code is stored\nand the file layout you use. Because the file layout for Terraform also\ndetermines the way Terraform state is stored, you should be especially\nmindful of how file layout affects your ability to provide isolation\nguarantees, such as ensuring that changes in a staging environment cannot\naccidentally cause problems in production. In a code review, you might\nwant to enforce the file layout described in “Isolation via File Layout”,\nwhich provides isolation between different environments (e.g., stage and\nprod) and different components (e.g., a network topology for the entire\nenvironment and a single app within that environment).\nStyle guide' metadata={'original_pages_range': '591', 'source': '174_Automated_tests_y_2_mas', 'file_path': '/mnt/c/Users/Carlos/Documents/Github/jupiter-iaa-azure/Scripts/RAG/../../data/optimized_chunks/Libro-TF/174_Automated_tests_y_2_mas.pdf', 'num_pages': 1}", "page_content='Every team should enforce a set of conventions about code style, including\nthe use of whitespace, newlines, indentation, curly braces, variable naming,\nand so on. Although programmers love to debate spaces versus tabs and\nwhere the curly brace should go, the more important thing is that you are\nconsistent throughout your codebase.\nTerraform has a built-in fmt command that can reformat code to a\nconsistent style automatically:\n$ terraform fmt\nI recommend running this command as part of a commit hook to ensure that\nall code committed to version control uses a consistent style.\nRun Automated Tests\nJust as with application code, your infrastructure code should have commit\nhooks that kick off automated tests in a CI server after every commit and\nshow the results of those tests in the pull request. You already saw how to\nwrite unit tests, integration tests, and end-to-end tests for your Terraform\ncode in Chapter 9. There’s one other critical type of test you should run:\nterraform plan. The rule here is simple:\nAlways run plan before apply.\nTerraform shows the plan output automatically when you run apply, so\nwhat this rule really means is that you should always pause and read the\nplan output! You’d be amazed at the type of errors you can catch by\ntaking 30 seconds to scan the “diff” you get as an output. A great way to\nencourage this behavior is by integrating plan into your code review flow.\nFor example, Atlantis is an open source tool that automatically runs\nterraform plan on commits and adds the plan output to pull requests\nas a comment, as shown in Figure 10-3.\n\n' metadata={'original_pages_range': '592-593', 'source': '175_Run_Automated_Tests', 'file_path': '/mnt/c/Users/Carlos/Documents/Github/jupiter-iaa-azure/Scripts/RAG/../../data/optimized_chunks/Libro-TF/175_Run_Automated_Tests.pdf', 'num_pages': 2}", "page_content='Figure 10-3. Atlantis can automatically add the output of the terraform plan command as a\ncomment on your pull requests.\nTerraform Cloud and Terraform Enterprise, HashiCorp’s paid tools, both\nsupport running plan automatically on pull requests as well.\nMerge and Release\nAfter your team members have had a chance to review the code changes\nand plan output and all the tests have passed, you can merge your changes\ninto the main branch and release the code. Similar to application code, you\ncan use Git tags to create a versioned release:\n$ git tag -a \"v0.0.6\" -m \"Updated hello-world-example text\" \n$ git push --follow-tags\nWhereas with application code, you often have a separate artifact to deploy,\nsuch as a Docker image or VM image, since Terraform natively supports\ndownloading code from Git, the repository at a specific tag is the\nimmutable, versioned artifact you will be deploying.\nDeploy\nNow that you have an immutable, versioned artifact, it’s time to deploy it.\nHere are a few of the key considerations for deploying Terraform code:\nDeployment tooling\nDeployment strategies\nDeployment server\nPromote artifacts across environments\nDeployment tooling\nWhen deploying Terraform code, Terraform itself is the main tool that you\nuse. However, there are a few other tools that you might find useful:' metadata={'original_pages_range': '594', 'source': '176_Merge_and_Release_y_2_mas', 'file_path': '/mnt/c/Users/Carlos/Documents/Github/jupiter-iaa-azure/Scripts/RAG/../../data/optimized_chunks/Libro-TF/176_Merge_and_Release_y_2_mas.pdf', 'num_pages': 1}", "page_content='Atlantis\nThe open source tool you saw earlier can not only add the plan output\nto your pull requests but also allows you to trigger a terraform\napply when you add a special comment to your pull request. Although\nthis provides a convenient web interface for Terraform deployments, be\naware that it doesn’t support versioning, which can make maintenance\nand debugging for larger projects more difficult.\nTerraform Cloud and Terraform Enterprise\nHashiCorp’s paid products provide a web UI that you can use to run\nterraform plan and terraform apply as well as manage\nvariables, secrets, and access permissions.\nTerragrunt\nThis is an open source wrapper for Terraform that fills in some gaps in\nTerraform. You’ll see how to use it a bit later in this chapter to deploy\nversioned Terraform code across multiple environments with minimal\ncopying and pasting.\nScripts\nAs always, you can write scripts in a general-purpose programming\nlanguage such as Python or Ruby or Bash to customize how you use\nTerraform.\nDeployment strategies\nFor most types of infrastructure changes, Terraform doesn’t offer any built-\nin deployment strategies: for example, there’s no way to do a blue-green\ndeployment for a VPC change, and there’s no way to feature toggle a\ndatabase change. You’re essentially limited to terraform apply, which\neither works or it doesn’t. A small subset of changes do support deployment\nstrategies, such as the zero-downtime rolling deployment in the asg-\n\nrolling-deploy module you built in previous chapters, but these are\nthe exceptions and not the norm.\nDue to these limitations, it’s critical to take into account what happens when\na deployment goes wrong. With an application deployment, many types of\nerrors are caught by the deployment strategy; for example, if the app fails to\npass health checks, the load balancer will never send it live traffic, so users\nwon’t be affected. Moreover, the rolling deployment or blue-green\ndeployment strategy can automatically roll back to the previous version of\nthe app in case of errors.\nTerraform, on the other hand, does not roll back automatically in case of\nerrors. In part, that’s because there is no reasonable way to roll back many\ntypes of infrastructure changes: for example, if an app deployment failed,\nit’s almost always safe to roll back to an older version of the app, but if the\nTerraform change you were deploying failed, and that change was to delete\na database or terminate a server, you can’t easily roll that back!\nTherefore, you should expect errors to happen and ensure you have a first-\nclass way to deal with them:\nRetries\nCertain types of Terraform errors are transient and go away if you rerun\nterraform apply. The deployment tooling you use with Terraform\nshould detect these known errors and automatically retry after a brief\npause. Terragrunt has automatic retries on known errors as a built-in\nfeature.\nTerraform state errors\nOccasionally, Terraform will fail to save state after running\nterraform apply. For example, if you lose internet connectivity\npartway through an apply, not only will the apply fail, but Terraform\nwon’t be able to write the updated state file to your remote backend\n(e.g., to Amazon S3). In these cases, Terraform will save the state file\non disk in a file called errored.tfstate. Make sure that your CI server\ndoes not delete these files (e.g., as part of cleaning up the workspace' metadata={'original_pages_range': '595-596', 'source': '177_Deployment_strategies', 'file_path': '/mnt/c/Users/Carlos/Documents/Github/jupiter-iaa-azure/Scripts/RAG/../../data/optimized_chunks/Libro-TF/177_Deployment_strategies.pdf', 'num_pages': 2}", "page_content='after a build)! If you can still access this file after a failed deployment,\nas soon as internet connectivity is restored, you can push this file to\nyour remote backend (e.g., to S3) using the state push command so\nthat the state information isn’t lost:\n$ terraform state push errored.tfstate\nErrors releasing locks\nOccasionally, Terraform will fail to release a lock. For example, if your\nCI server crashes in the middle of a terraform apply, the state\nwill remain permanently locked. Anyone else who tries to run apply\non the same module will get an error message saying the state is locked\nand showing the ID of the lock. If you’re absolutely sure this is an\naccidentally leftover lock, you can forcibly release it using the force-\nunlock command, passing it the ID of the lock from that error\nmessage:\n$ terraform force-unlock <LOCK_ID>\nDeployment server\nJust as with your application code, all of your infrastructure code changes\nshould be applied from a CI server and not from a developer’s computer.\nYou can run terraform from Jenkins, CircleCI, GitHub Actions,\nTerraform Cloud, Terraform Enterprise, Atlantis, or any other reasonably\nsecure automated platform. This gives you the same benefits as with\napplication code: it forces you to fully automate your deployment process,\nit ensures deployment always happens from a consistent environment, and it\ngives you better control over who has permissions to access production\nenvironments.\nThat said, permissions to deploy infrastructure code are quite a bit trickier\nthan for application code. With application code, you can usually give your\n\nCI server a minimal, fixed set of permissions to deploy your apps; for\nexample, to deploy to an ASG, the CI server typically needs only a few\nspecific ec2 and autoscaling permissions. However, to be able to\ndeploy arbitrary infrastructure code changes (e.g., your Terraform code\nmight try to deploy a database or a VPC or an entirely new AWS account),\nthe CI server needs arbitrary permissions—that is, admin permissions. And\nthat’s a problem.\nThe reason it’s a problem is that CI servers are (a) notoriously hard to\nsecure, (b) accessible to all the developers at your company, and (c) used\nto execute arbitrary code. Adding permanent admin permissions to this mix\nis just asking for trouble! You’d effectively be giving every single person\non your team admin permissions and turning your CI server into a very\nhigh-value target for attackers.\nThere are a few things you can do to minimize this risk:\nLock the CI server down\nMake it accessible solely over HTTPs, require all users to be\nauthenticated, and follow server-hardening practices (e.g., lock down\nthe firewall, install fail2ban, enable audit logging, etc.).\nDon’t expose your CI server on the public internet\nThat is, run the CI server in private subnets, without any public IP, so\nthat it’s accessible only over a VPN connection. That way, only users\nwith valid network access (e.g., via a VPN certificate) can access your\nCI server at all. Note that this does have a drawback: webhooks from\nexternal systems won’t work. For example, GitHub won’t automatically\nbe able to trigger builds in your CI server; instead, you’ll need to\nconfigure your CI server to poll your version control system for\nupdates. This is a small price to pay for a significantly more secure CI\nserver.\nEnforce an approval workflow\n5' metadata={'original_pages_range': '597-598', 'source': '178_Deployment_server', 'file_path': '/mnt/c/Users/Carlos/Documents/Github/jupiter-iaa-azure/Scripts/RAG/../../data/optimized_chunks/Libro-TF/178_Deployment_server.pdf', 'num_pages': 2}", "page_content='Configure your CI/CD pipeline to require that every deployment be\napproved by at least one person (other than the person who requested\nthe deployment in the first place). During this approval step, the\nreviewer should be able to see both the code changes and the plan\noutput, as one final check that things look OK before apply runs. This\nensures that every deployment, code change, and plan output has had\nat least two sets of eyes on it.\nDon’t give the CI server permanent credentials\nAs you saw in Chapter 6, instead of manually managed, permanent\ncredentials (e.g., AWS access keys copy/pasted into your CI server),\nyou should prefer to use authentication mechanisms that use temporary\ncredentials, such as IAM roles and OIDC.\nDon’t give the CI server admin credentials\nInstead, isolate the admin credentials to a totally separate, isolated\nworker: e.g., a separate server, a separate container, etc. That worker\nshould be extremely locked down, so no developers have access to it at\nall, and the only thing it allows is for the CI server to trigger that worker\nvia an extremely limited remote API. For example, that worker’s API\nmay only allow you to run specific commands (e.g., terraform\nplan and terraform apply), in specific repos (e.g., your live\nrepo), in specific branches (e.g., the main branch), and so on. This way,\neven if an attacker gets access to your CI server, they still won’t have\naccess to the admin credentials, and all they can do is request a\ndeployment on some code that’s already in your version control system,\nwhich isn’t nearly as much of a catastrophe as leaking the admin\ncredentials fully.\nPromote artifacts across environments\nJust as with application artifacts, you’ll want to promote your immutable,\nversioned infrastructure artifacts from environment to environment: for\n6\n\nexample, promote v0.0.6 from dev to stage to prod. The rule here is also\nsimple:\nAlways test Terraform changes in pre-prod before prod.\nBecause everything is automated with Terraform anyway, it doesn’t cost\nyou much extra effort to try a change in staging before production, but it\nwill catch a huge number of errors. Testing in pre-prod is especially\nimportant because, as mentioned earlier in this chapter, Terraform does not\nroll back changes in case of errors. If you run terraform apply and\nsomething goes wrong, you must fix it yourself. This is easier and less\nstressful to do if you catch the error in a pre-prod environment rather than\nprod.\nThe process for promoting Terraform code across environments is similar to\nthe process of promoting application artifacts, except there is an extra\napproval step, as mentioned in the previous section, where you run\nterraform plan and have someone manually review the output and\napprove the deployment. This step isn’t usually necessary for application\ndeployments, as most application deployments are similar and relatively\nlow risk. However, every infrastructure deployment can be completely\ndifferent, and mistakes can be very costly (e.g., deleting a database), so\nhaving one last chance to look at the plan output and review it is well\nworth the time.\nHere’s what the process looks like for promoting, for instance, v0.0.6 of\na Terraform module across the dev, stage, and prod environments:\n1. Update the dev environment to v0.0.6, and run terraform\nplan.\n2. Prompt someone to review and approve the plan; for example, send an\nautomated message via Slack.\n3. If the plan is approved, deploy v0.0.6 to dev by running\nterraform apply.\n4. Run your manual and automated tests in dev.\n7\n\n5. If v0.0.6 works well in dev, repeat steps 1–4 to promote v0.0.6 to\nstaging.\n6. If v0.0.6 works well in staging, repeat steps 1–4 again to promote\nv0.0.6 to production.\nOne important issue to deal with is all the code duplication between\nenvironments in the live repo. For example, consider the live repo shown in\nFigure 10-4.\n\n\n\nFigure 10-4. File layout with a large number of copy/pasted environments and modules within each\nenvironment.\nThis live repo has a large number of regions, and within each region, a large\nnumber of modules, most of which are copied and pasted. Sure, each\nmodule has a main.tf that references a module in your modules repo, so it’s\nnot as much copying and pasting as it could be, but even if all you’re doing\nis instantiating a single module, there is still a large amount of boilerplate\nthat needs to be duplicated between each environment:\nThe provider configuration\nThe backend configuration\nThe input variables to pass to the module\nThe output variables to proxy from the module\nThis can add up to dozens or hundreds of lines of mostly identical code in\neach module, copied and pasted into each environment. To make this code\nmore DRY, and to make it easier to promote Terraform code across\nenvironments, you can use the open source tool I’ve mentioned earlier\ncalled Terragrunt. Terragrunt is a thin wrapper for Terraform, which means\nthat you run all of the standard terraform commands, except you use\nterragrunt as the binary:\n$ terragrunt plan \n$ terragrunt apply \n$ terragrunt output\nTerragrunt will run Terraform with the command you specify, but based on\nconfiguration you specify in a terragrunt.hcl file, you can get some extra\nbehavior. In particular, Terragrunt allows you to define all of your\nTerraform code exactly once in the modules repo, whereas in the live repo,\nyou will have solely terragrunt.hcl files that provide a DRY way to\nconfigure and deploy each module in each environment. This will result in a\nlive repo with far fewer files and lines of code, as shown in Figure 10-5.\n\nTo get started, install Terragrunt by following the instructions on the\nTerragrunt website. Next, add a provider configuration to modules/data-\nstores/mysql/main.tf and modules/services/hello-world-app/main.tf:\nprovider \"aws\" {\n  region = \"us-east-2\"\n}\n\n\n\nFigure 10-5. Use Terragrunt in your live repos to reduce the amount of code duplication.\nCommit these changes and release a new version of your modules repo:\n$ git add modules/data-stores/mysql/main.tf \n$ git add modules/services/hello-world-app/main.tf \n$ git commit -m \"Update mysql and hello-world-app for Terragrunt\" \n$ git tag -a \"v0.0.7\" -m \"Update Hello, World text\" \n$ git push --follow-tags\nNow, head over to the live repo, and delete all the .tf files. You’re going to\nreplace all that copied and pasted Terraform code with a single\nterragrunt.hcl file for each module. For example, here’s terragrunt.hcl for\nlive/stage/data-stores/mysql/terragrunt.hcl:\nterraform {\n  source = \"github.com/<OWNER>/modules//data-stores/mysql?\nref=v0.0.7\"\n} \n \ninputs = {\n  db_name = \"example_stage\" \n \n  # Set the username using the TF_VAR_db_username environment \nvariable\n  # Set the password using the TF_VAR_db_password environment \nvariable\n}\nAs you can see, terragrunt.hcl files use the same HashiCorp Configuration\nLanguage (HCL) syntax as Terraform itself. When you run terragrunt\napply and it finds the source parameter in a terragrunt.hcl file,\nTerragrunt will do the following:\n1. Check out the URL specified in source to a temporary folder. This\nsupports the same URL syntax as the source parameter of Terraform\nmodules, so you can use local file paths, Git URLs, versioned Git\nURLs (with a ref parameter, as in the preceding example), and so on.\n\n2. Run terraform apply in the temporary folder, passing it the input\nvariables that you’ve specified in the inputs = { … } block.\nThe benefit of this approach is that the code in the live repo is reduced to\njust a single terragrunt.hcl file per module, which contains only a pointer to\nthe module to use (at a specific version), plus the input variables to set for\nthat specific environment. That’s about as DRY as you can get.\nTerragrunt also helps you keep your backend configuration DRY. Instead\nof having to define the bucket, key, dynamodb_table, and so on in\nevery single module, you can define it in a single terragrunt.hcl file per\nenvironment. For example, create the following in live/stage/terragrunt.hcl:\nremote_state {\n  backend = \"s3\" \n \n  generate = {\n    path      = \"backend.tf\"\n    if_exists = \"overwrite\" \n  } \n \n  config = {\n    bucket         = \"<YOUR BUCKET>\"\n    key            = \n\"${path_relative_to_include()}/terraform.tfstate\"\n    region         = \"us-east-2\"\n    encrypt        = true\n    dynamodb_table = \"<YOUR_TABLE>\" \n  }\n}\nFrom this one remote_state block, Terragrunt can generate the\nbackend configuration dynamically for each of your modules, writing the\nconfiguration in config to the file specified via the generate param.\nNote that the key value in config uses a Terragrunt built-in function\ncalled path_relative_to_include(), which will return the relative\npath between this root terragrunt.hcl file and any child module that\nincludes it. For example, to include this root file in live/stage/data-\nstores/mysql/terragrunt.hcl, add an include block:\n\nterraform {\n  source = \"github.com/<OWNER>/modules//data-stores/mysql?\nref=v0.0.7\"\n} \n \ninclude {\n  path = find_in_parent_folders()\n} \n \ninputs = {\n  db_name = \"example_stage\" \n \n  # Set the username using the TF_VAR_db_username environment \nvariable\n  # Set the password using the TF_VAR_db_password environment \nvariable\n}\nThe include block finds the root terragrunt.hcl using the Terragrunt\nbuilt-in function find_in_parent_folders(), automatically\ninheriting all the settings from that parent file, including the\nremote_state configuration. The result is that this mysql module will\nuse all the same backend settings as the root file, and the key value will\nautomatically resolve to data-stores/mysql/terraform.tfstate. This means\nthat your Terraform state will be stored in the same folder structure as your\nlive repo, which will make it easy to know which module produced which\nstate files.\nTo deploy this module, run terragrunt apply:\n$ terragrunt apply --terragrunt-log-level debug \nDEBU[0001] Reading Terragrunt config file at terragrunt.hcl \nDEBU[0001] Included config live/stage/terragrunt.hcl \nDEBU[0001] Downloading Terraform configurations into .terragrunt-\ncache \nDEBU[0001] Generated file backend.tf \nDEBU[0013] Running command: terraform init \n \n(...) \n \nInitializing the backend... \n \nSuccessfully configured the backend \"s3\"! Terraform will\n\nautomatically \nuse this backend unless the backend configuration changes. \n \n(...) \n \nDEBU[0024] Running command: terraform apply \n \n(...) \n \nTerraform will perform the following actions: \n \n(...) \n \nPlan: 5 to add, 0 to change, 0 to destroy. \n \nDo you want to perform these actions? \n  Terraform will perform the actions described above. \n  Only 'yes' will be accepted to approve. \n \n  Enter a value: yes \n \n(...) \n \nApply complete! Resources: 5 added, 0 changed, 0 destroyed.\nNormally, Terragrunt only shows the log output from Terraform itself, but\nas I included --terragrunt-log-level debug, the preceding\noutput shows what Terragrunt does under the hood:\n1. Read the terragrunt.hcl file in the mysql folder where you ran apply.\n2. Pull in all the settings from the included root terragrunt.hcl file.\n3. Download the Terraform code specified in the source URL into the\n.terragrunt-cache scratch folder.\n4. Generate a backend.tf file with your backend configuration.\n5. Detect that init has not been run and run it automatically (Terragrunt\nwill even create your S3 bucket and DynamoDB table automatically if\nthey don’t already exist).\n6. Run apply to deploy changes.\n\nNot bad for a couple of tiny terragrunt.hcl files!\nYou can now deploy the hello-world-app module in staging by\nadding live/stage/services/hello-world-app/terragrunt.hcl and running\nterragrunt apply:\nterraform {\n  source = \"github.com/<OWNER>/modules//services/hello-world-app?\nref=v0.0.7\"\n} \n \ninclude {\n  path = find_in_parent_folders()\n} \n \ndependency \"mysql\" {\n  config_path = \"../../data-stores/mysql\"\n} \n \ninputs = {\n  environment = \"stage\"\n  ami         = \"ami-0fb653ca2d3203ac1\" \n \n  min_size = 2\n  max_size = 2 \n \n  enable_autoscaling = false \n \n  mysql_config = dependency.mysql.outputs\n}\nThis terragrunt.hcl file uses the source URL and inputs just as you\nsaw before and uses include to pull in the settings from the root\nterragrunt.hcl file, so it will inherit the same backend settings, except for\nthe key, which will be automatically set to services/hello-world-\napp/terraform.tfstate, just as you’d expect. The one new thing in this\nterragrunt.hcl file is the dependency block:\ndependency \"mysql\" {\n  config_path = \"../../data-stores/mysql\"\n}' metadata={'original_pages_range': '599-610', 'source': '179_Promote_artifacts_across_environments', 'file_path': '/mnt/c/Users/Carlos/Documents/Github/jupiter-iaa-azure/Scripts/RAG/../../data/optimized_chunks/Libro-TF/179_Promote_artifacts_across_environments.pdf', 'num_pages': 12}", "page_content='This is a Terragrunt feature that can be used to automatically read the output\nvariables of another Terragrunt module, so you can pass them as input\nvariables to the current module, as follows:\n  mysql_config = dependency.mysql.outputs\nIn other words, dependency blocks are an alternative to using\nterraform_remote_state data sources to pass data between\nmodules. While terraform_remote_state data sources have the\nadvantage of being native to Terraform, the drawback is that they make\nyour modules more tightly coupled together, as each module needs to know\nhow other modules store state. Using Terragrunt dependency blocks\nallows your modules to expose generic inputs like mysql_config and\nvpc_id, instead of using data sources, which makes the modules less\ntightly coupled and easier to test and reuse.\nOnce you’ve got hello-world-app working in staging, create\nanalogous terragrunt.hcl files in live/prod and promote the exact same\nv0.0.7 artifact to production by running terragrunt apply in each\nmodule.\nPutting It All Together\nYou’ve now seen how to take both application code and infrastructure code\nfrom development all the way through to production. Table 10-1 shows an\noverview of the two workflows side by side.\n\nTable 10-1. Application and infrastructure code workflows\nApplication code Infrastructure code\nUse version control git clone\nOne repo per app\nUse branches\ngit clone\nlive and modules repos\nDon’t use branches\nRun the code\nlocally Run on localhost\nruby web-serv\ner.rb\nruby web-serv\ner-test.rb\nRun in a sandbox environment\nterraform apply\ngo test\nMake code changes Change the code\nruby web-serv\ner.rb\nruby web-serv\ner-test.rb\nChange the code\nterraform apply\ngo test\nUse test stages\nSubmit changes for\nreview Submit a pull\nrequest\nEnforce coding\nguidelines\nSubmit a pull request\nEnforce coding guidelines\nRun automated\ntests Tests run on CI\nserver\nUnit tests\nIntegration tests\nEnd-to-end tests\nStatic analysis\nTests run on CI server\nUnit tests\nIntegration tests\nEnd-to-end tests\nStatic analysis\nterraform plan\n\nApplication code Infrastructure code\nMerge and release git tag\nCreate versioned,\nimmutable artifact\ngit tag\nUse repo with tag as versioned, immutable\nartifact\nDeploy Deploy with\nTerraform,\norchestration tool\n(e.g., Kubernetes,\nMesos), scripts\nMany deployment\nstrategies: rolling\ndeployment, blue-\ngreen, canary\nRun deployment on\na CI server\nGive CI server\nlimited permissions\nPromote\nimmutable,\nversioned artifacts\nacross\nenvironments\nOnce a pull request\nis merged, deploy\nautomatically\nDeploy with Terraform, Atlantis, Terraform\nCloud, Terraform Enterprise, Terragrunt,\nscripts\nLimited deployment strategies (make sure to\nhandle errors: retries,\nerrored.tfstate!)\nRun deployment on a CI server\nGive CI server temporary credentials solely\nto invoke a separate, locked-down worker\nthat has admin permissions\nPromote immutable, versioned artifacts\nacross environments\nOnce a pull request is merged, go through\nan approval workflow where someone\nchecks the plan output one last time, and\nthen deploy automatically\nIf you follow this process, you will be able to run application and\ninfrastructure code in dev, test it, review it, package it into versioned,\nimmutable artifacts, and promote those artifacts from environment to\nenvironment, as shown in Figure 10-6.\n\n' metadata={'original_pages_range': '611-614', 'source': '180_Putting_It_All_Together', 'file_path': '/mnt/c/Users/Carlos/Documents/Github/jupiter-iaa-azure/Scripts/RAG/../../data/optimized_chunks/Libro-TF/180_Putting_It_All_Together.pdf', 'num_pages': 4}", "page_content='Figure 10-6. Promoting an immutable, versioned artifact of Terraform code from environment to\nenvironment.\nConclusion\nIf you’ve made it to this point in the book, you now know just about\neverything you need to use Terraform in the real world, including how to\nwrite Terraform code; how to manage Terraform state; how to create\nreusable modules with Terraform; how to do loops, if-statements, and\ndeployments; how to manage secrets; how to work with multiple regions,\naccounts, and clouds; how to write production-grade Terraform code; how\nto test your Terraform code; and how to use Terraform as a team. You’ve\nworked through examples of deploying and managing servers, clusters of\nservers, load balancers, databases, scheduled actions, CloudWatch alarms,\nIAM users, reusable modules, zero-downtime deployment, AWS Secrets\nManager, Kubernetes clusters, automated tests, and more. Phew! Just don’t\nforget to run terraform destroy in each module when you’re all\ndone.\nThe power of Terraform, and more generally, IaC, is that you can manage\nall the operational concerns around an application using the same coding\nprinciples as the application itself. This allows you to apply the full power\nof software engineering to your infrastructure, including modules, code\nreviews, version control, and automated testing.\nIf you use Terraform correctly, your team will be able to deploy faster and\nrespond to changes more quickly. Hopefully, deployments will become\nroutine and boring—and in the world of operations, boring is a very good\nthing. And if you really do your job right, rather than spending all your time\nmanaging infrastructure by hand, your team will be able to spend more and\nmore time improving that infrastructure, allowing you to go even faster.\nThis is the end of the book but just the beginning of your journey with\nTerraform. To learn more about Terraform, IaC, and DevOps, head over to\nAppendix A for a list of recommended reading. And if you’ve got feedback\n\nor questions, I’d love to hear from you at jim@ybrikman.com. Thank you\nfor reading!\n1 The Standish Group, “CHAOS Manifesto 2013: Think Big, Act Small,” 2013,\nhttps://oreil.ly/ydaWQ.\n2 Dan Milstein, “How to Survive a Ground-Up Rewrite Without Losing Your Sanity,”\nOnStartups.com, April 8, 2013, https://oreil.ly/nOGrU.\n3 See the Gruntwork Infrastructure as Code Library.\n4 Writing the README first is called Readme-Driven Development.\n5 See 10 real-world stories of how we’ve compromised CI/CD pipelines for some eye-opening\nexamples.\n6 Check out Gruntwork Pipelines for a real-world example of this worker pattern.\n7 Credit for how to promote Terraform code across environments goes to Kief Morris: Using\nPipelines to Manage Environments with Infrastructure as Code.' metadata={'original_pages_range': '615-616', 'source': '181_Conclusion', 'file_path': '/mnt/c/Users/Carlos/Documents/Github/jupiter-iaa-azure/Scripts/RAG/../../data/optimized_chunks/Libro-TF/181_Conclusion.pdf', 'num_pages': 2}", "page_content='Appendix A. Recommended\nReading\nThe following are some of the best resources I’ve found on DevOps and\ninfrastructure as code, including books, blog posts, newsletters, and talks.\nBooks\nInfrastructure as Code: Dynamic Systems for the Cloud Age by Kief\nMorris (O’Reilly)\nSite Reliability Engineering: How Google Runs Production Systems by\nBetsy Beyer, Chris Jones, Jennifer Petoff, and Niall Richard Murphy\n(O’Reilly)\nThe DevOps Handbook: How To Create World-Class Agility,\nReliability, and Security in Technology Organizations by Gene Kim,\nJez Humble, Patrick Debois, and John Willis (IT Revolution Press)\nDesigning Data-Intensive Applications by Martin Kleppmann\n(O’Reilly)\nContinuous Delivery: Reliable Software Releases through Build, Test,\nand Deployment Automation by Jez Humble and David Farley\n(Addison-Wesley Professional)\nRelease It! Design and Deploy Production-Ready Software by Michael\nT. Nygard (The Pragmatic Bookshelf)\nKubernetes in Action by Marko Luksa (Manning)\nLeading the Transformation: Applying Agile and DevOps Principles at\nScale by Gary Gruver and Tommy Mouser (IT Revolution Press)' metadata={'original_pages_range': '617', 'source': '182_A._Recommended_Reading_y_1_mas', 'file_path': '/mnt/c/Users/Carlos/Documents/Github/jupiter-iaa-azure/Scripts/RAG/../../data/optimized_chunks/Libro-TF/182_A._Recommended_Reading_y_1_mas.pdf', 'num_pages': 1}", "page_content='Visible Ops Handbook by Kevin Behr, Gene Kim, and George\nSpafford (Information Technology Process Institute)\nEffective DevOps by Jennifer Davis and Ryn Daniels (O’Reilly)\nLean Enterprise by Jez Humble, Joanne Molesky, Barry O’Reilly\n(O’Reilly)\nHello, Startup: A Programmer’s Guide to Building Products,\nTechnologies, and Teams by Yevgeniy Brikman (O’Reilly)\nBlogs\nHigh Scalability\nCode as Craft\nAWS News Blog\nKitchen Soap\nPaul Hammant’s blog\nMartin Fowler’s blog\nGruntwork Blog\nYevgeniy Brikman blog\nTalks\n“Reusable, Composable, Battle-Tested Terraform Modules” by\nYevgeniy Brikman\n“5 Lessons Learned from Writing Over 300,000 Lines of Infrastructure\nCode” by Yevgeniy Brikman\n“Automated Testing for Terraform, Docker, Packer, Kubernetes, and\nMore” by Yevgeniy Brikman' metadata={'original_pages_range': '618', 'source': '183_Blogs_y_1_mas', 'file_path': '/mnt/c/Users/Carlos/Documents/Github/jupiter-iaa-azure/Scripts/RAG/../../data/optimized_chunks/Libro-TF/183_Blogs_y_1_mas.pdf', 'num_pages': 1}", "page_content='“Infrastructure as Code: Running Microservices on AWS using\nDocker, Terraform, and ECS” by Yevgeniy Brikman\n“Agility Requires Safety” by Yevgeniy Brikman\n“Adopting Continuous Delivery” by Jez Humble\n“Continuously Deploying Culture” by Michael Rembetsy and Patrick\nMcDonnell\n“10+ Deploys Per Day: Dev and Ops Cooperation at Flickr” by John\nAllspaw and Paul Hammond\n“Why Google Stores Billions of Lines of Code in a Single Repository”\nby Rachel Potvin\n“The Language of the System” by Rich Hickey\n“Real Software Engineering” by Glenn Vanderburg\nNewsletters\nDevOps Weekly\nGruntwork Newsletter\nTerraform: Up & Running Newsletter\nTerraform Weekly Newsletter\nOnline Forums\nTerraform subforum of HashiCorp Discuss\nTerraform subreddit\nDevOps subreddit' metadata={'original_pages_range': '619', 'source': '184_Newsletters_y_1_mas', 'file_path': '/mnt/c/Users/Carlos/Documents/Github/jupiter-iaa-azure/Scripts/RAG/../../data/optimized_chunks/Libro-TF/184_Newsletters_y_1_mas.pdf', 'num_pages': 1}", "page_content='Index\nA\nAccess Key IDs (in AWS), Setting Up Your AWS Account, Installing\nTerraform, Providers-Human users, CircleCI as a CI server, with stored\nsecrets-CircleCI as a CI server, with stored secrets\naccessing secrets, interfaces for, The Interface You Use to Access Secrets\n(see also secrets management)\naccidental complexity, Why It Takes So Long to Build Production-Grade\nInfrastructure\naccounts (in AWS)\nconfiguring multiple, Working with Multiple AWS Accounts-Working\nwith Multiple AWS Accounts\nsetup, Setting Up Your AWS Account-Setting Up Your AWS Account\nad hoc scripts, Ad Hoc Scripts-Ad Hoc Scripts\nagent versus agentless software, Agent Versus Agentless-Agent Versus\nAgentless\nALB (Application Load Balancer), Deploying a Load Balancer-Deploying a\nLoad Balancer, Small Modules-Small Modules\naliases, Working with Multiple AWS Regions\nconfiguration aliases, Creating Modules That Can Work with Multiple\nProviders-Creating Modules That Can Work with Multiple Providers\nfor Gmail, Working with Multiple AWS Accounts\n\nKMS CMK aliases, Encrypted files\nwhen to use, Working with Multiple AWS Regions-Working with\nMultiple AWS Regions, Working with Multiple AWS Accounts\nAmazon S3 (Simple Storage Service), as remote backend, Shared Storage\nfor State Files-Shared Storage for State Files\nAmazon Web Services (see AWS)\nAMI (Amazon Machine Image)\nami parameter, Deploying a Single Server\nPacker template, Server Templating Tools-Server Templating Tools\nzero-downtime deployment, Zero-Downtime Deployment-Zero-\nDowntime Deployment\nAMI IDs, managing, Working with Multiple AWS Regions-Working with\nMultiple AWS Regions\nami parameter, Provisioning Tools, Deploying a Single Server\nAnsible, comparison with other IaC tools, How Does Terraform Compare to\nOther IaC Tools?-Provisioning plus server templating plus orchestration\napplication code workflow\nautomated tests, Run Automated Tests\nchanging code, Make Code Changes-Make Code Changes\ncomparison with infrastructure code workflow, Putting It All Together\ndeployment, Deploy-Promotion across environments\nlocally run code, Run the Code Locally\nmerge and release, Merge and Release\nreviewing code changes, Submit Changes for Review\n\nsteps in, A Workflow for Deploying Application Code\nversion control, Use Version Control\nApplication Load Balancer (ALB), Deploying a Load Balancer-Deploying a\nLoad Balancer, Small Modules-Small Modules\narguments for resources, Deploying a Single Server\narrays\nlookup syntax, Loops with the count Parameter\nof resources, Loops with the count Parameter\nASGs (Auto Scaling Groups), Deploying a Cluster of Web Servers-\nDeploying a Cluster of Web Servers\ninstance refresh, Zero-Downtime Deployment Has Limitations-Zero-\nDowntime Deployment Has Limitations\nscheduled actions, Module Outputs-Module Outputs\nzero-downtime deployment, Zero-Downtime Deployment-Zero-\nDowntime Deployment\nassume role policies, EC2 Instance running Jenkins as a CI server, with\nIAM roles\nAtlantis, Run Automated Tests-Deployment tooling\nauditing multiple AWS accounts, Working with Multiple AWS Accounts\nauthentication (see secrets management)\nauto healing, Orchestration Tools\nauto scaling, Orchestration Tools, Module Outputs-Module Outputs, If-\nstatements with the count parameter-If-statements with the count parameter\nautomated tests, Automated tests\n\nin application code workflow, Run Automated Tests\nend-to-end tests, End-to-End Tests-End-to-End Tests\nin infrastructure code workflow, Run Automated Tests-Run\nAutomated Tests\nintegration tests\nwith multiple modules, Integration Tests-Integration Tests\nretries in, Retries-Retries\nRuby comparison, Integration Tests-Integration Tests\nstages of, Test stages-Test stages\nplan testing, Plan testing-Plan testing\npurpose of, Automated Tests\nserver testing, Server testing-Server testing\nstatic analysis, Static analysis-Static analysis\ntypes of, Automated Tests-Automated Tests\nunit tests\ndependency injection, Dependency injection-Dependency\ninjection\nRuby comparison, Unit Tests-Unit Tests\nrunning in parallel, Running tests in parallel-Running tests in\nparallel\nwith Terratest, Unit testing Terraform code-Unit testing Terraform\ncode\nwhen to use, When to use validations, preconditions, and\npostconditions\n\nAvailability Zones (AZs), Deploying a Single Server, count and for_each\nHave Limitations-count and for_each Have Limitations\nAWS (Amazon Web Services)\naccounts\nconfiguring multiple, Working with Multiple AWS Accounts-\nWorking with Multiple AWS Accounts\nsetup, Setting Up Your AWS Account-Setting Up Your AWS\nAccount\nAZs (Availability Zones), Deploying a Single Server, count and\nfor_each Have Limitations-count and for_each Have Limitations\nbenefits of, Getting Started with Terraform\nconfiguring as provider, Deploying a Single Server\nregions, Deploying a Single Server, Working with Multiple AWS\nRegions-Working with Multiple AWS Regions\naws eks update-kubeconfig command, Deploying Docker Containers in\nAWS Using Elastic Kubernetes Service\nAWS Provider, Open Source Code Examples, What Is a Provider?\nAZs (Availability Zones), Deploying a Single Server, count and for_each\nHave Limitations-count and for_each Have Limitations\nB\nbackends\nconfiguring with Terragrunt, Promote artifacts across environments-\nPromote artifacts across environments\nlocal backends, Shared Storage for State Files\n\nremote backends\nlimitations of, Limitations with Terraform’s Backends-\nLimitations with Terraform’s Backends\nshared storage with, Shared Storage for State Files-Shared\nStorage for State Files\nBash scripts, Beyond Terraform Modules\nas ad hoc scripts, Ad Hoc Scripts-Ad Hoc Scripts\nexternalizing, The terraform_remote_state Data Source-The\nterraform_remote_state Data Source\nbest practices for testing, Conclusion\nblocking public access to S3 buckets, Shared Storage for State Files\nblue-green deployment, Deployment strategies\nbranches, reasons to avoid, The trouble with branches-The trouble with\nbranches\nbucket parameter, Shared Storage for State Files, Shared Storage for State\nFiles\nbuilt-in functions, explained, The terraform_remote_state Data Source-The\nterraform_remote_state Data Source\nbus factor, What Are the Benefits of Infrastructure as Code?\nC\ncanary deployment, Deployment strategies\ncentralized secret stores, The Way You Store Secrets\nchanging code\n\nin application code workflow, Make Code Changes-Make Code\nChanges\nin infrastructure code workflow, Make Code Changes\nchecklist for production-grade infrastructure, The Production-Grade\nInfrastructure Checklist-The Production-Grade Infrastructure Checklist\nChef, comparison with other IaC tools, How Does Terraform Compare to\nOther IaC Tools?-Provisioning plus server templating plus orchestration\nchild accounts, creating, Working with Multiple AWS Accounts-Working\nwith Multiple AWS Accounts\nCI servers\nautomated tests, Run Automated Tests\nas deployment servers, Deployment server, Deployment server-\nDeployment server\nmachine user authentication, Machine users-GitHub Actions as a CI\nserver, with OIDC, Conclusion\nCI/CD workflow\nfor application code\nautomated tests, Run Automated Tests\nchanging code, Make Code Changes-Make Code Changes\ncomparison with infrastructure code workflow, Putting It All\nTogether\ndeployment, Deploy-Promotion across environments\nlocally run code, Run the Code Locally\nmerge and release, Merge and Release\n\nreviewing code changes, Submit Changes for Review\nsteps in, A Workflow for Deploying Application Code\nversion control, Use Version Control\nCircleCI, CircleCI as a CI server, with stored secrets-CircleCI as a CI\nserver, with stored secrets\nEC2 Instances with IAM roles, EC2 Instance running Jenkins as a CI\nserver, with IAM roles-EC2 Instance running Jenkins as a CI server,\nwith IAM roles\nGitHub Actions, GitHub Actions as a CI server, with OIDC-GitHub\nActions as a CI server, with OIDC\nfor infrastructure code\nautomated tests, Run Automated Tests-Run Automated Tests\nchanging code, Make Code Changes\ncomparison with application code workflow, Putting It All\nTogether\ndeployment, Deploy-Promote artifacts across environments\nlocally run code, Run the Code Locally\nmerge and release, Merge and Release-Merge and Release\nreviewing code changes, Submit Changes for Review-Style guide\nsteps in, A Workflow for Deploying Infrastructure Code\nversion control, Use Version Control-The trouble with branches\nCIDR blocks, Deploying a Single Web Server, Static analysis\nCircleCI, CircleCI as a CI server, with stored secrets-CircleCI as a CI\nserver, with stored secrets, Run Automated Tests\n\nCircleCI Context, CircleCI as a CI server, with stored secrets\nCLB (Classic Load Balancer), Deploying a Load Balancer\ncleaning up manual tests, Cleaning Up After Tests-Cleaning Up After Tests\nCLI (command-line interface), The Interface You Use to Access Secrets\ncloud providers (see providers)\nCloudFormation, comparison with other IaC tools, How Does Terraform\nCompare to Other IaC Tools?-Provisioning plus server templating plus\norchestration\ncluster of web servers\ndeploying, Deploying a Cluster of Web Servers-Deploying a Cluster of\nWeb Servers\nrefactoring, Small Modules-Small Modules\nclusters (Kubernetes), Orchestration Tools\ndeploying Docker containers, Deploying Docker Containers in AWS\nUsing Elastic Kubernetes Service-Deploying Docker Containers in\nAWS Using Elastic Kubernetes Service\ninspecting, A Crash Course on Kubernetes-A Crash Course on\nKubernetes\nCMK (Customer Managed Key), Encrypted files\ncode examples\nfair use permissions, Using the Code Examples-Using the Code\nExamples\nlocation of, Open Source Code Examples-Open Source Code\nExamples\nversions used for, Open Source Code Examples\n\ncoding guidelines, Submit Changes for Review-Style guide\ncollaboration using Git repositories, Deploying a Single Server\ncombining IaC tools, Use of Multiple Tools Together-Provisioning plus\nserver templating plus orchestration\ncommand history, avoiding writing to, Human users\ncommand-line interface (CLI), The Interface You Use to Access Secrets\ncomments, as documentation, Documentation\ncommunity size in IaC tool comparison, Large Community Versus Small\nCommunity-Large Community Versus Small Community\ncomparison of IaC tools, How Does Terraform Compare to Other IaC\nTools?-Provisioning plus server templating plus orchestration\nagent versus agentless, Agent Versus Agentless-Agent Versus\nAgentless\ncombining tools, Use of Multiple Tools Together-Provisioning plus\nserver templating plus orchestration\nconfiguration management versus provisioning, Configuration\nManagement Versus Provisioning\ngeneral-purpose versus domain-specific language, General-Purpose\nLanguage Versus Domain-Specific Language-General-Purpose\nLanguage Versus Domain-Specific Language\nlarge versus small community, Large Community Versus Small\nCommunity-Large Community Versus Small Community\nmaster versus masterless, Master Versus Masterless-Master Versus\nMasterless\nmature versus cutting edge, Mature Versus Cutting Edge\n\nmutable versus immutable infrastructure, Mutable Infrastructure\nVersus Immutable Infrastructure-Mutable Infrastructure Versus\nImmutable Infrastructure\npaid versus free, Paid Versus Free Offering-Paid Versus Free Offering\nprocedural versus declarative language, Procedural Language Versus\nDeclarative Language-Procedural Language Versus Declarative\nLanguage\ncomposable modules for production-grade infrastructure, Composable\nModules-Composable Modules\nconcat built-in function, If-else-statements with the count parameter\nconditionals\ncount parameter\nif-else-statements with, If-else-statements with the count\nparameter-If-else-statements with the count parameter\nif-statements with, If-statements with the count parameter-If-\nstatements with the count parameter\nfor_each and for expressions, Conditionals with for_each and for\nExpressions-Conditionals with for_each and for Expressions\nif string directive, Conditionals with the if String Directive-\nConditionals with the if String Directive\nternary syntax, Isolation via Workspaces, If-statements with the count\nparameter, If-else-statements with the count parameter\ntypes and purpose of, Conditionals\nconfigurable web servers, deploying, Deploying a Configurable Web\nServer-Deploying a Configurable Web Server\n\nconfiguration aliases, Creating Modules That Can Work with Multiple\nProviders-Creating Modules That Can Work with Multiple Providers\nconfiguration drift, What Is DevOps?\nconfiguration files\nexplained, How Does Terraform Work?\nnaming conventions, Isolation via File Layout-Isolation via File\nLayout\nconfiguration management tools, Configuration Management Tools-\nConfiguration Management Tools\ncombining with provisioning, Provisioning plus configuration\nmanagement\nprovisioning tools versus, Configuration Management Versus\nProvisioning\ncontainer engines, Server Templating Tools\ncontainers, Server Templating Tools\ndeploying with EKS (Elastic Kubernetes Service), Deploying Docker\nContainers in AWS Using Elastic Kubernetes Service-Deploying\nDocker Containers in AWS Using Elastic Kubernetes Service\nexplained, A Crash Course on Docker-A Crash Course on Docker\nPods, Orchestration Tools, A Crash Course on Kubernetes-A Crash\nCourse on Kubernetes\ncontrol plane (Kubernetes), A Crash Course on Kubernetes, Deploying\nDocker Containers in AWS Using Elastic Kubernetes Service\ncost in IaC tool comparison, Paid Versus Free Offering-Paid Versus Free\nOffering\n\ncount parameter\nconditionals with\nif-else-statements, If-else-statements with the count parameter-If-\nelse-statements with the count parameter\nif-statements, If-statements with the count parameter-If-\nstatements with the count parameter\nlimitations, Loops with the count Parameter-Loops with the count\nParameter, count and for_each Have Limitations-count and for_each\nHave Limitations\nloops with, Loops with the count Parameter-Loops with the count\nParameter\ncreate_before_destroy lifecycle setting, Deploying a Cluster of Web\nServers, Terraform Tips and Tricks: Loops, If-Statements, Deployment, and\nGotchas, Zero-Downtime Deployment, Zero-Downtime Deployment Has\nLimitations, Refactoring Can Be Tricky\ncreation-time provisioners, Provisioners\ncredentials (see secrets)\nCustomer Managed Key (CMK), Encrypted files\ncustomer secrets, The Types of Secrets You Store\nD\ndata sources\nexternal, External data source-External data source\nreplicating in multiple AWS regions, Working with Multiple AWS\nRegions-Working with Multiple AWS Regions\nsecrets management, Resources and Data Sources\n\nwith encrypted files, Encrypted files-Encrypted files\nwith environment variables, Environment variables-Environment\nvariables\nwith secret stores, Secret stores-Secret stores\nfor subnets, Deploying a Cluster of Web Servers-Deploying a Cluster\nof Web Servers\nterraform_remote_state, The terraform_remote_state Data Source-The\nterraform_remote_state Data Source\ndeclarative language\nprocedural versus, Procedural Language Versus Declarative Language-\nProcedural Language Versus Declarative Language\nTerraform as, Terraform Tips and Tricks: Loops, If-Statements,\nDeployment, and Gotchas\ndeclaring variables, Deploying a Configurable Web Server-Deploying a\nConfigurable Web Server\ndefault parameter, Deploying a Configurable Web Server\nDefault VPC (in AWS), Setting Up Your AWS Account, Deploying a\nSingle Web Server, Deploying a Cluster of Web Servers\ndefault_tags block, Loops with for_each Expressions\ndefining listeners, Deploying a Load Balancer\ndependencies\nimplicit, Deploying a Single Web Server\nin Terragrunt, Promote artifacts across environments\ntypes of, Versioned Modules\n\nversioning pinning, Versioned Modules-Versioned Modules\ndependency injection, Dependency injection-Dependency injection\ndepends_on parameter, Deploying a Configurable Web Server\ndeploying\napplication code, Deploy-Promotion across environments\nDocker with EKS (Elastic Kubernetes Service), Deploying Docker\nContainers in AWS Using Elastic Kubernetes Service-Deploying\nDocker Containers in AWS Using Elastic Kubernetes Service\ninfrastructure code, Deploy-Promote artifacts across environments\nload balancers, Deploying a Load Balancer-Deploying a Load\nBalancer\nmodules in multiple AWS regions, Working with Multiple AWS\nRegions-Working with Multiple AWS Regions\nresources in multiple AWS regions, Working with Multiple AWS\nRegions-Working with Multiple AWS Regions\nservers\ncluster of web servers, Deploying a Cluster of Web Servers-\nDeploying a Cluster of Web Servers\nconfigurable web servers, Deploying a Configurable Web Server-\nDeploying a Configurable Web Server\nsingle servers, Deploying a Single Server-Deploying a Single\nServer\nweb servers, Deploying a Single Web Server-Deploying a Single\nWeb Server\n\nwith zero-downtime deployment, Zero-Downtime Deployment-Zero-\nDowntime Deployment\ndeployment servers, Deployment server, Deployment server-Deployment\nserver\ndeployment strategies, Deployment strategies-Deployment strategies,\nDeployment strategies-Deployment strategies\ndeployment tools, Deployment tooling, Deployment tooling\nDeployments (Kubernetes), Orchestration Tools, A Crash Course on\nKubernetes-A Crash Course on Kubernetes\ndescription parameter, Deploying a Configurable Web Server, Deploying a\nConfigurable Web Server\ndestroy-time provisioners, Provisioners\nDevOps, What Is DevOps?-What Is DevOps?\nproject time estimates, Why It Takes So Long to Build Production-\nGrade Infrastructure-Why It Takes So Long to Build Production-Grade\nInfrastructure\nresources for information, Recommended Reading-Online Forums\nDocker\ncontainers (see containers)\ndeploying with EKS (Elastic Kubernetes Service), Deploying Docker\nContainers in AWS Using Elastic Kubernetes Service-Deploying\nDocker Containers in AWS Using Elastic Kubernetes Service\nexplained, A Crash Course on Docker-A Crash Course on Docker\ninstalling, A Crash Course on Docker\npurpose of, Server Templating Tools\n\nDocker Hub, A Crash Course on Docker\ndocker kill command, A Crash Course on Kubernetes\ndocker ps command, A Crash Course on Kubernetes\nDocker Registry, A Crash Course on Docker\ndocker rm command, A Crash Course on Docker\ndocker run command, A Crash Course on Docker\ndocker start command, A Crash Course on Docker\ndocumentation\nIaC as, What Are the Benefits of Infrastructure as Code?\nfor Terraform, What You Won’t Find in This Book, Deploying a Single\nServer\ntypes of, Documentation-Documentation\nDRY (Don't Repeat Yourself) principle, Deploying a Configurable Web\nServer, Limitations with Terraform’s Backends, Promote artifacts across\nenvironments\nDSL (domain-specific language) versus GPL (general-purpose\nprogramming language), General-Purpose Language Versus Domain-\nSpecific Language-General-Purpose Language Versus Domain-Specific\nLanguage\nDynamoDB tables\nlimitations of remote backends, Limitations with Terraform’s\nBackends-Limitations with Terraform’s Backends\nlocking with, Shared Storage for State Files\ndynamodb_table parameter, Shared Storage for State Files\n\nE\nEC2 Instances, Deploying a Single Server, Deploying a Single Server\n(see also servers)\ndeploying in multiple AWS regions, Working with Multiple AWS\nRegions-Working with Multiple AWS Regions\nIAM roles for, EC2 Instance running Jenkins as a CI server, with IAM\nroles-EC2 Instance running Jenkins as a CI server, with IAM roles\nEKS (Elastic Kubernetes Service), Deploying Docker Containers in AWS\nUsing Elastic Kubernetes Service-Deploying Docker Containers in AWS\nUsing Elastic Kubernetes Service\nELB (Elastic Load Balancer) service, Deploying a Load Balancer\nenabling\nserver-side encryption, Shared Storage for State Files\nversioning in Amazon S3, Shared Storage for State Files\nencrypt parameter, Shared Storage for State Files\nencryption\nsecrets management with, Encrypted files-Encrypted files\nserver-side, enabling, Shared Storage for State Files\nend-to-end tests, Automated Tests, End-to-End Tests-End-to-End Tests\nenforcing tagging standards, Loops with for_each Expressions\nenvironment variables, secrets management with, The\nterraform_remote_state Data Source, Human users-Human users,\nEnvironment variables-Environment variables\nenvironments\n\nfolders for, Isolation via File Layout-Isolation via File Layout\npromotion across, Promotion across environments, Promote artifacts\nacross environments-Promote artifacts across environments\nsandbox, Manual Testing Basics-Cleaning Up After Tests, Unit testing\nTerraform code, Run the Code Locally\nerrors in deployment, Deployment strategies-Deployment strategies\nessential complexity, Why It Takes So Long to Build Production-Grade\nInfrastructure\nexample code, as documentation, Documentation\nexamples\nfair use permissions, Using the Code Examples-Using the Code\nExamples\nlocation of, Open Source Code Examples-Open Source Code\nExamples\nversions used for, Open Source Code Examples\nexecuting scripts\nfor external data source, External data source-External data source\nwith provisioners, Provisioners-Provisioners\nexpressions, Deploying a Single Web Server\nexternal data source, External data source-External data source\nexternalizing Bash scripts, The terraform_remote_state Data Source-The\nterraform_remote_state Data Source\nF\n\nfair use permissions for code examples, Using the Code Examples-Using\nthe Code Examples\nfalse incrementalism, Work Incrementally\nfeature toggles, Deployment strategies\nfile layout\nconventions for, File layout\nisolating state files, Isolation via File Layout-Isolation via File Layout\nfile paths\nlocal, Module Versioning\npath references for, File Paths-File Paths\nfile-based secret stores, The Way You Store Secrets\nfolders\nfor environments, Isolation via File Layout-Isolation via File Layout\nrunning commands across multiple, Isolation via File Layout\nfor expressions\nconditionals with, Conditionals with for_each and for Expressions-\nConditionals with for_each and for Expressions\nloops with, Loops with for Expressions-Loops with for Expressions\nfor string directive, loops with, Loops with the for String Directive-Loops\nwith the for String Directive\nformat built-in function, The terraform_remote_state Data Source\nfor_each expressions\nconditionals with, Conditionals with for_each and for Expressions-\nConditionals with for_each and for Expressions\n\nlimitations, count and for_each Have Limitations-count and for_each\nHave Limitations\nloops with, Loops with for_each Expressions-Loops with for_each\nExpressions\nfunction composition, Composable Modules-Composable Modules\nG\nget deployments command, A Crash Course on Kubernetes, Deploying\nDocker Containers in AWS Using Elastic Kubernetes Service\nget nodes command, A Crash Course on Kubernetes, Deploying Docker\nContainers in AWS Using Elastic Kubernetes Service\nget pods command, A Crash Course on Kubernetes, Deploying Docker\nContainers in AWS Using Elastic Kubernetes Service\nget services command, A Crash Course on Kubernetes, Deploying Docker\nContainers in AWS Using Elastic Kubernetes Service\ngit commit command, Deploying a Single Server\ngit push command, Deploying a Single Server\nGit repositories\ncollaborating with teammates, Deploying a Single Server\nSSH authentication for, Module Versioning\nGitHub Actions, GitHub Actions as a CI server, with OIDC-GitHub\nActions as a CI server, with OIDC\nGitHub, pull requests, Submit Changes for Review\n.gitignore file, Deploying a Single Server\nGmail aliases, Working with Multiple AWS Accounts\n\nGo, installing, Unit testing Terraform code\nGolden Rule of Terraform, The Golden Rule of Terraform-The Golden Rule\nof Terraform\nGPL (general-purpose programming language) versus DSL (domain-\nspecific language), General-Purpose Language Versus Domain-Specific\nLanguage-General-Purpose Language Versus Domain-Specific Language\nH\nHCL (HashiCorp Configuration Language), Deploying a Single Server\nhealth checks, Deploying a Load Balancer\nheredoc syntax, Deploying a Single Web Server, Conditionals with the if\nString Directive\nhistory files, avoiding writing to, Human users\nHofstadter's Law, Why It Takes So Long to Build Production-Grade\nInfrastructure\nhuman users, secrets management and, Human users-Human users\nhypervisors, Server Templating Tools\nI\nIaC (infrastructure as code), Production-Grade Terraform Code\n(see also production-grade infrastructure)\nad hoc scripts, Ad Hoc Scripts-Ad Hoc Scripts\nbenefits of, What Are the Benefits of Infrastructure as Code?-What\nAre the Benefits of Infrastructure as Code?\nconfiguration management tools, Configuration Management Tools-\nConfiguration Management Tools\n\nimporting into state files, Valid Plans Can Fail\norchestration tools, Orchestration Tools-Orchestration Tools\nout of band resources, Valid Plans Can Fail\nprovisioning tools, Provisioning Tools\npurpose of, What Is Infrastructure as Code?\nrefactoring, Refactoring Can Be Tricky-Refactoring Can Be Tricky\nresources for information, Recommended Reading-Online Forums\nserver templating tools, Server Templating Tools-Server Templating\nTools\nteam adoption of\nincrementalism in, Work Incrementally-Work Incrementally\nlearning curve, Give Your Team the Time to Learn-Give Your\nTeam the Time to Learn\nreturn on investment, Convince Your Boss-Convince Your Boss\ntool comparison, How Does Terraform Compare to Other IaC Tools?-\nProvisioning plus server templating plus orchestration\nagent versus agentless, Agent Versus Agentless-Agent Versus\nAgentless\ncombining tools, Use of Multiple Tools Together-Provisioning\nplus server templating plus orchestration\nconfiguration management versus provisioning, Configuration\nManagement Versus Provisioning\ngeneral-purpose versus domain-specific language, General-\nPurpose Language Versus Domain-Specific Language-General-\nPurpose Language Versus Domain-Specific Language\n\nlarge versus small community, Large Community Versus Small\nCommunity-Large Community Versus Small Community\nmaster versus masterless, Master Versus Masterless-Master\nVersus Masterless\nmature versus cutting edge, Mature Versus Cutting Edge\nmutable versus immutable infrastructure, Mutable Infrastructure\nVersus Immutable Infrastructure-Mutable Infrastructure Versus\nImmutable Infrastructure\npaid versus free, Paid Versus Free Offering-Paid Versus Free\nOffering\nprocedural versus declarative language, Procedural Language\nVersus Declarative Language-Procedural Language Versus\nDeclarative Language\nvalidating, Manual Testing Basics\nIAM (Identity and Access Management) service, Setting Up Your AWS\nAccount\nIAM OIDC identity providers, GitHub Actions as a CI server, with OIDC\nIAM Policies, Setting Up Your AWS Account\nIAM roles\nchild account authentication, Working with Multiple AWS Accounts\ncross-account authentication, Working with Multiple AWS Accounts\nfor EC2 Instances, EC2 Instance running Jenkins as a CI server, with\nIAM roles-EC2 Instance running Jenkins as a CI server, with IAM\nroles\nIAM users\n\nchanging policies, If-else-statements with the count parameter-If-else-\nstatements with the count parameter\ncreating, Setting Up Your AWS Account, Loops with the count\nParameter-Loops with for_each Expressions\nerrors, Valid Plans Can Fail-Valid Plans Can Fail\nidempotence, Configuration Management Tools, Deploying a Single Server,\nShared Storage for State Files\nif string directive, conditionals with, Conditionals with the if String\nDirective-Conditionals with the if String Directive\nif-else-statements with count parameter, If-else-statements with the count\nparameter-If-else-statements with the count parameter\nif-statements with count parameter, If-statements with the count parameter-\nIf-statements with the count parameter\nimages (of servers), Server Templating Tools-Server Templating Tools\nimmutable infrastructure\nexplained, Server Templating Tools\nmutable versus, Mutable Infrastructure Versus Immutable\nInfrastructure-Mutable Infrastructure Versus Immutable Infrastructure\npromotion across environments, Promotion across environments,\nPromote artifacts across environments\nreleasing application code, Merge and Release\nworkflows, Putting It All Together-Putting It All Together\nimplicit dependencies, Deploying a Single Web Server\nimporting infrastructure, Valid Plans Can Fail\n\nincrementalism in IaC (infrastructure as code) adoption, Work\nIncrementally-Work Incrementally\ninfrastructure as code (see IaC)\ninfrastructure code workflow\nautomated tests, Run Automated Tests-Run Automated Tests\nchanging code, Make Code Changes\ncomparison with application code workflow, Putting It All Together\ndeployment, Deploy-Promote artifacts across environments\nlocally run code, Run the Code Locally\nmerge and release, Merge and Release-Merge and Release\nreviewing code changes, Submit Changes for Review-Style guide\nsteps in, A Workflow for Deploying Infrastructure Code\nversion control, Use Version Control-The trouble with branches\ninfrastructure secrets, The Types of Secrets You Store\ninline blocks\ncreating multiple with for_each expressions, Loops with for_each\nExpressions-Loops with for_each Expressions\nseparate resources versus, Inline Blocks-Inline Blocks\ninput variables, Deploying a Configurable Web Server-Deploying a\nConfigurable Web Server\nin composable modules, Composable Modules-Composable Modules\nin modules, Module Inputs-Module Inputs\n\ninspecting Kubernetes clusters, A Crash Course on Kubernetes-A Crash\nCourse on Kubernetes\ninstalling\nDocker, A Crash Course on Docker\nGo, Unit testing Terraform code\nkubectl, A Crash Course on Kubernetes\nproviders, How Do You Install Providers?-How Do You Install\nProviders?\nTerraform, Installing Terraform-Installing Terraform, Versioned\nModules\nTerragrunt, Promote artifacts across environments\ninstance metadata endpoints, EC2 Instance running Jenkins as a CI server,\nwith IAM roles\ninstance profiles, EC2 Instance running Jenkins as a CI server, with IAM\nroles\ninstance refresh, Zero-Downtime Deployment Has Limitations-Zero-\nDowntime Deployment Has Limitations\ninstance_type parameter, Deploying a Single Server\nintegration tests\npurpose of, Automated Tests\nretries in, Retries-Retries\nRuby comparison, Integration Tests-Integration Tests\nstages of, Test stages-Test stages\nwith multiple modules, Integration Tests-Integration Tests\n\ninterfaces, accessing secrets, The Interface You Use to Access Secrets\ninterpolation, Deploying a Configurable Web Server\nisolating\nmultiple AWS accounts, Working with Multiple AWS Accounts\nat network level, Inline Blocks\nstate files, What Is Terraform State?\npurpose of, State File Isolation-State File Isolation\nvia file layout, Isolation via File Layout-Isolation via File Layout\nvia workspaces, Isolation via Workspaces-Isolation via\nWorkspaces\nK\nkernel, Server Templating Tools\nkernel space, Server Templating Tools\nkey parameter, Shared Storage for State Files\nkey policies, Encrypted files\nKMS (key management system), The Way You Store Secrets\nkubectl, A Crash Course on Kubernetes, A Crash Course on Kubernetes-A\nCrash Course on Kubernetes\nKubernetes\nclusters, Orchestration Tools\ndeploying Docker containers, Deploying Docker Containers in\nAWS Using Elastic Kubernetes Service-Deploying Docker\nContainers in AWS Using Elastic Kubernetes Service\n\ninspecting, A Crash Course on Kubernetes-A Crash Course on\nKubernetes\ncontrol plane, A Crash Course on Kubernetes, Deploying Docker\nContainers in AWS Using Elastic Kubernetes Service\nEKS (Elastic Kubernetes Service), Deploying Docker Containers in\nAWS Using Elastic Kubernetes Service-Deploying Docker Containers\nin AWS Using Elastic Kubernetes Service\nexplained, A Crash Course on Kubernetes-A Crash Course on\nKubernetes\nkubectl, A Crash Course on Kubernetes, A Crash Course on\nKubernetes-A Crash Course on Kubernetes\nobjects in, A Crash Course on Kubernetes\nPods, Orchestration Tools, A Crash Course on Kubernetes-A Crash\nCourse on Kubernetes\nrunning, A Crash Course on Kubernetes\nworker nodes, A Crash Course on Kubernetes, Deploying Docker\nContainers in AWS Using Elastic Kubernetes Service\nKubernetes Deployment, Orchestration Tools, A Crash Course on\nKubernetes-A Crash Course on Kubernetes\nKubernetes Service, A Crash Course on Kubernetes, A Crash Course on\nKubernetes-A Crash Course on Kubernetes\nL\nlarge modules, limitations of, Small Modules\nlaunch configurations, Deploying a Cluster of Web Servers\nlaunch templates, Deploying a Cluster of Web Servers\n\nlearning curve in IaC (infrastructure as code) adoption, Give Your Team the\nTime to Learn-Give Your Team the Time to Learn\nlength built-in function, Loops with the count Parameter\nlifecycle settings, Deploying a Cluster of Web Servers, Shared Storage for\nState Files, Zero-Downtime Deployment\nlist comprehensions (Python), Loops with for Expressions\nlistener rules, creating, Deploying a Load Balancer\nlisteners, defining, Deploying a Load Balancer\nliterals, Deploying a Single Web Server\nlive repository, Live repo and modules repo-The Golden Rule of Terraform,\nPromote artifacts across environments-Promote artifacts across\nenvironments\nload balancing, Orchestration Tools, Deploying a Load Balancer-Deploying\na Load Balancer\nlocal backends, Shared Storage for State Files\nlocal file paths, Module Versioning\nlocal names, How Do You Install Providers?\nlocal references, Module Locals\nlocal values in modules, Module Locals-Module Locals\nlocally run code\nin application code workflow, Run the Code Locally\nin infrastructure code workflow, Run the Code Locally\nlock files, Versioned Modules\nlocking\n\nwith DynamoDB tables, Shared Storage for State Files\nwith remote backends, Shared Storage for State Files\nstate files, What Is Terraform State?\nversion control disadvantages of, Shared Storage for State Files\nlookup syntax for arrays, Loops with the count Parameter\nloops\ncount parameter, Loops with the count Parameter-Loops with the\ncount Parameter\nfor expressions, Loops with for Expressions-Loops with for\nExpressions\nfor string directive, Loops with the for String Directive-Loops with the\nfor String Directive\nfor_each expressions, Loops with for_each Expressions-Loops with\nfor_each Expressions\ntypes and purpose of, Loops\nM\nmachine users, secrets management and, Machine users-GitHub Actions as\na CI server, with OIDC, Conclusion\nmanaged node groups, Deploying Docker Containers in AWS Using Elastic\nKubernetes Service-Deploying Docker Containers in AWS Using Elastic\nKubernetes Service\nManaged Policies (in AWS), Setting Up Your AWS Account\nmanual tests\ncleanup, Cleaning Up After Tests-Cleaning Up After Tests\n\nexplained, Manual Testing Basics-Manual Testing Basics\nRuby comparison, Manual Tests-Manual Tests\nmaps of resources, Loops with for_each Expressions\nmaster versus masterless servers, Master Versus Masterless-Master Versus\nMasterless\nmaturity in IaC tool comparison, Mature Versus Cutting Edge\nmerging code\nin application code workflow, Merge and Release\nin infrastructure code workflow, Merge and Release-Merge and\nRelease\nmocks, Automated Tests\nmodules\nauthenticating multiple AWS accounts, Working with Multiple AWS\nAccounts-Working with Multiple AWS Accounts\nconfiguring for Kubernetes, A Crash Course on Kubernetes-A Crash\nCourse on Kubernetes\ncount parameter, Loops with the count Parameter-Loops with the\ncount Parameter\ncreating for Kubernetes, A Crash Course on Kubernetes-A Crash\nCourse on Kubernetes\ncreating reusable, Module Basics-Module Basics\ndeploying in multiple AWS regions, Working with Multiple AWS\nRegions-Working with Multiple AWS Regions\nfor_each expressions, Loops with for_each Expressions\n\ninline blocks versus separate resources, Inline Blocks-Inline Blocks\ninput variables, Module Inputs-Module Inputs\nintegration tests, Integration Tests-Integration Tests\nlocal values, Module Locals-Module Locals\nfor multiple providers, Creating Modules That Can Work with\nMultiple Providers-Creating Modules That Can Work with Multiple\nProviders\noutput variables, Module Outputs-Module Outputs\npath references, File Paths-File Paths\nfor production-grade infrastructure\ncomposable modules, Composable Modules-Composable\nModules\nnon-Terraform code in, Beyond Terraform Modules-External data\nsource\nsmall modules, Small Modules-Small Modules\ntestable modules, Testable Modules-When to use validations,\npreconditions, and postconditions\nversioned modules, Versioned Modules-Versioned Modules\npublishing, Versioned Modules-Versioned Modules\npurpose of, Limitations with Terraform’s Backends, How to Create\nReusable Infrastructure with Terraform Modules\nrepository for, Live repo and modules repo-Live repo and modules\nrepo\nself-validating, Testable Modules\n\nprecondition/postcondition blocks in, Preconditions and\npostconditions-When to use validations, preconditions, and\npostconditions\nvalidation blocks in, Validations-Validations\ntagging standards, Loops with for_each Expressions\ntypes of, Module Basics, Creating Modules That Can Work with\nMultiple Providers\nunit tests for, Unit testing Terraform code-Unit testing Terraform code\nversioning, Module Versioning-Module Versioning\nversioning pinning, Versioned Modules-Versioned Modules\nmultiple AWS accounts, configuring, Working with Multiple AWS\nAccounts-Working with Multiple AWS Accounts\nmultiple AWS regions, configuring, Working with Multiple AWS Regions-\nWorking with Multiple AWS Regions\nmultiple folders, running commands across, Isolation via File Layout\nmultiple inline blocks, creating with for_each expressions, Loops with\nfor_each Expressions-Loops with for_each Expressions\nmultiple modules, integration tests with, Integration Tests-Integration Tests\nmultiple providers\nDocker, explained, A Crash Course on Docker-A Crash Course on\nDocker\nEKS (Elastic Kubernetes Service), Deploying Docker Containers in\nAWS Using Elastic Kubernetes Service-Deploying Docker Containers\nin AWS Using Elastic Kubernetes Service\n\nKubernetes, explained, A Crash Course on Kubernetes-A Crash\nCourse on Kubernetes\nmultiple AWS accounts, Working with Multiple AWS Accounts-\nWorking with Multiple AWS Accounts\nmultiple AWS regions, Working with Multiple AWS Regions-Working\nwith Multiple AWS Regions\nmultiple copies of different providers, Working with Multiple Different\nProviders-Deploying Docker Containers in AWS Using Elastic\nKubernetes Service\nmultiple copies of same provider, Working with Multiple Copies of the\nSame Provider-Creating Modules That Can Work with Multiple\nProviders\nreusable modules for, Creating Modules That Can Work with Multiple\nProviders-Creating Modules That Can Work with Multiple Providers\nmutable versus immutable infrastructure, Mutable Infrastructure Versus\nImmutable Infrastructure-Mutable Infrastructure Versus Immutable\nInfrastructure\nN\nnaming\nconfiguration files, Isolation via File Layout-Isolation via File Layout\nresources, Deploying a Single Server\nnetwork isolation, Inline Blocks\nnetwork security\nCIDR blocks, Deploying a Single Web Server, Static analysis\ndata sources for subnets, Deploying a Cluster of Web Servers\n\nDefault VPC (in AWS), Setting Up Your AWS Account\nnetwork isolation, Inline Blocks\nport numbers and, Deploying a Single Web Server\nsecurity groups, creating, Deploying a Single Web Server, Deploying a\nLoad Balancer\nVPC subnets, Deploying a Single Web Server\nNLB (Network Load Balancer), Deploying a Load Balancer\nnon-Terraform code, running in modules, Beyond Terraform Modules-\nExternal data source\nnull_resource, Provisioners with null_resource-Provisioners with\nnull_resource\nO\nobjects (Kubernetes), A Crash Course on Kubernetes\nOIDC (Open ID Connect), GitHub Actions as a CI server, with OIDC-\nGitHub Actions as a CI server, with OIDC\none built-in function, If-else-statements with the count parameter\nOPA (Open Policy Agent), Plan testing-Plan testing\nOpenStack Heat, comparison with other IaC tools, How Does Terraform\nCompare to Other IaC Tools?-Provisioning plus server templating plus\norchestration\norchestration tools, Orchestration Tools-Orchestration Tools\ncombining with provisioning and server templating, Provisioning plus\nserver templating plus orchestration-Provisioning plus server\ntemplating plus orchestration\n\nfor deployment, Deployment tooling\npurpose of, A Crash Course on Kubernetes\nout of band resources, Valid Plans Can Fail\noutput variables, Deploying a Configurable Web Server-Deploying a\nConfigurable Web Server\nin composable modules, Composable Modules-Composable Modules\nin modules, Module Outputs-Module Outputs\nP\nPacker, Server Templating Tools-Server Templating Tools\nparallel unit tests, running, Running tests in parallel-Running tests in\nparallel\npartial configurations, Limitations with Terraform’s Backends, Integration\nTests-Integration Tests\npassing secrets via environment variables, The terraform_remote_state Data\nSource, Human users-Human users\npath references in modules, File Paths-File Paths\npermissions for code examples, Using the Code Examples-Using the Code\nExamples\npersonal secrets, The Types of Secrets You Store\nplain text, avoiding secrets in, Secret Management Basics-Secret\nManagement Basics\nplan files, secrets management, Plan files-Plan files\nplan testing, Plan testing-Plan testing, Run Automated Tests\nplugins, providers as, What Is a Provider?-What Is a Provider?\n\nPod Template, A Crash Course on Kubernetes-A Crash Course on\nKubernetes\nPods, Orchestration Tools, A Crash Course on Kubernetes-A Crash Course\non Kubernetes\npolicy-as-code tools, Plan testing-Plan testing\nport numbers, Deploying a Single Web Server\npostcondition blocks, Preconditions and postconditions-When to use\nvalidations, preconditions, and postconditions\nprecondition blocks, Preconditions and postconditions-When to use\nvalidations, preconditions, and postconditions\npreferred local names, How Do You Install Providers?\nprevent_destroy parameter, Shared Storage for State Files\nprivate API, state files as, What Is Terraform State?\nprivate Git repositories, Module Versioning\nprivate keys, The Way You Store Secrets\nprivate subnets, Deploying a Single Web Server\nPrivate Terraform Registry, Versioned Modules\nprocedural versus declarative language, Procedural Language Versus\nDeclarative Language-Procedural Language Versus Declarative Language\nproduction-grade infrastructure\nchecklist for, The Production-Grade Infrastructure Checklist-The\nProduction-Grade Infrastructure Checklist\ndefinition of, Production-Grade Terraform Code\nmodules for\n\ncomposable modules, Composable Modules-Composable\nModules\nnon-Terraform code in, Beyond Terraform Modules-External data\nsource\nsmall modules, Small Modules-Small Modules\ntestable modules, Testable Modules-When to use validations,\npreconditions, and postconditions\nversioned modules, Versioned Modules-Versioned Modules\ntime estimates for, Production-Grade Terraform Code-Why It Takes So\nLong to Build Production-Grade Infrastructure\npromotion across environments, Promotion across environments, Promote\nartifacts across environments-Promote artifacts across environments\nproviders, A Crash Course on Docker\n(see also multiple providers)\nconfiguring, Deploying a Single Server, How Do You Use Providers?-\nHow Do You Use Providers?\nmultiple AWS accounts, Working with Multiple AWS Accounts-\nWorking with Multiple AWS Accounts\nmultiple AWS regions, Working with Multiple AWS Regions-\nWorking with Multiple AWS Regions\nin reusable modules, Creating Modules That Can Work with\nMultiple Providers-Creating Modules That Can Work with\nMultiple Providers\ninstalling, How Do You Install Providers?-How Do You Install\nProviders?\nas plugins, What Is a Provider?-What Is a Provider?\n\nsecrets management, Providers-Providers\nhuman users and, Human users-Human users\nmachine users and, Machine users-GitHub Actions as a CI server,\nwith OIDC, Conclusion\ntransparent portability, How Does Terraform Work?\nversioning pinning, Versioned Modules-Versioned Modules\nprovisioners\nexecuting scripts, Provisioners-Provisioners\nwith null_resource, Provisioners with null_resource-Provisioners with\nnull_resource\nUser Data scripts versus, Provisioners\nprovisioning tools, Provisioning Tools\ncombining with configuration management, Provisioning plus\nconfiguration management\ncombining with orchestration and server templating, Provisioning plus\nserver templating plus orchestration-Provisioning plus server\ntemplating plus orchestration\ncombining with server templating, Provisioning plus server\ntemplating-Provisioning plus server templating\nconfiguration management tools versus, Configuration Management\nVersus Provisioning\npublic access to S3 buckets, blocking, Shared Storage for State Files\npublic keys, The Way You Store Secrets\npublic subnets, Deploying a Single Web Server\n\nPublic Terraform Registry, Versioned Modules-Versioned Modules\npublishing modules, Versioned Modules-Versioned Modules\npull requests, Submit Changes for Review\nPulumi, comparison with other IaC tools, How Does Terraform Compare to\nOther IaC Tools?-Provisioning plus server templating plus orchestration\nPuppet, comparison with other IaC tools, How Does Terraform Compare to\nOther IaC Tools?-Provisioning plus server templating plus orchestration\nR\nrandom integers, generating, count and for_each Have Limitations\nRDS (Relational Database Service), The terraform_remote_state Data\nSource\nREADME files, Documentation\nreconciliation loops, A Crash Course on Kubernetes\nrefactoring\nTerraform code, Refactoring Can Be Tricky-Refactoring Can Be\nTricky\nweb server clusters, Small Modules-Small Modules\nreferences, Deploying a Single Web Server\nregion parameter, Shared Storage for State Files\nregions (in AWS), Deploying a Single Server, Working with Multiple AWS\nRegions-Working with Multiple AWS Regions\nRego, Plan testing-Plan testing\nRelational Database Service (RDS), The terraform_remote_state Data\nSource\n\nrelative filepaths, File Paths\nreleasing code\nin application code workflow, Merge and Release\nin infrastructure code workflow, Merge and Release-Merge and\nRelease\nremote backends\nlimitations of, Limitations with Terraform’s Backends-Limitations\nwith Terraform’s Backends\nshared storage with, Shared Storage for State Files-Shared Storage for\nState Files\nremote procedure calls (RPCs), What Is a Provider?\nremoving resources, Cleanup-Cleanup\nreplicas, Orchestration Tools\nreplicating data sources in multiple AWS regions, Working with Multiple\nAWS Regions-Working with Multiple AWS Regions\nrepositories\nsharing, Deploying a Single Server\nSSH authentication for, Module Versioning\nfor version control, Live repo and modules repo-Live repo and\nmodules repo\nrequired_providers block, How Do You Install Providers?-How Do You\nInstall Providers?\nrequired_version parameter, Versioned Modules\nresource attribute references, Deploying a Single Web Server\n\nresource dependencies, Isolation via File Layout\nresources\narrays of, Loops with the count Parameter\ncreating, Deploying a Single Server\ncreating out of band, Valid Plans Can Fail\ndeploying in multiple AWS regions, Working with Multiple AWS\nRegions-Working with Multiple AWS Regions\nlifecycle settings, Deploying a Cluster of Web Servers\nmaps of, Loops with for_each Expressions\nnaming, Deploying a Single Server\nnull_resource, Provisioners with null_resource-Provisioners with\nnull_resource\nremoving, Cleanup-Cleanup\nsecrets management, Resources and Data Sources\nwith encrypted files, Encrypted files-Encrypted files\nwith environment variables, Environment variables-Environment\nvariables\nwith secret stores, Secret stores-Secret stores\nseparate resources versus inline blocks, Inline Blocks-Inline Blocks\ntagging standards, Loops with for_each Expressions\nresources for information, Recommended Reading-Online Forums\nretries\nin integration tests, Retries-Retries\n\nwith known errors, Deployment strategies\nreturn on investment for IaC (infrastructure as code), Convince Your Boss-\nConvince Your Boss\nreusability of code, What Are the Benefits of Infrastructure as Code?\nreusable modules, Creating Modules That Can Work with Multiple\nProviders\ncreating, Module Basics-Module Basics\nfor multiple providers, Creating Modules That Can Work with\nMultiple Providers-Creating Modules That Can Work with Multiple\nProviders\nreviewing code changes\nin application code workflow, Submit Changes for Review\nfor infrastructure code, Submit Changes for Review-Style guide\nrolling deployments, Configuration Management Tools, Deployment\nstrategies-Deployment strategies\nroot accounts, Working with Multiple AWS Accounts\nroot modules, Module Basics, Creating Modules That Can Work with\nMultiple Providers\nroot users, Setting Up Your AWS Account\nport numbers and, Deploying a Single Web Server\nRPCs (remote procedure calls), What Is a Provider?\nrunning\nKubernetes, A Crash Course on Kubernetes\nparallel unit tests, Running tests in parallel-Running tests in parallel\n\nS\nS3 buckets\ncreating, Shared Storage for State Files-Shared Storage for State Files\nlimitations of remote backends, Limitations with Terraform’s\nBackends-Limitations with Terraform’s Backends\npublic access, blocking, Shared Storage for State Files\nserver-side encryption, enabling, Shared Storage for State Files\nversioning, enabling, Shared Storage for State Files\nsandbox environments, Manual Testing Basics-Cleaning Up After Tests,\nUnit testing Terraform code, Run the Code Locally\nscheduled actions, Module Outputs, If-statements with the count parameter\nscripts\nfor deployment, Deployment tooling, Deployment tooling\nexecuting\nfor external data source, External data source-External data\nsource\nwith provisioners, Provisioners-Provisioners\nSecret Access Keys (in AWS), Setting Up Your AWS Account, Installing\nTerraform\nsecret stores, secrets management with, Secret stores-Secret stores\nsecrets\naccess interface for, The Interface You Use to Access Secrets\navoiding in plain text, Secret Management Basics-Secret Management\nBasics\n\npassing via environment variables, The terraform_remote_state Data\nSource\nwith remote backends, Shared Storage for State Files\nstorage methods, The Way You Store Secrets-The Way You Store\nSecrets\ntypes of, The Types of Secrets You Store-The Types of Secrets You\nStore\nversion control disadvantages, Shared Storage for State Files\nsecrets management\nCI/CD workflow\nCircleCI, CircleCI as a CI server, with stored secrets-CircleCI as\na CI server, with stored secrets\nEC2 Instances with IAM roles, EC2 Instance running Jenkins as a\nCI server, with IAM roles-EC2 Instance running Jenkins as a CI\nserver, with IAM roles\nGitHub Actions, GitHub Actions as a CI server, with OIDC-\nGitHub Actions as a CI server, with OIDC\nmultiple AWS accounts, Working with Multiple AWS Accounts,\nWorking with Multiple AWS Accounts-Working with Multiple AWS\nAccounts\nfor plan files, Plan files-Plan files\nfor providers, Providers-Providers\nhuman users and, Human users-Human users\nmachine users and, Machine users-GitHub Actions as a CI server,\nwith OIDC, Conclusion\n\nfor resources and data sources, Resources and Data Sources\nwith encrypted files, Encrypted files-Encrypted files\nwith environment variables, Environment variables-Environment\nvariables\nwith secret stores, Secret stores-Secret stores\nrules of, Secret Management Basics-Secret Management Basics\nfor state files, State files-State files\nTerraform authentication options, Installing Terraform\ntool comparison, A Comparison of Secret Management Tools-A\nComparison of Secret Management Tools\nsecurity (see network security; secrets management)\nsecurity groups, creating, Deploying a Single Web Server, Deploying a\nLoad Balancer\nselector block, A Crash Course on Kubernetes\nself-validating modules, Testable Modules\nprecondition/postcondition blocks in, Preconditions and\npostconditions-When to use validations, preconditions, and\npostconditions\nvalidation blocks in, Validations-Validations\nsemantic versioning, Module Versioning\nsensitive parameter, Deploying a Configurable Web Server, Deploying a\nConfigurable Web Server\nseparate resources versus inline blocks, Inline Blocks-Inline Blocks\nserver templating tools, Server Templating Tools-Server Templating Tools\n\ncombining with orchestration and provisioning, Provisioning plus\nserver templating plus orchestration-Provisioning plus server\ntemplating plus orchestration\ncombining with provisioning, Provisioning plus server templating\nserver testing, Server testing-Server testing\nserver-side encryption, enabling, Shared Storage for State Files\nservers, deploying\ncluster of web servers, Deploying a Cluster of Web Servers-Deploying\na Cluster of Web Servers\nconfigurable web servers, Deploying a Configurable Web Server-\nDeploying a Configurable Web Server\nsingle servers, Deploying a Single Server-Deploying a Single Server\nweb servers, Deploying a Single Web Server-Deploying a Single Web\nServer\nservice discovery, Orchestration Tools\nshared storage for state files, Shared Storage for State Files-Shared Storage\nfor State Files\nwith remote backends, Shared Storage for State Files-Shared Storage\nfor State Files\nversion control disadvantages, Shared Storage for State Files-Shared\nStorage for State Files\nsharing Git repositories with teammates, Deploying a Single Server\nsmall modules for production-grade infrastructure, Small Modules-Small\nModules\nsnowflake servers, What Is DevOps?\n\nsoftware delivery, Why Terraform\nsops, Encrypted files\nsplat expression, Loops with the count Parameter\nSpolsky, Joel, Use Version Control\nSSH authentication for Git repositories, Module Versioning\nstages of integration tests, Test stages-Test stages\nstate files\nexplained, What Is Terraform State?-What Is Terraform State?\nimporting infrastructure, Valid Plans Can Fail\nisolating, What Is Terraform State?\npurpose of, State File Isolation-State File Isolation\nvia file layout, Isolation via File Layout-Isolation via File Layout\nvia workspaces, Isolation via Workspaces-Isolation via\nWorkspaces\nlocking, What Is Terraform State?\nas private API, What Is Terraform State?\nsecrets management, State files-State files\nshared storage for, Shared Storage for State Files-Shared Storage for\nState Files\nwith remote backends, Shared Storage for State Files-Shared\nStorage for State Files\nversion control disadvantages, Shared Storage for State Files-\nShared Storage for State Files\n\nterraform_remote_state data source, The terraform_remote_state Data\nSource-The terraform_remote_state Data Source\nstatic analysis, Static analysis-Static analysis\nstoring secrets, The Way You Store Secrets-The Way You Store Secrets\nstring directives, Loops with the for String Directive\nstrip markers, Conditionals with the if String Directive\nstructural types, Deploying a Configurable Web Server\nstyle guidelines, Style guide\nsubnets\ndata sources for, Deploying a Cluster of Web Servers-Deploying a\nCluster of Web Servers\nof VPCs, Deploying a Single Web Server\nsubnets parameter, Deploying a Load Balancer\nsubnet_ids parameter, Deploying a Cluster of Web Servers\nT\ntagging standards, enforcing, Loops with for_each Expressions\ntarget groups, creating, Deploying a Load Balancer\nTDD (Test-Driven Development), Testable Modules\nteam adoption of IaC\nincrementalism in, Work Incrementally-Work Incrementally\nlearning curve, Give Your Team the Time to Learn-Give Your Team\nthe Time to Learn\nreturn on investment, Convince Your Boss-Convince Your Boss\n\ntemplatefile built-in function, The terraform_remote_state Data Source-The\nterraform_remote_state Data Source, File Paths\nternary syntax, Isolation via Workspaces, If-statements with the count\nparameter, If-else-statements with the count parameter\nTerraform\nauthentication options, Installing Terraform\nchanges, 2017-2019, Changes from the First Edition to the Second\nEdition-Changes from the First Edition to the Second Edition\ncomparison with other IaC tools, How Does Terraform Compare to\nOther IaC Tools?-Provisioning plus server templating plus\norchestration\nagent versus agentless, Agent Versus Agentless-Agent Versus\nAgentless\ncombining tools, Use of Multiple Tools Together-Provisioning\nplus server templating plus orchestration\nconfiguration management versus provisioning, Configuration\nManagement Versus Provisioning\ngeneral-purpose versus domain-specific language, General-\nPurpose Language Versus Domain-Specific Language-General-\nPurpose Language Versus Domain-Specific Language\nlarge versus small community, Large Community Versus Small\nCommunity-Large Community Versus Small Community\nmaster versus masterless, Master Versus Masterless-Master\nVersus Masterless\nmature versus cutting edge, Mature Versus Cutting Edge\n\nmutable versus immutable infrastructure, Mutable Infrastructure\nVersus Immutable Infrastructure-Mutable Infrastructure Versus\nImmutable Infrastructure\npaid versus free, Paid Versus Free Offering-Paid Versus Free\nOffering\nprocedural versus declarative language, Procedural Language\nVersus Declarative Language-Procedural Language Versus\nDeclarative Language\ncore, What Is a Provider?, Versioned Modules-Versioned Modules\nas declarative language, Terraform Tips and Tricks: Loops, If-\nStatements, Deployment, and Gotchas\nas deployment tool, Deployment tooling\ndocumentation, What You Won’t Find in This Book, Deploying a\nSingle Server\nGolden Rule of Terraform, The Golden Rule of Terraform-The Golden\nRule of Terraform\ninstalling, Installing Terraform-Installing Terraform, Versioned\nModules\nnew features, Changes from the Second Edition to the Third Edition-\nChanges from the Second Edition to the Third Edition\noperational overview, How Does Terraform Work?-How Does\nTerraform Work?\npurpose of, Preface\nrefactoring, Refactoring Can Be Tricky-Refactoring Can Be Tricky\ntransparent portability, How Does Terraform Work?\nversion used, Open Source Code Examples\n\nterraform apply command, Deploying a Single Server\nTerraform Cloud, Deployment tooling\nTerraform configurations, How Does Terraform Work?\nterraform console command, The terraform_remote_state Data Source\nterraform destroy command, Cleanup\nTerraform Enterprise, Deployment tooling\nterraform fmt command, Style guide\nterraform graph command, Deploying a Single Web Server\nterraform import command, Valid Plans Can Fail\nterraform init command, Deploying a Single Server, Shared Storage for\nState Files\nterraform output command, Deploying a Configurable Web Server\nterraform plan command, Deploying a Single Server, Valid Plans Can Fail-\nValid Plans Can Fail, Plan testing-Plan testing, Run Automated Tests\nTerraform state files (see state files)\nterraform validate command, Static analysis\nterraform version command, Versioned Modules\nterraform workspace list command, Isolation via Workspaces\nterraform workspace new command, Isolation via Workspaces\nterraform workspace select command, Isolation via Workspaces\nterraform workspace show command, Isolation via Workspaces\nterraform_remote_state data source, The terraform_remote_state Data\nSource-The terraform_remote_state Data Source\n\nTerragrunt, Limitations with Terraform’s Backends, Deployment tooling\ninstalling, Promote artifacts across environments\npromotion across environments, Promote artifacts across\nenvironments-Promote artifacts across environments\ntgswitch, Versioned Modules\nTerratest, Working with Multiple Different Providers\nintegration tests with, Test stages-Test stages\nplan tests with, Plan testing-Plan testing\nretries in integration tests, Retries-Retries\nunit tests with, Unit testing Terraform code-Unit testing Terraform\ncode\nversions of, Unit testing Terraform code\ntest doubles, Automated Tests\ntest pyramid, End-to-End Tests\nTest-Driven Development (TDD), Testable Modules\ntestable modules for production-grade infrastructure, Testable Modules-\nWhen to use validations, preconditions, and postconditions\ntesting\nautomated tests, Automated tests\nin application code workflow, Run Automated Tests\nend-to-end tests, End-to-End Tests-End-to-End Tests\nin infrastructure code workflow, Run Automated Tests-Run\nAutomated Tests\nintegration tests, Integration Tests-Retries\n\nplan testing, Plan testing-Plan testing\npurpose of, Automated Tests\nserver testing, Server testing-Server testing\nstatic analysis, Static analysis-Static analysis\ntypes of, Automated Tests-Automated Tests\nunit tests, Unit Tests-Running tests in parallel\nwhen to use, When to use validations, preconditions, and\npostconditions\nbest practices, Conclusion\ncomparison of approaches, Conclusion\nmanual tests\ncleanup, Cleaning Up After Tests-Cleaning Up After Tests\nexplained, Manual Testing Basics-Manual Testing Basics\nRuby comparison, Manual Tests-Manual Tests\ntfenv, Versioned Modules-Versioned Modules\ntgswitch, Versioned Modules\ntime estimates for production-grade infrastructure, Production-Grade\nTerraform Code-Why It Takes So Long to Build Production-Grade\nInfrastructure\ntransparent portability, How Does Terraform Work?\ntype constraints, Deploying a Configurable Web Server\ntype parameter, Deploying a Configurable Web Server\nU\n\nUbuntu, A Crash Course on Docker\nUI (user interface), The Interface You Use to Access Secrets\nunit tests\ndependency injection, Dependency injection-Dependency injection\npurpose of, Automated Tests\nRuby comparison, Unit Tests-Unit Tests\nrunning in parallel, Running tests in parallel-Running tests in parallel\nwith Terratest, Unit testing Terraform code-Unit testing Terraform\ncode\nunits, defined, Automated Tests\nUser Data scripts versus provisioners, Provisioners\nuser interface (UI), The Interface You Use to Access Secrets\nuser space, Server Templating Tools\nuser_data parameter, Provisioning Tools, Deploying a Single Web Server\nuser_data_replace_on_change parameter, Deploying a Single Web Server\nV\nVagrant, purpose of, Server Templating Tools\nvalidating\ncode, What Are the Benefits of Infrastructure as Code?\ninfrastructure, Manual Testing Basics\nvalidation blocks, Validations-Validations\nvalidation parameter, Deploying a Configurable Web Server\n\nvalues built-in function, Loops with for_each Expressions\nvariable references, Deploying a Configurable Web Server\nvariables, Module Inputs\n(see also input variables; output variables)\ndeclaring, Deploying a Configurable Web Server-Deploying a\nConfigurable Web Server\nversion control, Deploying a Single Server\nin application code workflow, Use Version Control\ndisadvantages of, Shared Storage for State Files-Shared Storage for\nState Files\nimportance of, What Are the Benefits of Infrastructure as Code?\nin infrastructure code workflow, Use Version Control-The trouble with\nbranches\nsecrets in, Secret Management Basics\nversioning\nin Amazon S3, Shared Storage for State Files, Shared Storage for State\nFiles\nmodules, Module Versioning-Module Versioning\nfor production-grade infrastructure, Versioned Modules-Versioned\nModules\nversioning pinning, Versioned Modules-Versioned Modules\nVM images, Server Templating Tools\nVMs (virtual machines), Server Templating Tools\nVPCs (virtual private clouds)\n\nDefault VPC (in AWS), Setting Up Your AWS Account\nnetwork isolation, Inline Blocks\nsubnets, Deploying a Single Web Server\nW\nweb servers\ndeploying, Deploying a Single Web Server-Deploying a Single Web\nServer\ncluster of web servers, Deploying a Cluster of Web Servers-\nDeploying a Cluster of Web Servers\nconfigurable web servers, Deploying a Configurable Web Server-\nDeploying a Configurable Web Server\nrefactoring, Small Modules-Small Modules\nworker nodes (Kubernetes), A Crash Course on Kubernetes, Deploying\nDocker Containers in AWS Using Elastic Kubernetes Service\nworkflows (see CI/CD workflow)\nworkspaces\nisolating state files, Isolation via Workspaces-Isolation via Workspaces\nlimitations of, Isolation via Workspaces\nY\nyak shaving, Why It Takes So Long to Build Production-Grade\nInfrastructure-Why It Takes So Long to Build Production-Grade\nInfrastructure\nZ\n\nzero-downtime deployment, Zero-Downtime Deployment-Zero-Downtime\nDeployment\nlimitations, Zero-Downtime Deployment Has Limitations-Zero-\nDowntime Deployment Has Limitations\n\nAbout the Author\nYevgeniy (Jim) Brikman loves programming, writing, speaking, traveling,\nand lifting heavy things. He is the cofounder of Gruntwork, a company that\nprovides DevOps as a Service. He’s also the author of another book\npublished by O’Reilly Media called Hello, Startup: A Programmer’s Guide\nto Building Products, Technologies, and Teams. Previously, he worked as a\nsoftware engineer at LinkedIn, TripAdvisor, Cisco Systems, and Thomson\nFinancial and got his BS and master’s degrees at Cornell University. For\nmore info, check out ybrikman.com.\n\nColophon\nThe animal on the cover of Terraform: Up and Running is a flying dragon\nlizard (Draco volans), a small reptile so named for its ability to glide using\nwinglike flaps of skin known as patagia. The patagia are brightly colored\nand allow the animal to glide for up to eight meters. The flying dragon\nlizard is commonly found in many Southeast Asian countries, including\nIndonesia, Vietnam, Thailand, the Philippines, and Singapore.\nFlying dragon lizards feed on insects and can grow to more than 20\ncentimeters in length. They live primarily in forested regions, gliding from\ntree to tree to find prey and avoid predators. Females descend from the trees\nonly to lay their eggs in hidden holes in the ground. The males are highly\nterritorial and will chase rivals from tree to tree.\nAlthough once thought to be poisonous, flying dragon lizards pose no threat\nto humans and are sometimes kept as pets. They are not currently\nthreatened or endangered. Many of the animals on O’Reilly covers are\nendangered; all of them are important to the world.\nThe cover illustration is by Karen Montgomery, based on an antique line\nengraving from Johnson’s Natural History. The cover fonts are Gilroy\nSemibold and Guardian Sans. The text font is Adobe Minion Pro; the\nheading font is Adobe Myriad Condensed; and the code font is Dalton\nMaag’s Ubuntu Mono.' metadata={'original_pages_range': '620-680', 'source': '185_Index', 'file_path': '/mnt/c/Users/Carlos/Documents/Github/jupiter-iaa-azure/Scripts/RAG/../../data/optimized_chunks/Libro-TF/185_Index.pdf', 'num_pages': 61}"], "source": "qdrant"}}

{"timestamp": "2025-11-11T16:53:21.708205+00:00", "level": "ERROR", "message": "Error verificando colecciones", "module": "vector_store", "function": "<module>", "line": 36, "request_id": "qdrant_init_1762880001", "session_id": "anonymous", "extra": {"error": "[Errno 111] Connection refused", "tipo_error": "ResponseHandlingException", "request_id": "qdrant_init_1762880001", "source": "data_processing"}}

{"timestamp": "2025-11-11T16:53:21.710171+00:00", "level": "ERROR", "message": "Error crítico en inicialización de Qdrant", "module": "vector_store", "function": "<module>", "line": 54, "request_id": "qdrant_init_1762880001", "session_id": "anonymous", "extra": {"error": "[Errno 111] Connection refused", "tipo_error": "ResponseHandlingException", "url": "http://localhost:6333", "collection_name": "Terraform_Book_Index", "duration": "0.024s", "request_id": "qdrant_init_1762880001", "source": "data_processing", "process_time": "0.024s"}}

{"timestamp": "2025-11-11T16:53:25.375712+00:00", "level": "ERROR", "message": "Error verificando colecciones", "module": "vector_store", "function": "<module>", "line": 36, "request_id": "qdrant_init_1762880005", "session_id": "anonymous", "extra": {"error": "[Errno 111] Connection refused", "tipo_error": "ResponseHandlingException", "request_id": "qdrant_init_1762880005", "source": "data_processing"}}

{"timestamp": "2025-11-11T16:53:25.377490+00:00", "level": "ERROR", "message": "Error crítico en inicialización de Qdrant", "module": "vector_store", "function": "<module>", "line": 54, "request_id": "qdrant_init_1762880005", "session_id": "anonymous", "extra": {"error": "[Errno 111] Connection refused", "tipo_error": "ResponseHandlingException", "url": "http://localhost:6333", "collection_name": "Terraform_Book_Index", "duration": "0.024s", "request_id": "qdrant_init_1762880005", "source": "data_processing", "process_time": "0.024s"}}

{"timestamp": "2025-11-11T16:53:28.956127+00:00", "level": "ERROR", "message": "Error verificando colecciones", "module": "vector_store", "function": "<module>", "line": 36, "request_id": "qdrant_init_1762880008", "session_id": "anonymous", "extra": {"error": "[Errno 111] Connection refused", "tipo_error": "ResponseHandlingException", "request_id": "qdrant_init_1762880008", "source": "data_processing"}}

{"timestamp": "2025-11-11T16:53:28.957620+00:00", "level": "ERROR", "message": "Error crítico en inicialización de Qdrant", "module": "vector_store", "function": "<module>", "line": 54, "request_id": "qdrant_init_1762880008", "session_id": "anonymous", "extra": {"error": "[Errno 111] Connection refused", "tipo_error": "ResponseHandlingException", "url": "http://localhost:6333", "collection_name": "Terraform_Book_Index", "duration": "0.025s", "request_id": "qdrant_init_1762880008", "source": "data_processing", "process_time": "0.025s"}}

{"timestamp": "2025-11-11T16:53:32.620209+00:00", "level": "ERROR", "message": "Error verificando colecciones", "module": "vector_store", "function": "<module>", "line": 36, "request_id": "qdrant_init_1762880012", "session_id": "anonymous", "extra": {"error": "[Errno 111] Connection refused", "tipo_error": "ResponseHandlingException", "request_id": "qdrant_init_1762880012", "source": "data_processing"}}

{"timestamp": "2025-11-11T16:53:32.622171+00:00", "level": "ERROR", "message": "Error crítico en inicialización de Qdrant", "module": "vector_store", "function": "<module>", "line": 54, "request_id": "qdrant_init_1762880012", "session_id": "anonymous", "extra": {"error": "[Errno 111] Connection refused", "tipo_error": "ResponseHandlingException", "url": "http://localhost:6333", "collection_name": "Terraform_Book_Index", "duration": "0.024s", "request_id": "qdrant_init_1762880012", "source": "data_processing", "process_time": "0.024s"}}

{"timestamp": "2025-11-11T16:53:36.491765+00:00", "level": "ERROR", "message": "Error verificando colecciones", "module": "vector_store", "function": "<module>", "line": 36, "request_id": "qdrant_init_1762880016", "session_id": "anonymous", "extra": {"error": "[Errno 111] Connection refused", "tipo_error": "ResponseHandlingException", "request_id": "qdrant_init_1762880016", "source": "data_processing"}}

{"timestamp": "2025-11-11T16:53:36.493399+00:00", "level": "ERROR", "message": "Error crítico en inicialización de Qdrant", "module": "vector_store", "function": "<module>", "line": 54, "request_id": "qdrant_init_1762880016", "session_id": "anonymous", "extra": {"error": "[Errno 111] Connection refused", "tipo_error": "ResponseHandlingException", "url": "http://localhost:6333", "collection_name": "Terraform_Book_Index", "duration": "0.023s", "request_id": "qdrant_init_1762880016", "source": "data_processing", "process_time": "0.023s"}}

{"timestamp": "2025-11-11T16:53:41.115623+00:00", "level": "ERROR", "message": "Error verificando colecciones", "module": "vector_store", "function": "<module>", "line": 36, "request_id": "qdrant_init_1762880021", "session_id": "anonymous", "extra": {"error": "[Errno 111] Connection refused", "tipo_error": "ResponseHandlingException", "request_id": "qdrant_init_1762880021", "source": "data_processing"}}

{"timestamp": "2025-11-11T16:53:41.117098+00:00", "level": "ERROR", "message": "Error crítico en inicialización de Qdrant", "module": "vector_store", "function": "<module>", "line": 54, "request_id": "qdrant_init_1762880021", "session_id": "anonymous", "extra": {"error": "[Errno 111] Connection refused", "tipo_error": "ResponseHandlingException", "url": "http://localhost:6333", "collection_name": "Terraform_Book_Index", "duration": "0.023s", "request_id": "qdrant_init_1762880021", "source": "data_processing", "process_time": "0.023s"}}

{"timestamp": "2025-11-11T16:53:47.486024+00:00", "level": "ERROR", "message": "Error verificando colecciones", "module": "vector_store", "function": "<module>", "line": 36, "request_id": "qdrant_init_1762880027", "session_id": "anonymous", "extra": {"error": "[Errno 111] Connection refused", "tipo_error": "ResponseHandlingException", "request_id": "qdrant_init_1762880027", "source": "data_processing"}}

{"timestamp": "2025-11-11T16:53:47.487521+00:00", "level": "ERROR", "message": "Error crítico en inicialización de Qdrant", "module": "vector_store", "function": "<module>", "line": 54, "request_id": "qdrant_init_1762880027", "session_id": "anonymous", "extra": {"error": "[Errno 111] Connection refused", "tipo_error": "ResponseHandlingException", "url": "http://localhost:6333", "collection_name": "Terraform_Book_Index", "duration": "0.031s", "request_id": "qdrant_init_1762880027", "source": "data_processing", "process_time": "0.031s"}}

{"timestamp": "2025-11-11T16:53:57.138596+00:00", "level": "ERROR", "message": "Error verificando colecciones", "module": "vector_store", "function": "<module>", "line": 36, "request_id": "qdrant_init_1762880037", "session_id": "anonymous", "extra": {"error": "[Errno 111] Connection refused", "tipo_error": "ResponseHandlingException", "request_id": "qdrant_init_1762880037", "source": "data_processing"}}

{"timestamp": "2025-11-11T16:53:57.140119+00:00", "level": "ERROR", "message": "Error crítico en inicialización de Qdrant", "module": "vector_store", "function": "<module>", "line": 54, "request_id": "qdrant_init_1762880037", "session_id": "anonymous", "extra": {"error": "[Errno 111] Connection refused", "tipo_error": "ResponseHandlingException", "url": "http://localhost:6333", "collection_name": "Terraform_Book_Index", "duration": "0.057s", "request_id": "qdrant_init_1762880037", "source": "data_processing", "process_time": "0.057s"}}

{"timestamp": "2025-11-11T16:54:13.138161+00:00", "level": "ERROR", "message": "Error verificando colecciones", "module": "vector_store", "function": "<module>", "line": 36, "request_id": "qdrant_init_1762880053", "session_id": "anonymous", "extra": {"error": "[Errno 111] Connection refused", "tipo_error": "ResponseHandlingException", "request_id": "qdrant_init_1762880053", "source": "data_processing"}}

{"timestamp": "2025-11-11T16:54:13.139940+00:00", "level": "ERROR", "message": "Error crítico en inicialización de Qdrant", "module": "vector_store", "function": "<module>", "line": 54, "request_id": "qdrant_init_1762880053", "session_id": "anonymous", "extra": {"error": "[Errno 111] Connection refused", "tipo_error": "ResponseHandlingException", "url": "http://localhost:6333", "collection_name": "Terraform_Book_Index", "duration": "0.025s", "request_id": "qdrant_init_1762880053", "source": "data_processing", "process_time": "0.025s"}}

{"timestamp": "2025-11-11T16:54:41.949534+00:00", "level": "ERROR", "message": "Error verificando colecciones", "module": "vector_store", "function": "<module>", "line": 36, "request_id": "qdrant_init_1762880081", "session_id": "anonymous", "extra": {"error": "[Errno 111] Connection refused", "tipo_error": "ResponseHandlingException", "request_id": "qdrant_init_1762880081", "source": "data_processing"}}

{"timestamp": "2025-11-11T16:54:41.951088+00:00", "level": "ERROR", "message": "Error crítico en inicialización de Qdrant", "module": "vector_store", "function": "<module>", "line": 54, "request_id": "qdrant_init_1762880081", "session_id": "anonymous", "extra": {"error": "[Errno 111] Connection refused", "tipo_error": "ResponseHandlingException", "url": "http://localhost:6333", "collection_name": "Terraform_Book_Index", "duration": "0.023s", "request_id": "qdrant_init_1762880081", "source": "data_processing", "process_time": "0.023s"}}

{"timestamp": "2025-11-11T16:55:36.394362+00:00", "level": "ERROR", "message": "Error verificando colecciones", "module": "vector_store", "function": "<module>", "line": 36, "request_id": "qdrant_init_1762880136", "session_id": "anonymous", "extra": {"error": "[Errno 111] Connection refused", "tipo_error": "ResponseHandlingException", "request_id": "qdrant_init_1762880136", "source": "data_processing"}}

{"timestamp": "2025-11-11T16:55:36.395805+00:00", "level": "ERROR", "message": "Error crítico en inicialización de Qdrant", "module": "vector_store", "function": "<module>", "line": 54, "request_id": "qdrant_init_1762880136", "session_id": "anonymous", "extra": {"error": "[Errno 111] Connection refused", "tipo_error": "ResponseHandlingException", "url": "http://localhost:6333", "collection_name": "Terraform_Book_Index", "duration": "0.024s", "request_id": "qdrant_init_1762880136", "source": "data_processing", "process_time": "0.024s"}}

{"timestamp": "2025-11-11T16:56:39.702439+00:00", "level": "ERROR", "message": "Error verificando colecciones", "module": "vector_store", "function": "<module>", "line": 36, "request_id": "qdrant_init_1762880199", "session_id": "anonymous", "extra": {"error": "[Errno 111] Connection refused", "tipo_error": "ResponseHandlingException", "request_id": "qdrant_init_1762880199", "source": "data_processing"}}

{"timestamp": "2025-11-11T16:56:39.704093+00:00", "level": "ERROR", "message": "Error crítico en inicialización de Qdrant", "module": "vector_store", "function": "<module>", "line": 54, "request_id": "qdrant_init_1762880199", "session_id": "anonymous", "extra": {"error": "[Errno 111] Connection refused", "tipo_error": "ResponseHandlingException", "url": "http://localhost:6333", "collection_name": "Terraform_Book_Index", "duration": "0.024s", "request_id": "qdrant_init_1762880199", "source": "data_processing", "process_time": "0.024s"}}

{"timestamp": "2025-11-11T16:57:42.866666+00:00", "level": "ERROR", "message": "Error verificando colecciones", "module": "vector_store", "function": "<module>", "line": 36, "request_id": "qdrant_init_1762880262", "session_id": "anonymous", "extra": {"error": "[Errno 111] Connection refused", "tipo_error": "ResponseHandlingException", "request_id": "qdrant_init_1762880262", "source": "data_processing"}}

{"timestamp": "2025-11-11T16:57:42.868197+00:00", "level": "ERROR", "message": "Error crítico en inicialización de Qdrant", "module": "vector_store", "function": "<module>", "line": 54, "request_id": "qdrant_init_1762880262", "session_id": "anonymous", "extra": {"error": "[Errno 111] Connection refused", "tipo_error": "ResponseHandlingException", "url": "http://localhost:6333", "collection_name": "Terraform_Book_Index", "duration": "0.024s", "request_id": "qdrant_init_1762880262", "source": "data_processing", "process_time": "0.024s"}}

{"timestamp": "2025-11-11T16:58:46.104073+00:00", "level": "ERROR", "message": "Error verificando colecciones", "module": "vector_store", "function": "<module>", "line": 36, "request_id": "qdrant_init_1762880326", "session_id": "anonymous", "extra": {"error": "[Errno 111] Connection refused", "tipo_error": "ResponseHandlingException", "request_id": "qdrant_init_1762880326", "source": "data_processing"}}

{"timestamp": "2025-11-11T16:58:46.105504+00:00", "level": "ERROR", "message": "Error crítico en inicialización de Qdrant", "module": "vector_store", "function": "<module>", "line": 54, "request_id": "qdrant_init_1762880326", "session_id": "anonymous", "extra": {"error": "[Errno 111] Connection refused", "tipo_error": "ResponseHandlingException", "url": "http://localhost:6333", "collection_name": "Terraform_Book_Index", "duration": "0.026s", "request_id": "qdrant_init_1762880326", "source": "data_processing", "process_time": "0.026s"}}

{"timestamp": "2025-11-11T16:59:49.472028+00:00", "level": "ERROR", "message": "Error verificando colecciones", "module": "vector_store", "function": "<module>", "line": 36, "request_id": "qdrant_init_1762880389", "session_id": "anonymous", "extra": {"error": "[Errno 111] Connection refused", "tipo_error": "ResponseHandlingException", "request_id": "qdrant_init_1762880389", "source": "data_processing"}}

{"timestamp": "2025-11-11T16:59:49.473593+00:00", "level": "ERROR", "message": "Error crítico en inicialización de Qdrant", "module": "vector_store", "function": "<module>", "line": 54, "request_id": "qdrant_init_1762880389", "session_id": "anonymous", "extra": {"error": "[Errno 111] Connection refused", "tipo_error": "ResponseHandlingException", "url": "http://localhost:6333", "collection_name": "Terraform_Book_Index", "duration": "0.028s", "request_id": "qdrant_init_1762880389", "source": "data_processing", "process_time": "0.028s"}}

{"timestamp": "2025-11-11T17:00:52.621558+00:00", "level": "ERROR", "message": "Error verificando colecciones", "module": "vector_store", "function": "<module>", "line": 36, "request_id": "qdrant_init_1762880452", "session_id": "anonymous", "extra": {"error": "[Errno 111] Connection refused", "tipo_error": "ResponseHandlingException", "request_id": "qdrant_init_1762880452", "source": "data_processing"}}

{"timestamp": "2025-11-11T17:00:52.623169+00:00", "level": "ERROR", "message": "Error crítico en inicialización de Qdrant", "module": "vector_store", "function": "<module>", "line": 54, "request_id": "qdrant_init_1762880452", "session_id": "anonymous", "extra": {"error": "[Errno 111] Connection refused", "tipo_error": "ResponseHandlingException", "url": "http://localhost:6333", "collection_name": "Terraform_Book_Index", "duration": "0.023s", "request_id": "qdrant_init_1762880452", "source": "data_processing", "process_time": "0.023s"}}

{"timestamp": "2025-11-11T17:09:50.474590+00:00", "level": "ERROR", "message": "Error inesperado en consulta a API", "module": "ui", "function": "get_api_response", "line": 53, "request_id": "721d206e", "session_id": "anonymous", "extra": {"error": "'Response' object has no attribute 'get'", "tipo_error": "AttributeError", "source": "ui"}}

{"timestamp": "2025-11-11T17:11:41.399273+00:00", "level": "ERROR", "message": "Error inesperado en consulta a API", "module": "ui", "function": "get_api_response", "line": 53, "request_id": "721d206e", "session_id": "anonymous", "extra": {"error": "'Response' object has no attribute 'get'", "tipo_error": "AttributeError", "source": "ui"}}

